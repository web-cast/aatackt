# 研究现状与趋势报告（自动综述）

- 生成日期：2026-02-13
- 文献数量：122
- 置信度阈值：0.85

## Executive Summary
本综述对工作区内已抽取为 txt 的文献进行批量结构化解析与聚类分析，围绕“研究主题—研究方法—主要结论”三维构建知识图谱，并对近五年（以抽取发表年份为准）的高频关键词演变进行统计与突现词识别。整体上，研究热点集中在间接提示注入/环境注入、Web/GUI 代理的安全评测基准、红队攻击与自动化对抗测试、以及多智能体协作场景下的传播式攻击与防御。方法层面呈现从静态提示集合评测向交互式环境注入与轨迹级评估演进的趋势，但在可验证安全策略、真实业务流数据、跨域泛化与人机协作安全协议等方面仍存在显著空白。

## 领域概览
以“工具使用/环境交互”为特征的 LLM 代理系统正在从单体模型转向多代理协作与多模态（Web、GUI、移动端）执行。与传统对话式安全不同，代理的风险主要体现为：对外部环境文本/视觉信号的信任边界不清、对工具调用副作用缺少约束、以及多轮轨迹中的状态与记忆可被操控。现有研究通常分为攻击（间接提示注入、越狱、后门/投毒、环境注入等）、评测（基准与红队框架）、防御（策略约束、隔离执行、检测与纠错、对齐与拒答等）三类，并逐步开始关注真实世界部署中的可追溯审计与治理。

## 热点趋势
### 主题—方法—结论三维聚类（主题簇概览）
| 主题簇 | 规模 | 代表关键词（自动） | 代表论文（节选） |
|---|---:|---|---|
| T1 | 18 | china, acm, software, bytedance, engineering, conference | Deceptive Alignment · Reverse Turing Test · Multi-Agent Systems · Prompt Injection · Agent Autonomy · Ethical, Poster: Agentic Shell Honeypot Using Structured Logging, Agent Large Language Model, Agent for Program Repair, Framework for Computer-Use Agents, Mimicking Human Expertise, Unity Is Strength: Collaborative LLM-Based Agents for Code Reviewer, AEGIS: An Agent-based Framework for Bug Reproduction from Issue (+10) |
| T2 | 15 | gui, backdoor, mobile, knowledge, multimodal, user | BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents, Dynamically Encrypted Multi-Backdoor Im-, MLLM-Powered Mobile GUI Agents, Mandarin apps. This paper introduces MobileFlow, a multimodal large language model meticulously, Poisoning Memory or Knowledge Bases, Progress and Prospects, Towards Trustworthy GUI Agents: A Survey, Watch Out for Your Agents! Investigating Backdoor (+7) |
| T3 | 11 | jailbreak, prompts, ieee, member, senior, aligned | The Crescendo Multi-Turn LLM Jailbreak Attack, Multimodal LLM Agents Exponentially Fast, PROMPTS ON ALIGNED LARGE LANGUAGE MODELS, Jailbreak Attacks, Amplified Vulnerabilities: Structured Jailbreak Attacks, Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework, JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks, Cyber Defense Perspective (+3) |
| T4 | 10 | web, cuas, tasks, content, adversarial, realistic | GENERALIST WEB AGENTS FOR PRIVACY LEAKAGE, AGENT OCCAM : A S IMPLE YET STRONG BASELINE, AdvAgent: Controllable Blackbox Red-teaming on Web Agents, Multimodal Web Agents, introduce a novel threat, WIPI, that indirectly controls Web, Agents via Advertising Delivery, Mind the Web: The Security of Web Use Agents, testing in realistic but controlled environments or ignore hybrid web-OS attack (+2) |
| T5 | 6 | prompt, indirect, injection, ipi, 2024, against | AGENT VIGIL : Generic Black-Box Red-teaming for Indirect Prompt Injection, Attacks in AI Agents, PromptArmor: Simple yet Effective Prompt Injection Defenses, Indirect Prompt Injection, Tool-Integrated Large Language Model Agents, Adaptive Attacks Break Defenses Against Indirect Prompt Injection |
| T6 | 3 | mas, communication, multi, systems, safeguard, association | Optimized Prompt Attacks, LLM-based Multi-agent Systems, Red-Teaming LLM Multi-Agent Systems via Communication Attacks |
| T7 | 2 | concordia, software, encs, performance, analysis, code | Identifying Performance-Sensitive Configurations in Software, Software Process Models Using Large Language |
| T8 | 2 | singapore, wuhan, nanyang, architecture, technological, city | Architecture Design, Engineering: Literature Review, Vision, and the Road |
| T9 | 1 | china, consequences, gui, unintended, gao, tsinghua | Characterizing Unintended Consequences in Human-GUI Agent |
| T10 | 1 | contextual, embodied, defects, backdoor, singapore, demonstrations | Compromising Embodied Agents with Contextual |
| T11 | 1 | deceptive, perspective, formation, game, refinement, association | Boosting LLM Agents with Recursive Contemplation |
| T12 | 1 | door, thought, actions, breaking, likelihood, requests | Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In |
| T13 | 1 | survey, systems, field, offer, multi, depth | DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis |
| T14 | 1 | role, beijing, delving, china, original, 2020 | Evil Geniuses: Delving into the Safety of LLM-based Agents |
| T15 | 1 | websites, autonomously, capable, function, 2023, gpt | LLM Agents can Autonomously Hack Websites |
| T16 | 1 | components, adversarial, present, like, against, risks | Mapping Adversarial Attacks against Language Agents |
| T17 | 1 | multi, off, malicious, 2023, systems, interacting | Multi-Agent Systems |
| T18 | 1 | dangerous, behaviors, multi, comprehensive, systems, collective | PsySafe: A Comprehensive Framework for Psychological-based Attack, |
| T19 | 1 | planning, 2023a, post, strategy, safe, framework | TrustAgent: Towards Safe and Trustworthy LLM-based Agents |
| T20 | 1 | bench, failure, evaluating, improvement, tsinghua, lack | AGENT-SAFETY BENCH : Evaluating the Safety of |
| T21 | 1 | 2024a, risks, guardrail, specific, adaptive, effective | AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety |
| T22 | 1 | values, singapore, human, alignment, nus, test | ALI-Agent: Assessing LLMs'Alignment with Human |
| T23 | 1 | reasoning, dynamically, malicious, then, hijacking, jiawei | Agents by Dynamically Hijacking Their Own Reasoning |
| T24 | 1 | google, view, usa, united, states, com | AirGapAgent: Protecting Privacy-Conscious Conversational Agents |
| T25 | 1 | automation, android, task, autodroid, tsinghua, knowledge | Android task automation with 158 common tasks. The results |
| T26 | 1 | vlm, vision, tasks, completing, daily, recognize | Attacking Vision-Language Computer Agents via Pop-ups |
| T27 | 1 | robustness, lms, adversarial, components, attacker, flow | DISSECTING ADVERSARIAL ROBUSTNESS OF |
| T28 | 1 | influence, decision, making, outcomes, device, prompting | Enhancing LLM Agent Safety via Causal Influence Prompting |
| T29 | 1 | awareness, social, web, queries, evaluating, shopping | Evaluating Cultural and Social Awareness of LLM Web Agents |
| T30 | 1 | threat, easily, framework, agentic, combine, sota | Evolving Security Threats |
| T31 | 1 | policies, aligned, social, alignment, approximately, decision | Existence of Probably Approximately Aligned Policies |
| T32 | 1 | microsoft, washington, united, states, cause, usa | Exploring LLM-Based Agents for Root Cause Analysis |
| T33 | 1 | embodied, input, chuan, benchmarks, behavioral, edu | From Safety Benchmarks to Input Moderation |
| T34 | 1 | choice, decision, cognitive, technology, biases, findings | LLM Agents Can Be Choice-Supportive Biased Evaluators: An Empirical Study |
| T35 | 1 | resilience, collaboration, multi, downstream, tasks, systems | On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents |
| T36 | 1 | clean, visual, mobile, backdoor, once, poisoned | Poison Once, Control Anywhere: Clean-Text Visual Backdoors in |
| T37 | 1 | misaligned, actions, detection, deploying, misalignment, outcomes | Preemptive Detection and Correction of Misaligned Actions in LLM Agents |
| T38 | 1 | fair, meta, web, end, prompt, attacker | Prompt Injection Attacks |
| T39 | 1 | agentdojo, tasks, solve, environment, injection, prompt | Prompt Injection Attacks and Defenses |
| T40 | 1 | browser, harmful, red, chat, teaming, trained | Refusal-Trained LLMs Are Easily Jailbroken |
| T41 | 1 | policy, action, rule, sota, guardrail, trajectory | SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning |
| T42 | 1 | task, indirect, alignment, psu, sources, defend | The Task Shield: Enforcing Task Alignment to Defend Against Indirect |
| T43 | 1 | memory, private, privacy, he1, demon, attacking | Unveiling Privacy Risks in LLM Agent Memory |
| T44 | 1 | code, execution, provide, unsafe, generation, input | evaluation of unsafe code generation and execution, diverse input formats, and high- |
| T45 | 1 | delft, retrieval, chain, generation, netherlands, project | A Multi-agent Onboarding Assistant based on Large Language |
| T46 | 1 | unsafe, real, automated, diverse, behaviors, risk | Automated Risk Simulator |
| T47 | 1 | deception, generative, frontier, multi, oversight, form | Deception via Steganography |
| T48 | 1 | visual, hard, usenix, national, solving, challenges | Generalized Visual CAPTCHA Solving |
| T49 | 1 | nguyen, compared, sis, optimize, various, baselines | GitHub README.MD Summarization |
| T50 | 1 | task, planning, risk, perception, static, rule | Graphormer-Guided Task Planning: Beyond Static Rules with LLM |
| T51 | 1 | review, issue, code, oriented, software, repair | Issue-Oriented Code Review |
| T52 | 1 | principles, design, utility, privacy, embedding, benign | LLM Agents Should Employ Security Principles |
| T53 | 1 | testing, louis, creating, department, scenarios, test | LLM-Agents Driven Automated Simulation Testing |
| T54 | 1 | game, chat, navigation, agile, constrained, optimal | Optimal Multi-Agent Navigation in Constrained Environments |
| T55 | 1 | usenix, gurion, ben, tokens, against, network | Proactive Defenses Against LLM Agents |
| T56 | 1 | lms, prompt, calls, injection, tool, privacy | RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage |
| T57 | 1 | context, manipulation, injection, memory, blockchain, prompt | Real AI Agents with Fake Memories: Fatal Context Manipulation Attacks on Web3 |
| T58 | 1 | harm, computer, benchmark, osworld, etc, misuse | Safety of Computer Use Agents |
| T59 | 1 | vulnerabilities, usenix, yuan, fudan, fuzzing, seed | Taint-Style Vulnerabilities in LLM-based Agents |
| T60 | 1 | browsing, dangers, hidden, autonomous, browser, web | The Hidden Dangers of Browsing AI Agents |
| T61 | 1 | adaptive, software, number, design, faculty, sciences | Towards Adaptive Software Agents for Debugging |
| T62 | 1 | dame, notre, usa, edu, virginia, guis | Vulnerability to Fine-Print Injections |
| T63 | 1 | isolation, apps, execution, systems, architecture, washington | design architecture that demonstrates the feasibility of execution |

### 主题—方法—结论三维聚类（结论簇概览）
| 结论簇 | 规模 | 代表关键词（自动） | 代表论文（节选） |
|---|---:|---|---|
| C1 | 5 | data, backdoor, asr, context, environment, training | Compromising Embodied Agents with Contextual, BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents, AirGapAgent: Protecting Privacy-Conscious Conversational Agents, Watch Out for Your Agents! Investigating Backdoor, AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety |
| C2 | 5 | 2023, capable, 2023a, call, 2022, tools | Evil Geniuses: Delving into the Safety of LLM-based Agents, LLM Agents can Autonomously Hack Websites, PROMPTS ON ALIGNED LARGE LANGUAGE MODELS, TrustAgent: Towards Safe and Trustworthy LLM-based Agents, Mapping Adversarial Attacks against Language Agents |
| C3 | 5 | 2024, 2025, pop, 2023, mobile, 2024a | Attacking Vision-Language Computer Agents via Pop-ups, Multimodal Web Agents, Poison Once, Control Anywhere: Clean-Text Visual Backdoors in, Attacks in AI Agents, Evolving Security Threats |
| C4 | 4 | web, code, elements, box, page, attacker | AGENT OCCAM : A S IMPLE YET STRONG BASELINE, Mind the Web: The Security of Web Use Agents, AdvAgent: Controllable Blackbox Red-teaming on Web Agents, introduce a novel threat, WIPI, that indirectly controls Web |
| C5 | 4 | asr, methods, baseline, table, llama, best | Agents by Dynamically Hijacking Their Own Reasoning, Jailbreak Attacks, AEGIS: An Agent-based Framework for Bug Reproduction from Issue, Agents via Advertising Delivery |
| C6 | 4 | computer, claude, anthropic, https, sonnet, com | Computer-Use Agents, Framework for Computer-Use Agents, testing in realistic but controlled environments or ignore hybrid web-OS attack, JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks |
| C7 | 3 | attacker, prompt, injection, important, benchmark, cases | Tool-Integrated Large Language Model Agents, Prompt Injection Attacks and Defenses, Prompt Injection Attacks |
| C8 | 2 | success, rate, adversarial, task, initial, against | AGENT VIGIL : Generic Black-Box Red-teaming for Indirect Prompt Injection, Environmental Injection Attacks |
| C9 | 2 | visual, challenges, bot, web, human, autonomy | GENERALIST WEB AGENTS FOR PRIVACY LEAKAGE, Generalized Visual CAPTCHA Solving |
| C10 | 2 | 2024, risks, adaptive, external, tools, tacks | Adaptive Attacks Break Defenses Against Indirect Prompt Injection, Enhancing LLM Agent Safety via Causal Influence Prompting |
| C11 | 2 | gui, 2025, fine, tuning, two, stage | FOR MULTIMODAL GUI- ORIENTED UNDERSTANDING, Progress and Prospects |
| C12 | 2 | software, end, development, process, 100, implementation | Red-Teaming LLM Multi-Agent Systems via Communication Attacks, Engineering: Literature Review, Vision, and the Road |
| C13 | 2 | code, review, issue, software, issues, analyzing | Unity Is Strength: Collaborative LLM-Based Agents for Code Reviewer, Issue-Oriented Code Review |
| C14 | 2 | university, mobile, gui, edu, manipulation, context | Real AI Agents with Fake Memories: Fatal Context Manipulation Attacks on Web3, Systematic Categorization, Construction and Evaluation of New Attacks against |
| C15 | 1 | both, against, increasing, each, input, indicates | The Crescendo Multi-Turn LLM Jailbreak Attack |
| C16 | 1 | collaborative, community, associated, emerging, technical, advancements | Characterizing Unintended Consequences in Human-GUI Agent |
| C17 | 1 | because, systems, deceptive, stakes, deployment, threats | Deceptive Alignment · Reverse Turing Test · Multi-Agent Systems · Prompt Injection · Agent Autonomy · Ethical |
| C18 | 1 | order, deceptive, others, refinement, sive, perspective | Boosting LLM Agents with Recursive Contemplation |
| C19 | 1 | door, likelihood, react, requests, subsequent, become | Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In |
| C20 | 1 |  | DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis |
| C21 | 1 | multi, cooperation, malicious, system, one, scenario | Multi-Agent Systems |
| C22 | 1 | jailbreak, adversary, multimodal, images, multi, any | Multimodal LLM Agents Exponentially Fast |
| C23 | 1 | mechanisms, bandwidth, multi, llama, systems, novel | Optimized Prompt Attacks |
| C24 | 1 | dangerous, multi, what, likely, negative, values | PsySafe: A Comprehensive Framework for Psychological-based Attack, |
| C25 | 1 | knowledge, sun, best, responses, known, memory | Synergistic Multi-Agent Framework with Trajectory Learning for |
| C26 | 1 | baselines, performance, attribute, caused, comparable, answers | Tool-Using Large Language Model Agents |
| C27 | 1 |  | Y ou Only Look at Screens: Multimodal Chain-of-Action Agents |
| C28 | 1 | threats, threat, 2025, affected, aspects, indicates | A Survey on the Safety and Security Threats of Computer-Using Agents |
| C29 | 1 | tools, when, given, ignore, modes, different | AGENT-SAFETY BENCH : Evaluating the Safety of |
| C30 | 1 | alignment, values, risks, test, scenarios, ethics | ALI-Agent: Assessing LLMs'Alignment with Human |
| C31 | 1 | addition, bugs, fixed, larger, introducing, iteratively | Agent Large Language Model |
| C32 | 1 | repair, bugs, tools, fixed, bug, michael | Agent for Program Repair |
| C33 | 1 | automation, knowledge, android, task, efforts, domain | Android task automation with 158 common tasks. The results |
| C34 | 1 | jailbreak, threats, jailbreaks, preventing, prompts, perspective | Cyber Defense Perspective |
| C35 | 1 | multimodal, adversarial, environment, chatbots, web, 2023 | DISSECTING ADVERSARIAL ROBUSTNESS OF |
| C36 | 1 | 100, achieves, table, benchmark, performance, normal | Dynamically Encrypted Multi-Backdoor Im- |
| C37 | 1 | gpt, forum, mini, 10k, adhere, 70b | Evaluating Cultural and Social Awareness of LLM Web Agents |
| C38 | 1 | social, choice, alignment, decision, setting, work | Existence of Probably Approximately Aligned Policies |
| C39 | 1 | react, retrieval, work, call, burden, cal | Exploring LLM-Based Agents for Root Cause Analysis |
| C40 | 1 |  | From Safety Benchmarks to Input Moderation |
| C41 | 1 | configurations, performance, sensitive, classifying, requirements, manual | Identifying Performance-Sensitive Configurations in Software |
| C42 | 1 | smart, edu, fine, cause, tuning, general | Intuitive Smart Contract Auditing with Justifications |
| C43 | 1 | biases, responsible, 2024, choice, equal, liu | LLM Agents Can Be Choice-Supportive Biased Evaluators: An Empirical Study |
| C44 | 1 | action, annotated, actions, operating, screen, often | LLM Agents are Susceptible to Environmental Distractions |
| C45 | 1 | larger, scale, different, table, performance, bers | LLM-based Multi-agent Systems |
| C46 | 1 | path, action, clean, activated, history, reflection | MLLM-Powered Mobile GUI Agents |
| C47 | 1 | chain, business, quantitative, complexity, steps, tasks | Mandarin apps. This paper introduces MobileFlow, a multimodal large language model meticulously |
| C48 | 1 | collaboration, multi, gpt, backbone, structure, 2024 | On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents |
| C49 | 1 | knowledge, action, plan, adversarial, benign, carefully | Poisoning Memory or Knowledge Bases |
| C50 | 1 | clean, detector, github, human, actions, critical | Preemptive Detection and Correction of Misaligned Actions in LLM Agents |
| C51 | 1 | effective, against, leveraging, powerful, shelf, yet | PromptArmor: Simple yet Effective Prompt Injection Defenses |
| C52 | 1 | harmful, 2023, behaviors, browser, chatbots, reinforcement | Refusal-Trained LLMs Are Easily Jailbroken |
| C53 | 1 | rule, each, specifically, action, algorithm, categories | SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning |
| C54 | 1 | pass, value, when, changes, see, similar | Software Process Models Using Large Language |
| C55 | 1 | task, while, signi, sources, tion, user | The Task Shield: Enforcing Task Alignment to Defend Against Indirect |
| C56 | 1 |  | Towards LLM-augmented multiagent systems for agile soware |
| C57 | 1 |  | Towards Trustworthy GUI Agents: A Survey |
| C58 | 1 |  | Unveiling Privacy Risks in LLM Agent Memory |
| C59 | 1 | specific, back, wang, backdoors, door, triggers | Y our Agent Can Defend Itself against Backdoor Attacks |
| C60 | 1 | think, put, react, code, here, codes | evaluation of unsafe code generation and execution, diverse input formats, and high- |
| C61 | 1 |  | A Multi-agent Onboarding Assistant based on Large Language |
| C62 | 1 | recall, mode, precision, score, accuracy, gpt | Advanced Smart Contract Vulnerability Detection |
| C63 | 1 | network, manually, injected, assess, data, bandwidth | AgentFM: Role-Aware Failure Management for Distributed Databases |
| C64 | 1 | ieee, jailbreak, debate, iterative, vulnerabilities, role | Amplified Vulnerabilities: Structured Jailbreak Attacks |
| C65 | 1 | architecture, requirements, functional, quality, non, require | Architecture Design |
| C66 | 1 | risk, scenarios, confirm, improvements, reflection, experimental | Automated Risk Simulator |
| C67 | 1 |  | Autonomous agents in soware development for information retrieval |
| C68 | 1 | repository, module, review, summary, aims, structure | Comprehensive Repository Exploration |
| C69 | 1 | form, oversight, multi, limitations, threat, interactions | Deception via Steganography |
| C70 | 1 | work, related, behavior, fine, generation, authors | Enhancing Game AI Behaviors with Large Language Models and |
| C71 | 1 |  | Enhancing Human-IDE Interaction in the SDLC using LLM-based |
| C72 | 1 |  | Facilitating Trustworthy Human-Agent Collaboration in |
| C73 | 1 | testing, prompts, guided, inputs, efficiently, process | Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework |
| C74 | 1 | various, compared, software, amount, optimize, baselines | GitHub README.MD Summarization |
| C75 | 1 | risk, task, static, planning, perception, framework | Graphormer-Guided Task Planning: Beyond Static Rules with LLM |
| C76 | 1 | attention, injections, interface, goal, gui, text | Indirect Prompt Injection |
| C77 | 1 | principles, utility, design, benign, secure, chen | LLM Agents Should Employ Security Principles |
| C78 | 1 | testing, scenarios, developers, university, computer, test | LLM-Agents Driven Automated Simulation Testing |
| C79 | 1 |  | Mediating between Human Programmers and Integrated |
| C80 | 1 | error, 2018, free, under, 2017, bug | Mimicking Human Expertise |
| C81 | 1 | chat, game, agile, safe, environments, spatial | Optimal Multi-Agent Navigation in Constrained Environments |
| C82 | 1 |  | Poster: Agentic Shell Honeypot Using Structured Logging |
| C83 | 1 | task, tokens, accuracy, added, context, fake | Proactive Defenses Against LLM Agents |
| C84 | 1 | test, gui, tests, natural, cases, maintain | ProphetAgent: Automatically Synthesizing GUI Tests from Test |
| C85 | 1 | 100, description, calls, tool, recent, amount | RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage |
| C86 | 1 | 100, unsafe, rate, completion, mini, applications | Safety of Computer Use Agents |
| C87 | 1 |  | Security Risks of Mobile LLM Agents |
| C88 | 1 | code, users, various, data, answering, build | Taint-Style Vulnerabilities in LLM-based Agents |
| C89 | 1 | perception, stage, browser, scale, data, elements | The Hidden Dangers of Browsing AI Agents |
| C90 | 1 |  | Towards Adaptive Software Agents for Debugging |
| C91 | 1 | embedded, gui, vulnerabilities, deceptive, evaluated, ios | Vulnerability to Fine-Print Injections |
| C92 | 1 | apps, execution, natural, systems, architecture, third | design architecture that demonstrates the feasibility of execution |

### 方法标签分布（Top 20）
| 方法标签组合 | 规模 | 代表论文（节选） |
|---|---:|---|
| 未识别方法标签 | 7 | TrustAgent: Towards Safe and Trustworthy LLM-based Agents, Agent for Program Repair, AirGapAgent: Protecting Privacy-Conscious Conversational Agents, Intuitive Smart Contract Auditing with Justifications, Unity Is Strength: Collaborative LLM-Based Agents for Code Reviewer, Autonomous agents in soware development for information retrieval, Comprehensive Repository Exploration |
| 基准/Benchmark / 多智能体/Multi-Agent | 5 | Agent Large Language Model, On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents, Software Process Models Using Large Language, Deception via Steganography, GitHub README.MD Summarization |
| 仿真/Simulator / 基准/Benchmark / 多模态/Multimodal / 调查/Survey | 4 | FOR MULTIMODAL GUI- ORIENTED UNDERSTANDING, LLM Agents are Susceptible to Environmental Distractions, Progress and Prospects, Towards Trustworthy GUI Agents: A Survey |
| 仿真/Simulator / 多智能体/Multi-Agent | 4 | Preemptive Detection and Correction of Misaligned Actions in LLM Agents, Towards LLM-augmented multiagent systems for agile soware, Mediating between Human Programmers and Integrated, Optimal Multi-Agent Navigation in Constrained Environments |
| 基准/Benchmark | 4 | ALI-Agent: Assessing LLMs'Alignment with Human, Exploring LLM-Based Agents for Root Cause Analysis, Identifying Performance-Sensitive Configurations in Software, LLM Agents Can Be Choice-Supportive Biased Evaluators: An Empirical Study |
| 仿真/Simulator / 基准/Benchmark | 3 | AGENT-SAFETY BENCH : Evaluating the Safety of, evaluation of unsafe code generation and execution, diverse input formats, and high-, Taint-Style Vulnerabilities in LLM-based Agents |
| 仿真/Simulator / 基准/Benchmark / 多模态/Multimodal | 3 | Enhancing LLM Agent Safety via Causal Influence Prompting, Multimodal Web Agents, Graphormer-Guided Task Planning: Beyond Static Rules with LLM |
| 仿真/Simulator / 基准/Benchmark / 系统防御/Defense | 3 | From Safety Benchmarks to Input Moderation, SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning, Framework for Computer-Use Agents |
| 仿真/Simulator / 多智能体/Multi-Agent / 多模态/Multimodal | 3 | Characterizing Unintended Consequences in Human-GUI Agent, A Multi-agent Onboarding Assistant based on Large Language, Enhancing Human-IDE Interaction in the SDLC using LLM-based |
| 基准/Benchmark / 多智能体/Multi-Agent / 多模态/Multimodal | 3 | Boosting LLM Agents with Recursive Contemplation, AgentFM: Role-Aware Failure Management for Distributed Databases, Facilitating Trustworthy Human-Agent Collaboration in |
| 基准/Benchmark / 多模态/Multimodal | 3 | Attacking Vision-Language Computer Agents via Pop-ups, Evaluating Cultural and Social Awareness of LLM Web Agents, Systematic Categorization, Construction and Evaluation of New Attacks against |
| 多智能体/Multi-Agent | 3 | Synergistic Multi-Agent Framework with Trajectory Learning for, Advanced Smart Contract Vulnerability Detection, Architecture Design |
| 仿真/Simulator / 基准/Benchmark / 多模态/Multimodal / 提示注入/Prompt Injection / 系统防御/Defense | 2 | Prompt Injection Attacks and Defenses, Environmental Injection Attacks |
| 仿真/Simulator / 基准/Benchmark / 多模态/Multimodal / 系统防御/Defense | 2 | DISSECTING ADVERSARIAL ROBUSTNESS OF, Automated Risk Simulator |
| 仿真/Simulator / 基准/Benchmark / 提示注入/Prompt Injection | 2 | Prompt Injection Attacks, Safety of Computer Use Agents |
| 仿真/Simulator / 基准/Benchmark / 提示注入/Prompt Injection / 红队/Red-Teaming | 2 | AGENT VIGIL : Generic Black-Box Red-teaming for Indirect Prompt Injection, testing in realistic but controlled environments or ignore hybrid web-OS attack |
| 仿真/Simulator / 多模态/Multimodal | 2 | Y ou Only Look at Screens: Multimodal Chain-of-Action Agents, Unveiling Privacy Risks in LLM Agent Memory |
| 仿真/Simulator / 提示注入/Prompt Injection / 系统防御/Defense | 2 | introduce a novel threat, WIPI, that indirectly controls Web, RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage |
| 基准/Benchmark / 多模态/Multimodal / 提示注入/Prompt Injection | 2 | Tool-Integrated Large Language Model Agents, Vulnerability to Fine-Print Injections |
| 多智能体/Multi-Agent / 形式化/Optimization | 2 | AGENT OCCAM : A S IMPLE YET STRONG BASELINE, Issue-Oriented Code Review |

### 近五年关键词演变与突现词
统计窗口：2022–2026。下表给出 top-10 高频关键词的逐年频次（仅统计年份可抽取且落入窗口的论文）；突现词依据相邻年度显著增量启发式识别。

| 关键词 | 2022 | 2023 | 2024 | 2025 | 2026 | 突现 |
|---|---:|---:|---:|---:|---:|---|
| 2024 | 0 | 4 | 36 | 4 | 0 | 是 || 2023 | 0 | 16 | 13 | 0 | 0 | 是 || user | 0 | 3 | 12 | 14 | 0 | 是 || 2025 | 0 | 0 | 6 | 14 | 0 | 是 || prompt | 0 | 3 | 9 | 8 | 0 | 是 || multi | 1 | 7 | 5 | 6 | 0 | 是 || code | 0 | 0 | 9 | 7 | 0 | 是 || acm | 0 | 0 | 6 | 9 | 0 | 是 || software | 0 | 0 | 4 | 11 | 0 | 是 || data | 0 | 3 | 5 | 6 | 0 | 是 |

## 研究空白
以下空白从理论、方法、数据、场景四维归纳，均给出潜在研究方向与可行性评级（高/中/低）。

| 维度 | 空白陈述 | 潜在研究方向 | 可行性 |
|---|---|---|---|
| 理论 | 间接提示注入的威胁模型仍偏“输入层”，缺少对“代理决策—工具调用—环境反馈”闭环中的可证伪因果解释。 | 构建可解释的因果图/机制模型，将注入信号与行为偏移建立可检验关系；结合反事实评估。 | 中 || 理论 | 多智能体系统的对抗传播研究多聚焦通信拓扑与提示分发，缺少与组织结构/角色分工一致的理论框架。 | 以组织理论/协同决策为基础，形式化角色、信任与权限边界，推导安全性约束。 | 中 || 方法 | 防御方法常以过滤/拒答为主，缺少“任务对齐可证明”的端到端策略合成与验证流程。 | 将安全策略写成可验证约束（policy-as-code），用模型检验/符号执行验证关键不变式。 | 中 || 方法 | 红队评测多是离线静态提示集合，缺少能覆盖真实网页/GUI微扰与多轮交互的动态生成式攻击基准。 | 用交互式生成器合成环境注入（广告/弹窗/细字），并对轨迹进行自动标注与复现。 | 高 || 数据 | 公开数据集对“真实世界网页内容分布”和“工具链多样性”覆盖不足，导致防御泛化性难评估。 | 采集多域网页快照与工具调用日志，设计隐私合规的数据脱敏与发布协议。 | 中 || 数据 | 对后门/投毒研究而言，缺少可复现实验所需的训练数据、触发器设计空间与检测基线的统一封装。 | 建立投毒/触发器库与检测任务套件（含可控强度、跨模型迁移），提供统一接口。 | 中 || 场景 | 研究多集中在 web 与移动端 GUI，针对企业内部业务流（审批、财务、运维）的代理安全研究不足。 | 构建企业流程的仿真环境与合成数据，评测权限边界、审计日志与最小授权策略。 | 中 || 场景 | 对 Web3/智能合约代理的“上下文操控+工具链执行”复合攻击研究仍少，缺少行业级基准。 | 建立链上/链下混合环境，评测记忆污染、交易诱导与风险阈值策略。 | 中 || 理论 | 现有工作对“代理自我纠错/反思”在对抗场景中的失效边界缺少系统化刻画。 | 设计对抗下的反思鲁棒性度量与消融，分解错误源（感知/规划/执行）。 | 中 || 方法 | 多模态环境注入（细字、视觉后门、弹窗）的防御多依赖启发式，缺少跨分辨率、跨UI风格的稳健方案。 | 结合视觉不变特征与语义一致性检测，加入跨UI域自适应训练与不确定性估计。 | 高 || 数据 | 缺少覆盖多语言、多文化网页内容的安全评测数据，导致跨区域部署风险被低估。 | 构建多语言网页注入与社会文化敏感内容的评测集，增加区域化风险标签。 | 中 || 场景 | 对“代理与人协作”的混合模式下，缺少针对社会工程+注入的联合建模与防线设计。 | 引入人机协作协议与确认机制，评测不同提示/界面设计对误操作率的影响。 | 高 |

## 未来展望
未来工作可围绕三条主线推进：其一，将“安全策略”从对话层过滤升级为对轨迹与副作用的可验证约束（含最小权限、可审计执行与失败回滚）；其二，构建覆盖真实环境注入与多轮交互的动态基准，推动防御在跨网页/跨UI/跨模型下的泛化评估；其三，在人机协作与多智能体组织化场景中引入协议与治理机制，使系统在面对社会工程、上下文操控与记忆污染时仍具备可解释、可追溯、可恢复的安全能力。

## 人工校验提示（置信度 < 阈值）
以下字段的自动抽取置信度低于阈值。建议打开对应 txt 文件，跳转到给定页码/段落号附近进行人工确认；若需修正，可在输出 Markdown 中直接编辑对应条目。

- Great, Now Write an Article About That- The Crescendo Multi-Turn LLM Jailbreak Attack.txt｜发表年份｜conf=0.40｜定位：p.? @段1
- Great, Now Write an Article About That- The Crescendo Multi-Turn LLM Jailbreak Attack.txt｜期刊/会议｜conf=0.45｜定位：p.? @段1
- Great, Now Write an Article About That- The Crescendo Multi-Turn LLM Jailbreak Attack.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Great, Now Write an Article About That- The Crescendo Multi-Turn LLM Jailbreak Attack.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Great, Now Write an Article About That- The Crescendo Multi-Turn LLM Jailbreak Attack.txt｜研究方法｜conf=0.82｜定位：p.? @段11
- Great, Now Write an Article About That- The Crescendo Multi-Turn LLM Jailbreak Attack.txt｜主要结论｜conf=0.80｜定位：p.? @段54
- Characterizing unintended consequences in human-GUI agent collaboration for web browsing.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Characterizing unintended consequences in human-GUI agent collaboration for web browsing.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Characterizing unintended consequences in human-GUI agent collaboration for web browsing.txt｜研究方法｜conf=0.82｜定位：p.? @段5
- Characterizing unintended consequences in human-GUI agent collaboration for web browsing.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- Compromising embodied agents with contextual backdoor attacks.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Compromising embodied agents with contextual backdoor attacks.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Compromising embodied agents with contextual backdoor attacks.txt｜研究方法｜conf=0.82｜定位：p.? @段4
- Compromising embodied agents with contextual backdoor attacks.txt｜主要结论｜conf=0.80｜定位：p.? @段57
- Guardians of the agentic system Preventing many shots jailbreak with agentic system.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Guardians of the agentic system Preventing many shots jailbreak with agentic system.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Guardians of the agentic system Preventing many shots jailbreak with agentic system.txt｜研究方法｜conf=0.82｜定位：p.? @段6
- Guardians of the agentic system Preventing many shots jailbreak with agentic system.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- AgentVigil Generic black-box red-teaming for indirect prompt injection against LLM agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- AgentVigil Generic black-box red-teaming for indirect prompt injection against LLM agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- AgentVigil Generic black-box red-teaming for indirect prompt injection against LLM agents.txt｜研究方法｜conf=0.82｜定位：p.? @段2
- AgentVigil Generic black-box red-teaming for indirect prompt injection against LLM agents.txt｜主要结论｜conf=0.80｜定位：p.? @段4
- BadAgent Inserting and activating backdoor attacks in LLM agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- BadAgent Inserting and activating backdoor attacks in LLM agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- BadAgent Inserting and activating backdoor attacks in LLM agents.txt｜研究方法｜conf=0.82｜定位：p.? @段16
- BadAgent Inserting and activating backdoor attacks in LLM agents.txt｜主要结论｜conf=0.80｜定位：p.? @段16
- Boosting LLM agents with recursive contemplation for effective deception handling.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Boosting LLM agents with recursive contemplation for effective deception handling.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Boosting LLM agents with recursive contemplation for effective deception handling.txt｜研究方法｜conf=0.82｜定位：p.? @段5
- Boosting LLM agents with recursive contemplation for effective deception handling.txt｜主要结论｜conf=0.80｜定位：p.? @段4
- Breaking ReAct agents Foot-in-the-door attack will get you in.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Breaking ReAct agents Foot-in-the-door attack will get you in.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Breaking ReAct agents Foot-in-the-door attack will get you in.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- Breaking ReAct agents Foot-in-the-door attack will get you in.txt｜主要结论｜conf=0.80｜定位：p.? @段2
- DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis.txt｜标题｜conf=0.80｜定位：p.? @段1
- DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis.txt｜关键词｜conf=0.78｜定位：p.? @段1
- DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis.txt｜摘要｜conf=0.72｜定位：p.? @段1
- DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis.txt｜主要结论｜conf=0.50｜定位：p.? @段1
- Evil geniuses Delving into the safety of LLM-based agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Evil geniuses Delving into the safety of LLM-based agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Evil geniuses Delving into the safety of LLM-based agents.txt｜研究方法｜conf=0.82｜定位：p.? @段2
- Evil geniuses Delving into the safety of LLM-based agents.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- EIA Environmental injection attack on generalist web agents for privacy leakage.txt｜关键词｜conf=0.78｜定位：p.? @段1
- EIA Environmental injection attack on generalist web agents for privacy leakage.txt｜摘要｜conf=0.72｜定位：p.? @段1
- EIA Environmental injection attack on generalist web agents for privacy leakage.txt｜研究方法｜conf=0.82｜定位：p.? @段9
- EIA Environmental injection attack on generalist web agents for privacy leakage.txt｜主要结论｜conf=0.80｜定位：p.? @段18
- LLM agents can autonomously hack websites.txt｜关键词｜conf=0.78｜定位：p.? @段1
- LLM agents can autonomously hack websites.txt｜摘要｜conf=0.72｜定位：p.? @段1
- LLM agents can autonomously hack websites.txt｜研究方法｜conf=0.50｜定位：p.? @段1
- LLM agents can autonomously hack websites.txt｜主要结论｜conf=0.80｜定位：p.? @段2
- A trembling house of cards Mapping adversarial attacks against language agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- A trembling house of cards Mapping adversarial attacks against language agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- A trembling house of cards Mapping adversarial attacks against language agents.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- A trembling house of cards Mapping adversarial attacks against language agents.txt｜主要结论｜conf=0.80｜定位：p.? @段17
- Multi-agent security tax Trading off security and collaboration capabilities in multi-agent systems.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Multi-agent security tax Trading off security and collaboration capabilities in multi-agent systems.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Multi-agent security tax Trading off security and collaboration capabilities in multi-agent systems.txt｜研究方法｜conf=0.82｜定位：p.? @段10
- Multi-agent security tax Trading off security and collaboration capabilities in multi-agent systems.txt｜主要结论｜conf=0.80｜定位：p.? @段20
- Agent smith A single image can jailbreak one million multimodal LLM agents exponentially fast.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Agent smith A single image can jailbreak one million multimodal LLM agents exponentially fast.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Agent smith A single image can jailbreak one million multimodal LLM agents exponentially fast.txt｜研究方法｜conf=0.82｜定位：p.? @段56
- Agent smith A single image can jailbreak one million multimodal LLM agents exponentially fast.txt｜主要结论｜conf=0.80｜定位：p.? @段2
- Agents under siege Breaking pragmatic multi-agent LLM systems with optimized prompt attacks.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Agents under siege Breaking pragmatic multi-agent LLM systems with optimized prompt attacks.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Agents under siege Breaking pragmatic multi-agent LLM systems with optimized prompt attacks.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- Agents under siege Breaking pragmatic multi-agent LLM systems with optimized prompt attacks.txt｜主要结论｜conf=0.80｜定位：p.? @段2
- AutoDAN Generating stealthy jailbreak prompts on aligned large language models.txt｜关键词｜conf=0.78｜定位：p.? @段1
- AutoDAN Generating stealthy jailbreak prompts on aligned large language models.txt｜摘要｜conf=0.72｜定位：p.? @段1
- AutoDAN Generating stealthy jailbreak prompts on aligned large language models.txt｜研究方法｜conf=0.82｜定位：p.? @段17
- AutoDAN Generating stealthy jailbreak prompts on aligned large language models.txt｜主要结论｜conf=0.80｜定位：p.? @段17
- PsySafe A comprehensive framework for psychological-based attack, defense, and evaluation of multi-.txt｜关键词｜conf=0.78｜定位：p.? @段1
- PsySafe A comprehensive framework for psychological-based attack, defense, and evaluation of multi-.txt｜摘要｜conf=0.72｜定位：p.? @段1
- PsySafe A comprehensive framework for psychological-based attack, defense, and evaluation of multi-.txt｜研究方法｜conf=0.82｜定位：p.? @段6
- PsySafe A comprehensive framework for psychological-based attack, defense, and evaluation of multi-.txt｜主要结论｜conf=0.80｜定位：p.? @段9
- Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks.txt｜研究方法｜conf=0.82｜定位：p.? @段5
- Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks.txt｜主要结论｜conf=0.80｜定位：p.? @段5
- InjecAgent Benchmarking indirect prompt injections in tool-integrated large language model agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- InjecAgent Benchmarking indirect prompt injections in tool-integrated large language model agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- InjecAgent Benchmarking indirect prompt injections in tool-integrated large language model agents.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- InjecAgent Benchmarking indirect prompt injections in tool-integrated large language model agents.txt｜主要结论｜conf=0.80｜定位：p.? @段7
- PrivacyAsst Safeguarding user privacy in tool-using large language model agents.txt｜期刊/会议｜conf=0.45｜定位：p.? @段1
- PrivacyAsst Safeguarding user privacy in tool-using large language model agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- PrivacyAsst Safeguarding user privacy in tool-using large language model agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- PrivacyAsst Safeguarding user privacy in tool-using large language model agents.txt｜研究方法｜conf=0.82｜定位：p.? @段2
- PrivacyAsst Safeguarding user privacy in tool-using large language model agents.txt｜主要结论｜conf=0.80｜定位：p.? @段28
- TrustAgent Towards safe and trustworthy LLM-based agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- TrustAgent Towards safe and trustworthy LLM-based agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- TrustAgent Towards safe and trustworthy LLM-based agents.txt｜研究方法｜conf=0.82｜定位：p.? @段4
- TrustAgent Towards safe and trustworthy LLM-based agents.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- You only look at screens Multimodal chain-of-action agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- You only look at screens Multimodal chain-of-action agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- You only look at screens Multimodal chain-of-action agents.txt｜研究方法｜conf=0.82｜定位：p.? @段4
- You only look at screens Multimodal chain-of-action agents.txt｜主要结论｜conf=0.50｜定位：p.? @段1
- JARVIS or Ultron A Survey on the Safety and Security Threats of Computer-Using Agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- JARVIS or Ultron A Survey on the Safety and Security Threats of Computer-Using Agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- JARVIS or Ultron A Survey on the Safety and Security Threats of Computer-Using Agents.txt｜研究方法｜conf=0.82｜定位：p.? @段5
- JARVIS or Ultron A Survey on the Safety and Security Threats of Computer-Using Agents.txt｜主要结论｜conf=0.80｜定位：p.? @段8
- AgentOccam A simple yet strong baseline for LLM-based web agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- AgentOccam A simple yet strong baseline for LLM-based web agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- AgentOccam A simple yet strong baseline for LLM-based web agents.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- AgentOccam A simple yet strong baseline for LLM-based web agents.txt｜主要结论｜conf=0.80｜定位：p.? @段12
- Agent-SafetyBench Evaluating the safety of LLM agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Agent-SafetyBench Evaluating the safety of LLM agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Agent-SafetyBench Evaluating the safety of LLM agents.txt｜研究方法｜conf=0.82｜定位：p.? @段6
- Agent-SafetyBench Evaluating the safety of LLM agents.txt｜主要结论｜conf=0.80｜定位：p.? @段21
- AGrail A lifelong agent guardrail with effective and adaptive safety detection.txt｜关键词｜conf=0.78｜定位：p.? @段1
- AGrail A lifelong agent guardrail with effective and adaptive safety detection.txt｜摘要｜conf=0.72｜定位：p.? @段1
- AGrail A lifelong agent guardrail with effective and adaptive safety detection.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- AGrail A lifelong agent guardrail with effective and adaptive safety detection.txt｜主要结论｜conf=0.80｜定位：p.? @段25
- ALI-agent Assessing LLMs'alignment with human values via agent-based evaluation.txt｜关键词｜conf=0.78｜定位：p.? @段1
- ALI-agent Assessing LLMs'alignment with human values via agent-based evaluation.txt｜摘要｜conf=0.72｜定位：p.? @段1
- ALI-agent Assessing LLMs'alignment with human values via agent-based evaluation.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- ALI-agent Assessing LLMs'alignment with human values via agent-based evaluation.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- Adaptive attacks break defenses against indirect prompt injection attacks on LLM agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Adaptive attacks break defenses against indirect prompt injection attacks on LLM agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Adaptive attacks break defenses against indirect prompt injection attacks on LLM agents.txt｜研究方法｜conf=0.82｜定位：p.? @段9
- Adaptive attacks break defenses against indirect prompt injection attacks on LLM agents.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- AdvAgent Controllable blackbox red-teaming on web agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- AdvAgent Controllable blackbox red-teaming on web agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- AdvAgent Controllable blackbox red-teaming on web agents.txt｜研究方法｜conf=0.82｜定位：p.? @段2
- AdvAgent Controllable blackbox red-teaming on web agents.txt｜主要结论｜conf=0.80｜定位：p.? @段2
- Poster Repairing bugs with the introduction of new variables a multi-agent large language model.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Poster Repairing bugs with the introduction of new variables a multi-agent large language model.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Poster Repairing bugs with the introduction of new variables a multi-agent large language model.txt｜研究方法｜conf=0.50｜定位：p.? @段1
- Poster Repairing bugs with the introduction of new variables a multi-agent large language model.txt｜主要结论｜conf=0.80｜定位：p.? @段20
- RepairAgent An autonomous, LLM-based agent for program repair.txt｜关键词｜conf=0.78｜定位：p.? @段1
- RepairAgent An autonomous, LLM-based agent for program repair.txt｜摘要｜conf=0.72｜定位：p.? @段1
- RepairAgent An autonomous, LLM-based agent for program repair.txt｜研究方法｜conf=0.82｜定位：p.? @段1
- RepairAgent An autonomous, LLM-based agent for program repair.txt｜主要结论｜conf=0.80｜定位：p.? @段1
- UDora A unified red teaming framework against LLM agents by dynamically hijacking their own reasoni.txt｜关键词｜conf=0.78｜定位：p.? @段1
- UDora A unified red teaming framework against LLM agents by dynamically hijacking their own reasoni.txt｜摘要｜conf=0.72｜定位：p.? @段1
- UDora A unified red teaming framework against LLM agents by dynamically hijacking their own reasoni.txt｜研究方法｜conf=0.82｜定位：p.? @段2
- UDora A unified red teaming framework against LLM agents by dynamically hijacking their own reasoni.txt｜主要结论｜conf=0.80｜定位：p.? @段14
- AirGapAgent Protecting privacy-conscious conversational agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- AirGapAgent Protecting privacy-conscious conversational agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- AirGapAgent Protecting privacy-conscious conversational agents.txt｜研究方法｜conf=0.82｜定位：p.? @段7
- AirGapAgent Protecting privacy-conscious conversational agents.txt｜主要结论｜conf=0.80｜定位：p.? @段7
- AutoDroid LLM-powered task automation in android.txt｜关键词｜conf=0.78｜定位：p.? @段1
- AutoDroid LLM-powered task automation in android.txt｜摘要｜conf=0.72｜定位：p.? @段1
- AutoDroid LLM-powered task automation in android.txt｜研究方法｜conf=0.82｜定位：p.? @段6
- AutoDroid LLM-powered task automation in android.txt｜主要结论｜conf=0.80｜定位：p.? @段4
- Attacking vision-language computer agents via pop-ups.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Attacking vision-language computer agents via pop-ups.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Attacking vision-language computer agents via pop-ups.txt｜研究方法｜conf=0.82｜定位：p.? @段4
- Attacking vision-language computer agents via pop-ups.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- MELON Provable defense against indirect prompt injection attacks in AI agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- MELON Provable defense against indirect prompt injection attacks in AI agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- MELON Provable defense against indirect prompt injection attacks in AI agents.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- MELON Provable defense against indirect prompt injection attacks in AI agents.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- Preventing jailbreak prompts as malicious tools for cybercriminals A cyber defense perspective.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Preventing jailbreak prompts as malicious tools for cybercriminals A cyber defense perspective.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Preventing jailbreak prompts as malicious tools for cybercriminals A cyber defense perspective.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- Preventing jailbreak prompts as malicious tools for cybercriminals A cyber defense perspective.txt｜主要结论｜conf=0.80｜定位：p.? @段5
- Dissecting adversarial robustness of multimodal LM agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Dissecting adversarial robustness of multimodal LM agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Dissecting adversarial robustness of multimodal LM agents.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- Dissecting adversarial robustness of multimodal LM agents.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- DemonAgent Dynamically encrypted multi-backdoor implantation attack on LLM-based agent.txt｜关键词｜conf=0.78｜定位：p.? @段1
- DemonAgent Dynamically encrypted multi-backdoor implantation attack on LLM-based agent.txt｜摘要｜conf=0.72｜定位：p.? @段1
- DemonAgent Dynamically encrypted multi-backdoor implantation attack on LLM-based agent.txt｜研究方法｜conf=0.82｜定位：p.? @段9
- DemonAgent Dynamically encrypted multi-backdoor implantation attack on LLM-based agent.txt｜主要结论｜conf=0.80｜定位：p.? @段36
- Enhancing LLM agent safety via causal influence prompting.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Enhancing LLM agent safety via causal influence prompting.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Enhancing LLM agent safety via causal influence prompting.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- Enhancing LLM agent safety via causal influence prompting.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- Evaluating cultural and social awareness of LLM web agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Evaluating cultural and social awareness of LLM web agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Evaluating cultural and social awareness of LLM web agents.txt｜研究方法｜conf=0.82｜定位：p.? @段8
- Evaluating cultural and social awareness of LLM web agents.txt｜主要结论｜conf=0.80｜定位：p.? @段17
- DoomArena A framework for testing AI agents against evolving security threats.txt｜关键词｜conf=0.78｜定位：p.? @段1
- DoomArena A framework for testing AI agents against evolving security threats.txt｜摘要｜conf=0.72｜定位：p.? @段1
- DoomArena A framework for testing AI agents against evolving security threats.txt｜研究方法｜conf=0.82｜定位：p.? @段16
- DoomArena A framework for testing AI agents against evolving security threats.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- Can an AI agent safely run a government Existence of probably approximately aligned policies.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Can an AI agent safely run a government Existence of probably approximately aligned policies.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Can an AI agent safely run a government Existence of probably approximately aligned policies.txt｜研究方法｜conf=0.82｜定位：p.? @段2
- Can an AI agent safely run a government Existence of probably approximately aligned policies.txt｜主要结论｜conf=0.80｜定位：p.? @段19
- Exploring LLM-based agents for root cause analysis.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Exploring LLM-based agents for root cause analysis.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Exploring LLM-based agents for root cause analysis.txt｜研究方法｜conf=0.82｜定位：p.? @段7
- Exploring LLM-based agents for root cause analysis.txt｜主要结论｜conf=0.80｜定位：p.? @段4
- GUI-world A video benchmark and dataset for multimodal GUI-oriented understanding.txt｜关键词｜conf=0.78｜定位：p.? @段1
- GUI-world A video benchmark and dataset for multimodal GUI-oriented understanding.txt｜摘要｜conf=0.72｜定位：p.? @段1
- GUI-world A video benchmark and dataset for multimodal GUI-oriented understanding.txt｜研究方法｜conf=0.82｜定位：p.? @段5
- GUI-world A video benchmark and dataset for multimodal GUI-oriented understanding.txt｜主要结论｜conf=0.80｜定位：p.? @段14
- Advancing embodied agent security From safety benchmarks to input moderation.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Advancing embodied agent security From safety benchmarks to input moderation.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Advancing embodied agent security From safety benchmarks to input moderation.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- Advancing embodied agent security From safety benchmarks to input moderation.txt｜主要结论｜conf=0.50｜定位：p.? @段1
- Identifying performance-sensitive configurations in software systems through code analysis with LLM.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Identifying performance-sensitive configurations in software systems through code analysis with LLM.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Identifying performance-sensitive configurations in software systems through code analysis with LLM.txt｜研究方法｜conf=0.82｜定位：p.? @段5
- Identifying performance-sensitive configurations in software systems through code analysis with LLM.txt｜主要结论｜conf=0.80｜定位：p.? @段2
- Combining fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Combining fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Combining fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications.txt｜研究方法｜conf=0.82｜定位：p.? @段7
- Combining fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications.txt｜主要结论｜conf=0.80｜定位：p.? @段4
- AutoDefense Multi-agent LLM defense against jailbreak attacks.txt｜关键词｜conf=0.78｜定位：p.? @段1
- AutoDefense Multi-agent LLM defense against jailbreak attacks.txt｜摘要｜conf=0.72｜定位：p.? @段1
- AutoDefense Multi-agent LLM defense against jailbreak attacks.txt｜研究方法｜conf=0.82｜定位：p.? @段14
- AutoDefense Multi-agent LLM defense against jailbreak attacks.txt｜主要结论｜conf=0.80｜定位：p.? @段12
- LLM agents can be choice-supportive biased evaluators An empirical study.txt｜关键词｜conf=0.78｜定位：p.? @段1
- LLM agents can be choice-supportive biased evaluators An empirical study.txt｜摘要｜conf=0.72｜定位：p.? @段1
- LLM agents can be choice-supportive biased evaluators An empirical study.txt｜研究方法｜conf=0.50｜定位：p.? @段1
- LLM agents can be choice-supportive biased evaluators An empirical study.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- Caution for the environment Multimodal LLM agents are susceptible to environmental distractions.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Caution for the environment Multimodal LLM agents are susceptible to environmental distractions.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Caution for the environment Multimodal LLM agents are susceptible to environmental distractions.txt｜研究方法｜conf=0.82｜定位：p.? @段9
- Caution for the environment Multimodal LLM agents are susceptible to environmental distractions.txt｜主要结论｜conf=0.80｜定位：p.? @段10
- G-safeguard A topology-guided security lens and treatment on LLM-based multi-agent systems.txt｜关键词｜conf=0.78｜定位：p.? @段1
- G-safeguard A topology-guided security lens and treatment on LLM-based multi-agent systems.txt｜摘要｜conf=0.72｜定位：p.? @段1
- G-safeguard A topology-guided security lens and treatment on LLM-based multi-agent systems.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- G-safeguard A topology-guided security lens and treatment on LLM-based multi-agent systems.txt｜主要结论｜conf=0.80｜定位：p.? @段18
- Hidden ghost hand Unveiling backdoor vulnerabilities in MLLM-powered mobile GUI agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Hidden ghost hand Unveiling backdoor vulnerabilities in MLLM-powered mobile GUI agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Hidden ghost hand Unveiling backdoor vulnerabilities in MLLM-powered mobile GUI agents.txt｜研究方法｜conf=0.82｜定位：p.? @段2
- Hidden ghost hand Unveiling backdoor vulnerabilities in MLLM-powered mobile GUI agents.txt｜主要结论｜conf=0.80｜定位：p.? @段28
- MobileFlow A multimodal LLM for mobile GUI agent.txt｜关键词｜conf=0.78｜定位：p.? @段1
- MobileFlow A multimodal LLM for mobile GUI agent.txt｜摘要｜conf=0.72｜定位：p.? @段1
- MobileFlow A multimodal LLM for mobile GUI agent.txt｜研究方法｜conf=0.82｜定位：p.? @段12
- MobileFlow A multimodal LLM for mobile GUI agent.txt｜主要结论｜conf=0.80｜定位：p.? @段10
- Explorer Scaling exploration-driven web trajectory synthesis for multimodal web agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Explorer Scaling exploration-driven web trajectory synthesis for multimodal web agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Explorer Scaling exploration-driven web trajectory synthesis for multimodal web agents.txt｜研究方法｜conf=0.82｜定位：p.? @段4
- Explorer Scaling exploration-driven web trajectory synthesis for multimodal web agents.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- On the resilience of LLM-based multi-agent collaboration with faulty agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- On the resilience of LLM-based multi-agent collaboration with faulty agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- On the resilience of LLM-based multi-agent collaboration with faulty agents.txt｜研究方法｜conf=0.82｜定位：p.? @段2
- On the resilience of LLM-based multi-agent collaboration with faulty agents.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- Poison once, control anywhere Clean-text visual backdoors in VLM-based mobile agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Poison once, control anywhere Clean-text visual backdoors in VLM-based mobile agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Poison once, control anywhere Clean-text visual backdoors in VLM-based mobile agents.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- Poison once, control anywhere Clean-text visual backdoors in VLM-based mobile agents.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- AGENTPOISON Red-teaming LLM agents via poisoning memory or knowledge bases.txt｜关键词｜conf=0.78｜定位：p.? @段1
- AGENTPOISON Red-teaming LLM agents via poisoning memory or knowledge bases.txt｜摘要｜conf=0.72｜定位：p.? @段1
- AGENTPOISON Red-teaming LLM agents via poisoning memory or knowledge bases.txt｜研究方法｜conf=0.82｜定位：p.? @段9
- AGENTPOISON Red-teaming LLM agents via poisoning memory or knowledge bases.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- Preemptive detection and correction of misaligned actions in LLM agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Preemptive detection and correction of misaligned actions in LLM agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Preemptive detection and correction of misaligned actions in LLM agents.txt｜研究方法｜conf=0.50｜定位：p.? @段1
- Preemptive detection and correction of misaligned actions in LLM agents.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- LLM-Powered GUI Agents in Phone Automation Surveying Progress and Prospects.txt｜关键词｜conf=0.78｜定位：p.? @段1
- LLM-Powered GUI Agents in Phone Automation Surveying Progress and Prospects.txt｜摘要｜conf=0.72｜定位：p.? @段1
- LLM-Powered GUI Agents in Phone Automation Surveying Progress and Prospects.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- LLM-Powered GUI Agents in Phone Automation Surveying Progress and Prospects.txt｜主要结论｜conf=0.80｜定位：p.? @段90
- WASP Benchmarking web agent security against prompt injection attacks.txt｜关键词｜conf=0.78｜定位：p.? @段1
- WASP Benchmarking web agent security against prompt injection attacks.txt｜摘要｜conf=0.72｜定位：p.? @段1
- WASP Benchmarking web agent security against prompt injection attacks.txt｜研究方法｜conf=0.82｜定位：p.? @段11
- WASP Benchmarking web agent security against prompt injection attacks.txt｜主要结论｜conf=0.80｜定位：p.? @段15
- AgentDojo A dynamic environment to evaluate prompt injection attacks and defenses for LLM agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- AgentDojo A dynamic environment to evaluate prompt injection attacks and defenses for LLM agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- AgentDojo A dynamic environment to evaluate prompt injection attacks and defenses for LLM agents.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- AgentDojo A dynamic environment to evaluate prompt injection attacks and defenses for LLM agents.txt｜主要结论｜conf=0.80｜定位：p.? @段18
- PromptArmor Simple yet effective prompt injection defenses.txt｜关键词｜conf=0.78｜定位：p.? @段1
- PromptArmor Simple yet effective prompt injection defenses.txt｜摘要｜conf=0.72｜定位：p.? @段1
- PromptArmor Simple yet effective prompt injection defenses.txt｜研究方法｜conf=0.50｜定位：p.? @段1
- PromptArmor Simple yet effective prompt injection defenses.txt｜主要结论｜conf=0.80｜定位：p.? @段19
- Red-teaming LLM multi-agent systems via communication attacks.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Red-teaming LLM multi-agent systems via communication attacks.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Red-teaming LLM multi-agent systems via communication attacks.txt｜研究方法｜conf=0.82｜定位：p.? @段2
- Red-teaming LLM multi-agent systems via communication attacks.txt｜主要结论｜conf=0.80｜定位：p.? @段14
- Refusal-trained LLMs are easily jailbroken as browser agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Refusal-trained LLMs are easily jailbroken as browser agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Refusal-trained LLMs are easily jailbroken as browser agents.txt｜研究方法｜conf=0.82｜定位：p.? @段6
- Refusal-trained LLMs are easily jailbroken as browser agents.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- ShieldAgent Shielding agents via verifiable safety policy reasoning.txt｜关键词｜conf=0.78｜定位：p.? @段1
- ShieldAgent Shielding agents via verifiable safety policy reasoning.txt｜摘要｜conf=0.72｜定位：p.? @段1
- ShieldAgent Shielding agents via verifiable safety policy reasoning.txt｜研究方法｜conf=0.82｜定位：p.? @段4
- ShieldAgent Shielding agents via verifiable safety policy reasoning.txt｜主要结论｜conf=0.80｜定位：p.? @段11
- SOEN-101 Code generation by emulating software process models using large language model agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- SOEN-101 Code generation by emulating software process models using large language model agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- SOEN-101 Code generation by emulating software process models using large language model agents.txt｜研究方法｜conf=0.82｜定位：p.? @段17
- SOEN-101 Code generation by emulating software process models using large language model agents.txt｜主要结论｜conf=0.80｜定位：p.? @段17
- The task shield Enforcing task alignment to defend against indirect prompt injection in LLM agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- The task shield Enforcing task alignment to defend against indirect prompt injection in LLM agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- The task shield Enforcing task alignment to defend against indirect prompt injection in LLM agents.txt｜研究方法｜conf=0.82｜定位：p.? @段9
- The task shield Enforcing task alignment to defend against indirect prompt injection in LLM agents.txt｜主要结论｜conf=0.80｜定位：p.? @段2
- Towards LLM-augmented multiagent systems for agile software engineering.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Towards LLM-augmented multiagent systems for agile software engineering.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Towards LLM-augmented multiagent systems for agile software engineering.txt｜研究方法｜conf=0.82｜定位：p.? @段10
- Towards LLM-augmented multiagent systems for agile software engineering.txt｜主要结论｜conf=0.50｜定位：p.? @段1
- Towards trustworthy GUI agents A survey.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Towards trustworthy GUI agents A survey.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Towards trustworthy GUI agents A survey.txt｜研究方法｜conf=0.82｜定位：p.? @段11
- Towards trustworthy GUI agents A survey.txt｜主要结论｜conf=0.50｜定位：p.? @段1
- Unity is strength Collaborative LLM-based agents for code reviewer recommendation.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Unity is strength Collaborative LLM-based agents for code reviewer recommendation.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Unity is strength Collaborative LLM-based agents for code reviewer recommendation.txt｜研究方法｜conf=0.82｜定位：p.? @段13
- Unity is strength Collaborative LLM-based agents for code reviewer recommendation.txt｜主要结论｜conf=0.80｜定位：p.? @段7
- Unveiling privacy risks in LLM agent memory.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Unveiling privacy risks in LLM agent memory.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Unveiling privacy risks in LLM agent memory.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- Unveiling privacy risks in LLM agent memory.txt｜主要结论｜conf=0.50｜定位：p.? @段1
- Watch out for your agents! Investigating backdoor threats to LLM-based agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Watch out for your agents! Investigating backdoor threats to LLM-based agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Watch out for your agents! Investigating backdoor threats to LLM-based agents.txt｜研究方法｜conf=0.82｜定位：p.? @段2
- Watch out for your agents! Investigating backdoor threats to LLM-based agents.txt｜主要结论｜conf=0.80｜定位：p.? @段2
- Your agent can defend itself against backdoor attacks.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Your agent can defend itself against backdoor attacks.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Your agent can defend itself against backdoor attacks.txt｜研究方法｜conf=0.82｜定位：p.? @段4
- Your agent can defend itself against backdoor attacks.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- RedCode Risky code execution and generation benchmark for code agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- RedCode Risky code execution and generation benchmark for code agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- RedCode Risky code execution and generation benchmark for code agents.txt｜研究方法｜conf=0.82｜定位：p.? @段16
- RedCode Risky code execution and generation benchmark for code agents.txt｜主要结论｜conf=0.80｜定位：p.? @段21
- WIPI A new web threat for LLM-driven web agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- WIPI A new web threat for LLM-driven web agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- WIPI A new web threat for LLM-driven web agents.txt｜研究方法｜conf=0.82｜定位：p.? @段8
- WIPI A new web threat for LLM-driven web agents.txt｜主要结论｜conf=0.80｜定位：p.? @段6
- A multi-agent onboarding assistant based on large language models, retrieval augmented generation, a.txt｜关键词｜conf=0.78｜定位：p.? @段1
- A multi-agent onboarding assistant based on large language models, retrieval augmented generation, a.txt｜摘要｜conf=0.72｜定位：p.? @段1
- A multi-agent onboarding assistant based on large language models, retrieval augmented generation, a.txt｜研究方法｜conf=0.82｜定位：p.? @段8
- A multi-agent onboarding assistant based on large language models, retrieval augmented generation, a.txt｜主要结论｜conf=0.50｜定位：p.? @段1
- AEGIS An agent-based framework for bug reproduction from issue descriptions.txt｜关键词｜conf=0.78｜定位：p.? @段1
- AEGIS An agent-based framework for bug reproduction from issue descriptions.txt｜摘要｜conf=0.72｜定位：p.? @段1
- AEGIS An agent-based framework for bug reproduction from issue descriptions.txt｜研究方法｜conf=0.82｜定位：p.? @段9
- AEGIS An agent-based framework for bug reproduction from issue descriptions.txt｜主要结论｜conf=0.80｜定位：p.? @段23
- Advanced smart contract vulnerability detection via LLM-powered multi-agent systems.txt｜期刊/会议｜conf=0.82｜定位：p.? @段1
- Advanced smart contract vulnerability detection via LLM-powered multi-agent systems.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Advanced smart contract vulnerability detection via LLM-powered multi-agent systems.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Advanced smart contract vulnerability detection via LLM-powered multi-agent systems.txt｜研究方法｜conf=0.82｜定位：p.? @段1
- Advanced smart contract vulnerability detection via LLM-powered multi-agent systems.txt｜主要结论｜conf=0.80｜定位：p.? @段6
- AgentFM Role-aware failure management for distributed databases with LLM-driven multi-agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- AgentFM Role-aware failure management for distributed databases with LLM-driven multi-agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- AgentFM Role-aware failure management for distributed databases with LLM-driven multi-agents.txt｜研究方法｜conf=0.82｜定位：p.? @段7
- AgentFM Role-aware failure management for distributed databases with LLM-driven multi-agents.txt｜主要结论｜conf=0.80｜定位：p.? @段16
- AdInject Real-world black-box attacks on web agents via advertising delivery.txt｜关键词｜conf=0.78｜定位：p.? @段1
- AdInject Real-world black-box attacks on web agents via advertising delivery.txt｜摘要｜conf=0.72｜定位：p.? @段1
- AdInject Real-world black-box attacks on web agents via advertising delivery.txt｜研究方法｜conf=0.82｜定位：p.? @段10
- AdInject Real-world black-box attacks on web agents via advertising delivery.txt｜主要结论｜conf=0.80｜定位：p.? @段15
- Amplified vulnerabilities Structured jailbreak attacks on LLM-based multi-agent debate.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Amplified vulnerabilities Structured jailbreak attacks on LLM-based multi-agent debate.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Amplified vulnerabilities Structured jailbreak attacks on LLM-based multi-agent debate.txt｜研究方法｜conf=0.82｜定位：p.? @段1
- Amplified vulnerabilities Structured jailbreak attacks on LLM-based multi-agent debate.txt｜主要结论｜conf=0.80｜定位：p.? @段1
- Knowledge-based multi-agent framework for automated software architecture design.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Knowledge-based multi-agent framework for automated software architecture design.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Knowledge-based multi-agent framework for automated software architecture design.txt｜研究方法｜conf=0.82｜定位：p.? @段6
- Knowledge-based multi-agent framework for automated software architecture design.txt｜主要结论｜conf=0.80｜定位：p.? @段9
- SafeAgent Safeguarding LLM agents via an automated risk simulator.txt｜关键词｜conf=0.78｜定位：p.? @段1
- SafeAgent Safeguarding LLM agents via an automated risk simulator.txt｜摘要｜conf=0.72｜定位：p.? @段1
- SafeAgent Safeguarding LLM agents via an automated risk simulator.txt｜研究方法｜conf=0.82｜定位：p.18 @段21
- SafeAgent Safeguarding LLM agents via an automated risk simulator.txt｜主要结论｜conf=0.80｜定位：p.? @段20
- Autonomous agents in software development for information retrieval using LLM models.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Autonomous agents in software development for information retrieval using LLM models.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Autonomous agents in software development for information retrieval using LLM models.txt｜研究方法｜conf=0.50｜定位：p.? @段1
- Autonomous agents in software development for information retrieval using LLM models.txt｜主要结论｜conf=0.50｜定位：p.? @段1
- Alibaba LingmaAgent Improving automated issue resolution via comprehensive repository exploration.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Alibaba LingmaAgent Improving automated issue resolution via comprehensive repository exploration.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Alibaba LingmaAgent Improving automated issue resolution via comprehensive repository exploration.txt｜研究方法｜conf=0.82｜定位：p.? @段28
- Alibaba LingmaAgent Improving automated issue resolution via comprehensive repository exploration.txt｜主要结论｜conf=0.80｜定位：p.? @段21
- VPI-bench Visual prompt injection attacks for computer-use agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- VPI-bench Visual prompt injection attacks for computer-use agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- VPI-bench Visual prompt injection attacks for computer-use agents.txt｜研究方法｜conf=0.82｜定位：p.? @段14
- VPI-bench Visual prompt injection attacks for computer-use agents.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- Secret collusion among AI agents Multi-agent deception via steganography.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Secret collusion among AI agents Multi-agent deception via steganography.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Secret collusion among AI agents Multi-agent deception via steganography.txt｜研究方法｜conf=0.82｜定位：p.? @段14
- Secret collusion among AI agents Multi-agent deception via steganography.txt｜主要结论｜conf=0.80｜定位：p.? @段2
- LLM-based multi-agent systems for software engineering Literature review, vision, and the road ahea.txt｜期刊/会议｜conf=0.82｜定位：p.? @段1
- LLM-based multi-agent systems for software engineering Literature review, vision, and the road ahea.txt｜关键词｜conf=0.78｜定位：p.? @段1
- LLM-based multi-agent systems for software engineering Literature review, vision, and the road ahea.txt｜摘要｜conf=0.72｜定位：p.? @段1
- LLM-based multi-agent systems for software engineering Literature review, vision, and the road ahea.txt｜研究方法｜conf=0.82｜定位：p.? @段21
- LLM-based multi-agent systems for software engineering Literature review, vision, and the road ahea.txt｜主要结论｜conf=0.80｜定位：p.? @段15
- Enhancing game AI behaviors with large language models and agentic AI.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Enhancing game AI behaviors with large language models and agentic AI.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Enhancing game AI behaviors with large language models and agentic AI.txt｜研究方法｜conf=0.82｜定位：p.? @段4
- Enhancing game AI behaviors with large language models and agentic AI.txt｜主要结论｜conf=0.80｜定位：p.? @段7
- Enhancing human-IDE interaction in the SDLC using LLM-based mediator agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Enhancing human-IDE interaction in the SDLC using LLM-based mediator agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Enhancing human-IDE interaction in the SDLC using LLM-based mediator agents.txt｜研究方法｜conf=0.82｜定位：p.? @段10
- Enhancing human-IDE interaction in the SDLC using LLM-based mediator agents.txt｜主要结论｜conf=0.50｜定位：p.? @段1
- Evaluating the robustness of multimodal agents against active environmental injection attacks.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Evaluating the robustness of multimodal agents against active environmental injection attacks.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Evaluating the robustness of multimodal agents against active environmental injection attacks.txt｜研究方法｜conf=0.82｜定位：p.? @段5
- Evaluating the robustness of multimodal agents against active environmental injection attacks.txt｜主要结论｜conf=0.80｜定位：p.? @段27
- Facilitating trustworthy human-agent collaboration in LLM-based multi-agent system oriented software.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Facilitating trustworthy human-agent collaboration in LLM-based multi-agent system oriented software.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Facilitating trustworthy human-agent collaboration in LLM-based multi-agent system oriented software.txt｜研究方法｜conf=0.82｜定位：p.? @段4
- Facilitating trustworthy human-agent collaboration in LLM-based multi-agent system oriented software.txt｜主要结论｜conf=0.50｜定位：p.? @段1
- AgentSentinel An end-to-end and real-time security defense framework for computer-use agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- AgentSentinel An end-to-end and real-time security defense framework for computer-use agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- AgentSentinel An end-to-end and real-time security defense framework for computer-use agents.txt｜研究方法｜conf=0.82｜定位：p.? @段4
- AgentSentinel An end-to-end and real-time security defense framework for computer-use agents.txt｜主要结论｜conf=0.80｜定位：p.? @段6
- Fuzz-testing meets LLM-based agents An automated and efficient framework for jailbreaking text-to-i.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Fuzz-testing meets LLM-based agents An automated and efficient framework for jailbreaking text-to-i.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Fuzz-testing meets LLM-based agents An automated and efficient framework for jailbreaking text-to-i.txt｜研究方法｜conf=0.82｜定位：p.? @段4
- Fuzz-testing meets LLM-based agents An automated and efficient framework for jailbreaking text-to-i.txt｜主要结论｜conf=0.80｜定位：p.? @段4
- Are CAPTCHAs still bot-hard Generalized visual CAPTCHA solving with agentic vision language model.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Are CAPTCHAs still bot-hard Generalized visual CAPTCHA solving with agentic vision language model.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Are CAPTCHAs still bot-hard Generalized visual CAPTCHA solving with agentic vision language model.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- Are CAPTCHAs still bot-hard Generalized visual CAPTCHA solving with agentic vision language model.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- Teamwork makes the dream work LLMs-based agents for GitHub README.MD summarization.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Teamwork makes the dream work LLMs-based agents for GitHub README.MD summarization.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Teamwork makes the dream work LLMs-based agents for GitHub README.MD summarization.txt｜研究方法｜conf=0.82｜定位：p.? @段7
- Teamwork makes the dream work LLMs-based agents for GitHub README.MD summarization.txt｜主要结论｜conf=0.80｜定位：p.? @段5
- Graphormer-guided task planning Beyond static rules with LLM safety perception.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Graphormer-guided task planning Beyond static rules with LLM safety perception.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Graphormer-guided task planning Beyond static rules with LLM safety perception.txt｜研究方法｜conf=0.82｜定位：p.? @段8
- Graphormer-guided task planning Beyond static rules with LLM safety perception.txt｜主要结论｜conf=0.80｜定位：p.? @段1
- EVA Red-teaming GUI agents via evolving indirect prompt injection.txt｜关键词｜conf=0.78｜定位：p.? @段1
- EVA Red-teaming GUI agents via evolving indirect prompt injection.txt｜摘要｜conf=0.72｜定位：p.? @段1
- EVA Red-teaming GUI agents via evolving indirect prompt injection.txt｜研究方法｜conf=0.82｜定位：p.? @段2
- EVA Red-teaming GUI agents via evolving indirect prompt injection.txt｜主要结论｜conf=0.80｜定位：p.? @段18
- AutoReview An LLM-based multi-agent system for security issue-oriented code review.txt｜关键词｜conf=0.78｜定位：p.? @段1
- AutoReview An LLM-based multi-agent system for security issue-oriented code review.txt｜摘要｜conf=0.72｜定位：p.? @段1
- AutoReview An LLM-based multi-agent system for security issue-oriented code review.txt｜研究方法｜conf=0.82｜定位：p.? @段2
- AutoReview An LLM-based multi-agent system for security issue-oriented code review.txt｜主要结论｜conf=0.80｜定位：p.? @段2
- JailbreakRadar Comprehensive assessment of jailbreak attacks against LLMs.txt｜关键词｜conf=0.78｜定位：p.? @段1
- JailbreakRadar Comprehensive assessment of jailbreak attacks against LLMs.txt｜摘要｜conf=0.72｜定位：p.? @段1
- JailbreakRadar Comprehensive assessment of jailbreak attacks against LLMs.txt｜研究方法｜conf=0.82｜定位：p.6584 @段35
- JailbreakRadar Comprehensive assessment of jailbreak attacks against LLMs.txt｜主要结论｜conf=0.80｜定位：p.6584 @段30
- LLM agents should employ security principles.txt｜关键词｜conf=0.78｜定位：p.? @段1
- LLM agents should employ security principles.txt｜摘要｜conf=0.72｜定位：p.? @段1
- LLM agents should employ security principles.txt｜研究方法｜conf=0.82｜定位：p.? @段1
- LLM agents should employ security principles.txt｜主要结论｜conf=0.80｜定位：p.? @段1
- LLM-agents driven automated simulation testing and analysis of small uncrewed aerial systems.txt｜关键词｜conf=0.78｜定位：p.? @段1
- LLM-agents driven automated simulation testing and analysis of small uncrewed aerial systems.txt｜摘要｜conf=0.72｜定位：p.? @段1
- LLM-agents driven automated simulation testing and analysis of small uncrewed aerial systems.txt｜研究方法｜conf=0.82｜定位：p.? @段1
- LLM-agents driven automated simulation testing and analysis of small uncrewed aerial systems.txt｜主要结论｜conf=0.80｜定位：p.? @段1
- Mediating between human programmers and integrated development environments using LLM-based agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Mediating between human programmers and integrated development environments using LLM-based agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Mediating between human programmers and integrated development environments using LLM-based agents.txt｜研究方法｜conf=0.82｜定位：p.? @段5
- Mediating between human programmers and integrated development environments using LLM-based agents.txt｜主要结论｜conf=0.50｜定位：p.? @段1
- Patchagent A practical program repair agent mimicking human expertise.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Patchagent A practical program repair agent mimicking human expertise.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Patchagent A practical program repair agent mimicking human expertise.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- Patchagent A practical program repair agent mimicking human expertise.txt｜主要结论｜conf=0.80｜定位：p.? @段64
- Mind the web The security of web use agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Mind the web The security of web use agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Mind the web The security of web use agents.txt｜研究方法｜conf=0.82｜定位：p.? @段2
- Mind the web The security of web use agents.txt｜主要结论｜conf=0.80｜定位：p.? @段4
- GameChat Multi-LLM dialogue for safe, agile, and socially optimal multi-agent navigation in constra.txt｜关键词｜conf=0.78｜定位：p.? @段1
- GameChat Multi-LLM dialogue for safe, agile, and socially optimal multi-agent navigation in constra.txt｜摘要｜conf=0.72｜定位：p.? @段1
- GameChat Multi-LLM dialogue for safe, agile, and socially optimal multi-agent navigation in constra.txt｜研究方法｜conf=0.82｜定位：p.? @段1
- GameChat Multi-LLM dialogue for safe, agile, and socially optimal multi-agent navigation in constra.txt｜主要结论｜conf=0.80｜定位：p.? @段1
- Poster Agentic shell honeypot using structured logging.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Poster Agentic shell honeypot using structured logging.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Poster Agentic shell honeypot using structured logging.txt｜研究方法｜conf=0.82｜定位：p.? @段7
- Poster Agentic shell honeypot using structured logging.txt｜主要结论｜conf=0.50｜定位：p.? @段1
- Cloak, honey, trap Proactive defenses against LLM agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Cloak, honey, trap Proactive defenses against LLM agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Cloak, honey, trap Proactive defenses against LLM agents.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- Cloak, honey, trap Proactive defenses against LLM agents.txt｜主要结论｜conf=0.80｜定位：p.? @段31
- ProphetAgent Automatically synthesizing GUI tests from test cases in natural language for mobile ap.txt｜关键词｜conf=0.78｜定位：p.? @段1
- ProphetAgent Automatically synthesizing GUI tests from test cases in natural language for mobile ap.txt｜摘要｜conf=0.72｜定位：p.? @段1
- ProphetAgent Automatically synthesizing GUI tests from test cases in natural language for mobile ap.txt｜研究方法｜conf=0.82｜定位：p.? @段5
- ProphetAgent Automatically synthesizing GUI tests from test cases in natural language for mobile ap.txt｜主要结论｜conf=0.80｜定位：p.? @段5
- RTBAS Defending LLM agents against prompt injection and privacy leakage.txt｜关键词｜conf=0.78｜定位：p.? @段1
- RTBAS Defending LLM agents against prompt injection and privacy leakage.txt｜摘要｜conf=0.72｜定位：p.? @段1
- RTBAS Defending LLM agents against prompt injection and privacy leakage.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- RTBAS Defending LLM agents against prompt injection and privacy leakage.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- Real AI agents with fake memories Fatal context manipulation attacks on Web3 agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Real AI agents with fake memories Fatal context manipulation attacks on Web3 agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Real AI agents with fake memories Fatal context manipulation attacks on Web3 agents.txt｜研究方法｜conf=0.82｜定位：p.? @段2
- Real AI agents with fake memories Fatal context manipulation attacks on Web3 agents.txt｜主要结论｜conf=0.80｜定位：p.? @段1
- OS-harm A benchmark for measuring safety of computer use agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- OS-harm A benchmark for measuring safety of computer use agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- OS-harm A benchmark for measuring safety of computer use agents.txt｜研究方法｜conf=0.82｜定位：p.? @段24
- OS-harm A benchmark for measuring safety of computer use agents.txt｜主要结论｜conf=0.80｜定位：p.? @段24
- From assistants to adversaries Exploring the security risks of mobile LLM agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- From assistants to adversaries Exploring the security risks of mobile LLM agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- From assistants to adversaries Exploring the security risks of mobile LLM agents.txt｜研究方法｜conf=0.82｜定位：p.? @段1
- From assistants to adversaries Exploring the security risks of mobile LLM agents.txt｜主要结论｜conf=0.50｜定位：p.? @段1
- Systematic categorization, construction and evaluation of new attacks against multi-modal mobile GUI.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Systematic categorization, construction and evaluation of new attacks against multi-modal mobile GUI.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Systematic categorization, construction and evaluation of new attacks against multi-modal mobile GUI.txt｜研究方法｜conf=0.82｜定位：p.? @段1
- Systematic categorization, construction and evaluation of new attacks against multi-modal mobile GUI.txt｜主要结论｜conf=0.80｜定位：p.? @段1
- Make agent defeat agent Automatic detection of taint-style vulnerabilities in LLM-based agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Make agent defeat agent Automatic detection of taint-style vulnerabilities in LLM-based agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Make agent defeat agent Automatic detection of taint-style vulnerabilities in LLM-based agents.txt｜研究方法｜conf=0.82｜定位：p.? @段2
- Make agent defeat agent Automatic detection of taint-style vulnerabilities in LLM-based agents.txt｜主要结论｜conf=0.80｜定位：p.? @段3
- The hidden dangers of browsing AI agents.txt｜关键词｜conf=0.78｜定位：p.? @段1
- The hidden dangers of browsing AI agents.txt｜摘要｜conf=0.72｜定位：p.? @段1
- The hidden dangers of browsing AI agents.txt｜研究方法｜conf=0.82｜定位：p.? @段29
- The hidden dangers of browsing AI agents.txt｜主要结论｜conf=0.80｜定位：p.? @段10
- Towards adaptive software agents for debugging.txt｜关键词｜conf=0.78｜定位：p.? @段1
- Towards adaptive software agents for debugging.txt｜摘要｜conf=0.72｜定位：p.? @段1
- Towards adaptive software agents for debugging.txt｜研究方法｜conf=0.82｜定位：p.? @段15
- Towards adaptive software agents for debugging.txt｜主要结论｜conf=0.50｜定位：p.? @段1
- The obvious invisible threat LLM-powered GUI agents' vulnerability to fine-print injections.txt｜关键词｜conf=0.78｜定位：p.? @段1
- The obvious invisible threat LLM-powered GUI agents' vulnerability to fine-print injections.txt｜摘要｜conf=0.72｜定位：p.? @段1
- The obvious invisible threat LLM-powered GUI agents' vulnerability to fine-print injections.txt｜研究方法｜conf=0.50｜定位：p.? @段1
- The obvious invisible threat LLM-powered GUI agents' vulnerability to fine-print injections.txt｜主要结论｜conf=0.80｜定位：p.? @段29
- IsolateGPT An execution isolation architecture for LLM-based systems.txt｜期刊/会议｜conf=0.82｜定位：p.? @段1
- IsolateGPT An execution isolation architecture for LLM-based systems.txt｜关键词｜conf=0.78｜定位：p.? @段1
- IsolateGPT An execution isolation architecture for LLM-based systems.txt｜摘要｜conf=0.72｜定位：p.? @段1
- IsolateGPT An execution isolation architecture for LLM-based systems.txt｜研究方法｜conf=0.82｜定位：p.? @段3
- IsolateGPT An execution isolation architecture for LLM-based systems.txt｜主要结论｜conf=0.80｜定位：p.? @段1
- RedTeamCUA Realistic adversarial testing of computer-use agents in hybrid web-OS environments.txt｜关键词｜conf=0.78｜定位：p.? @段1
- RedTeamCUA Realistic adversarial testing of computer-use agents in hybrid web-OS environments.txt｜摘要｜conf=0.72｜定位：p.? @段1
- RedTeamCUA Realistic adversarial testing of computer-use agents in hybrid web-OS environments.txt｜研究方法｜conf=0.82｜定位：p.? @段7
- RedTeamCUA Realistic adversarial testing of computer-use agents in hybrid web-OS environments.txt｜主要结论｜conf=0.80｜定位：p.? @段26

## 逐篇结构化解析

<details><summary>1. The Crescendo Multi-Turn LLM Jailbreak Attack（n.d.）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | The Crescendo Multi-Turn LLM Jailbreak Attack | 0.92 | “The Crescendo Multi-Turn LLM Jailbreak Attack” (Great, Now Write an Article About That- The Crescendo Multi-Turn LLM Jailbreak Attack.txt @段2) || 作者 | Great, Now Write an Article About That: | 0.90 | “Great, Now Write an Article About That:” (Great, Now Write an Article About That- The Crescendo Multi-Turn LLM Jailbreak Attack.txt @段1) || 发表年份 | N/A | 0.40 | N/A || 期刊/会议 | N/A | 0.45 | N/A || 关键词 | crescendo, jailbreak, turn, multi, task, alignment, gemini, gpt, other, target | 0.78 | “Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack Mark Russinovich Microsoft Azure Ahmed Salem Microsoft Ronen Eldan Microsoft” (Great, Now Write an Article About That- The Crescendo Multi-Turn LLM Jailbreak Attack.txt @段1) || 摘要 | Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack Mark Russinovich Microsoft Azure Ahmed Salem Microsoft Ronen Eldan Microsoft Abstract Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engag- ing in illegal or unethical topics as a means to avoid contribut- ing to responsible AI harms. However, a recent line of attacks, known as “jailbreaks”, seek to overc… | 0.72 | “Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack Mark Russinovich Microsoft Azure Ahmed Salem Microsoft Ronen Eldan Microsoft” (Great, Now Write an Article About That- The Crescendo Multi-Turn LLM Jailbreak Attack.txt @段1) || 研究方法 | 4.1 Overview Intuitively, Crescendomation leverages an LLM to automate the Crescendo attack sequence. For this work we use GPT-4 for Crescendomation. The process begins with the generation of an initial prompt or question, which is then sent to the target LLM. The tool then processes the received response and, in an adaptive manner, Crescendomation formulates the subsequent prompt or question. This cycle of interaction continues over multiple turns until the tool successfully jailbreaks the model and accomplishes … | 0.82 | “We present the Crescendomation’s concrete algorithm in Algorithm 1.” (Great, Now Write an Article About That- The Crescendo Multi-Turn LLM Jailbreak Attack.txt @段11) || 主要结论 | 5.7 Evaluating Crescendo Against Defenses Finally, we evaluate Crescendomation against state-of-the-art defenses, namely Self-Reminder [30] and Goal Prioritiza- tion [34]. Both of these defenses aim to clarify the priority for the model by appending each user input into a template, which includes both a prefix and a suffix designed to remind the model to remain ethical and aligned with its values. For example, the suffix used by [30] is appended after each user input: “Did your response consider the principles of … | 0.80 | “We attempted to increase the number of rounds to 30; however, due to the ad- ditional rounds and the significantly higher tokens generated for the Goal Prioritization defense (since it generates model thoughts each time), and the limitation of access to GPT-4 with a maximum of 32k tokens, we could not run it.” (Great, Now Write an Article About That- The Crescendo Multi-Turn LLM Jailbreak Attack.txt @段54) |

- 主题标签：Jailbreak/越狱攻击, Web/Browser Agent 安全, 防御与对齐机制, 隐私与记忆风险
- 方法标签：系统防御/Defense, 调查/Survey, 越狱/Jailbreak
- 源文件：`Great, Now Write an Article About That- The Crescendo Multi-Turn LLM Jailbreak Attack.txt`

</details>
<details><summary>2. Characterizing Unintended Consequences in Human-GUI Agent（2018）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Characterizing Unintended Consequences in Human-GUI Agent | 0.92 | “Characterizing Unintended Consequences in Human-GUI Agent” (Characterizing unintended consequences in human-GUI agent collaboration for web browsing.txt @段2) || 作者 | SHUNING ZHANG, Tsinghua University, China | 0.90 | “SHUNING ZHANG, Tsinghua University, China” (Characterizing unintended consequences in human-GUI agent collaboration for web browsing.txt @段1) || 发表年份 | 2018 | 0.90 | “Shuning Zhang, Jingruo Chen, Zhiqi Gao, Jiajing Gao, Xin Yi, and Hewu Li. 2018. Characterizing Unintended Consequences in” (Characterizing unintended consequences in human-GUI agent collaboration for web browsing.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2505.09875v2  [cs.HC]  16 May 2025” (Characterizing unintended consequences in human-GUI agent collaboration for web browsing.txt @段1) || 关键词 | gui, ucs, user, human, research, web, china, privacy, consequences, university | 0.78 | “arXiv:2505.09875v2 [cs.HC] 16 May 2025 Characterizing Unintended Consequences in Human-GUI Agent Collaboration for Web Browsing SHUNING ZHANG, Tsinghua University, China JINGRUO CHEN, Information Science, Cornell University, USA ZHIQI GAO, Nankai University, China JIAJING GAO, Institute of Future Human Habitat, Tsinghu” (Characterizing unintended consequences in human-GUI agent collaboration for web browsing.txt @段1) || 摘要 | arXiv:2505.09875v2 [cs.HC] 16 May 2025 Characterizing Unintended Consequences in Human-GUI Agent Collaboration for Web Browsing SHUNING ZHANG, Tsinghua University, China JINGRUO CHEN, Information Science, Cornell University, USA ZHIQI GAO, Nankai University, China JIAJING GAO, Institute of Future Human Habitat, Tsinghua University, China XIN YI∗, Tsinghua University, China and Zhongguancun Laboratory, China HEWU LI, Tsinghua University, China and Zhongguancun Laboratory, China The proliferation of Large Language M… | 0.72 | “arXiv:2505.09875v2 [cs.HC] 16 May 2025 Characterizing Unintended Consequences in Human-GUI Agent Collaboration for Web Browsing SHUNING ZHANG, Tsinghua University, China JINGRUO CHEN, Information Science, Cornell University, USA ZHIQI GAO, Nankai University, China JIAJING GAO, Institute of Future Human Habitat, Tsinghu” (Characterizing unintended consequences in human-GUI agent collaboration for web browsing.txt @段1) || 研究方法 | 2.2 Task Automation and GUI Agents Traditional GUI automation primarily employed rule-based frameworks such as Selenium8, Robot Framework9 and AutoIt10 [15]. These tools automate predefined interaction sequences (e.g., clicks, text input) but exhibit limited adaptability in dynamic environments and necessitate substantial manual configuration. The advent of LLMs catalyzed a paradigm shift, fostering sophisticated GUI agents engineered for real- time GUI component interpretation and dynamic adaptation to interface … | 0.82 | “These agents leverage LLMs to comprehend visual interfaces and autonomously execute user 8https://www.selenium.dev/documentation/ 9https://robotframework.org/robotframework/ 10https://www.autoitscript.com/site/autoit/documentation-localization/ , Vol.” (Characterizing unintended consequences in human-GUI agent collaboration for web browsing.txt @段5) || 主要结论 | 2 Related Work This section synthesizes three research streams vital to the CSCW community: the study of UCs from emerging socio-technical systems [25, 26, 44], the advancements and limitations of LLM agents [ 16], and the security, privacy and safety risks associated with their deployment, especially in collaborative interactions [41, 70]. | 0.80 | “2 Related Work This section synthesizes three research streams vital to the CSCW community: the study of UCs from emerging socio-technical systems [25, 26, 44], the advancements and limitations of LLM agents [ 16], and the security, privacy and safety risks associated with their deployment, especially in collaborative” (Characterizing unintended consequences in human-GUI agent collaboration for web browsing.txt @段3) |

- 主题标签：GUI/Computer-Use Agent 安全, Web/Browser Agent 安全, 多智能体对抗与协作, 隐私与记忆风险
- 方法标签：仿真/Simulator, 多智能体/Multi-Agent, 多模态/Multimodal
- 源文件：`综述参考文献\Characterizing unintended consequences in human-GUI agent collaboration for web browsing.txt`

</details>
<details><summary>3. Compromising Embodied Agents with Contextual（2021）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Compromising Embodied Agents with Contextual | 0.92 | “Compromising Embodied Agents with Contextual” (Compromising embodied agents with contextual backdoor attacks.txt @段2) || 作者 | Aishan Liu1, Yuguang Zhou1, Xianglong Liu1*, Tianyuan Zhang1, Siyuan Liang2, Jiakai | 0.90 | “Aishan Liu1, Yuguang Zhou1, Xianglong Liu1*, Tianyuan Zhang1, Siyuan Liang2, Jiakai” (Compromising embodied agents with contextual backdoor attacks.txt @段1) || 发表年份 | 2021 | 0.86 | “Springer Nature 2021 LATEX template” (Compromising embodied agents with contextual backdoor attacks.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2408.02882v1  [cs.AI]  6 Aug 2024” (Compromising embodied agents with contextual backdoor attacks.txt @段1) || 关键词 | contextual, embodied, programs, environment, microwave, task, backdoor, defects, specific, generate | 0.78 | “Springer Nature 2021 LATEX template Compromising Embodied Agents with Contextual Backdoor Attacks Aishan Liu1, Yuguang Zhou1, Xianglong Liu1*, Tianyuan Zhang1, Siyuan Liang2, Jiakai Wang3, Yanjun Pu3, Tianlin Li4, Junqi Zhang5, Wenbo Zhou5, Qing Guo6 and Dacheng Tao4 1Beihang University, China. 2National University of” (Compromising embodied agents with contextual backdoor attacks.txt @段1) || 摘要 | Springer Nature 2021 LATEX template Compromising Embodied Agents with Contextual Backdoor Attacks Aishan Liu1, Yuguang Zhou1, Xianglong Liu1*, Tianyuan Zhang1, Siyuan Liang2, Jiakai Wang3, Yanjun Pu3, Tianlin Li4, Junqi Zhang5, Wenbo Zhou5, Qing Guo6 and Dacheng Tao4 1Beihang University, China. 2National University of Singapore, Singapore. 3Zhongguancun Laboratory, China. 4Nanyang Technological University, Singapore. 5University of Science and Technology of China, China. 6A*STAR, Singapore. Abstract Large language… | 0.72 | “Springer Nature 2021 LATEX template Compromising Embodied Agents with Contextual Backdoor Attacks Aishan Liu1, Yuguang Zhou1, Xianglong Liu1*, Tianyuan Zhang1, Siyuan Liang2, Jiakai Wang3, Yanjun Pu3, Tianlin Li4, Junqi Zhang5, Wenbo Zhou5, Qing Guo6 and Dacheng Tao4 1Beihang University, China. 2National University of” (Compromising embodied agents with contextual backdoor attacks.txt @段1) || 研究方法 | 2 Article Title cognitive behaviors by learning and adapting to the environment through direct interactions, encompass- ing sensing, movement, and manipulation. Recently, the emergence of LLMs ( e.g., ChatGPT [4]), has fundamentally transformed the landscape of program- ming, developing, and using embodied intelligence. Leveraging a few task demonstrations (e.g., the exam- ples for solving the task), LLMs can generate task- specific programs, enabling users to effortlessly create their own embodied agents [5–7]. I… | 0.82 | “Unlike conventional attacks targeting the agent directly in the environment, this novel approach focuses on only poisoning the source (LLMs) of the entire pipeline, allowing the poison to propagate from the origin to the endpoint (embodied agent) like a chain, with code serving as the conduit (illustrated in Fig.” (Compromising embodied agents with contextual backdoor attacks.txt @段4) || 主要结论 | 10 Article Title Table 1: Attacks on ProgPrompt. We report the aver- age results (%) on three trigger words at a poisoning ratio of 0.5. Metrics Task Performance (CA)Attack Performance SR↑ Exec↑ GCR↑ ASR↑ False-ASR↓ No attack 0.18 0.68 0.42 – – Multi-target 0.16 0.65 0.39 5.0 20.0 ICLAttack 0.14 0.64 0.36 62.5 40.0 Perplexity-based0.16 0.67 0.38 70.0 25.0 Ours 0.16 0.66 0.39 82.5 7.5 environment ( e.g., microwave salmon). In particu- lar, the user first provides a short task description like “put book back in the … | 0.80 | “1, from which we can identify: ❶ Our attack works efficiently on the ProgPrompt agent and achieves the highest ASR ( ≥ 82.5%) with low False-ASR (7.5%), which outperforms other attacks significantly (+12.5% at least).” (Compromising embodied agents with contextual backdoor attacks.txt @段57) |

- 主题标签：Backdoor/投毒
- 方法标签：仿真/Simulator, 后门/Backdoor, 多模态/Multimodal
- 源文件：`Compromising embodied agents with contextual backdoor attacks.txt`

</details>
<details><summary>4. Deceptive Alignment · Reverse Turing Test · Multi-Agent Systems · Prompt Injection · Agent Autonomy · Ethical（2022）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Deceptive Alignment · Reverse Turing Test · Multi-Agent Systems · Prompt Injection · Agent Autonomy · Ethical | 0.92 | “Deceptive Alignment · Reverse Turing Test · Multi-Agent Systems · Prompt Injection · Agent Autonomy · Ethical” (Guardians of the agentic system Preventing many shots jailbreak with agentic system.txt @段35) || 作者 | Saikat Barua1,a, Mostafizur Rahman2, Rafiul Islam3, Shehenaz Khaled4, Md Jafor Sadek5, and Dr. Ahmedul Kabir6 | 0.90 | “Saikat Barua1,a, Mostafizur Rahman2, Rafiul Islam3, Shehenaz Khaled4, Md Jafor Sadek5, and Dr. Ahmedul Kabir6” (Guardians of the agentic system Preventing many shots jailbreak with agentic system.txt @段1) || 发表年份 | 2022 | 0.86 | “based on supervised training and guardrails from Ouyang et al. (2022) and Bai et al. (2022) respectively prove” (Guardians of the agentic system Preventing many shots jailbreak with agentic system.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2502.16750v4  [cs.CR]  12 Jun 2025” (Guardians of the agentic system Preventing many shots jailbreak with agentic system.txt @段1) || 关键词 | systems, agentic, adversarial, frameworks, multi, which, context, research, vulnerabilities, alignment | 0.78 | “arXiv:2502.16750v4 [cs.CR] 12 Jun 2025 GUARDIANS OF THE AGENTIC SYSTEM : P REVENTING MANY SHOT JAILBREAKING WITH AGENTIC SYSTEM Saikat Barua1,a, Mostafizur Rahman2, Rafiul Islam3, Shehenaz Khaled4, Md Jafor Sadek5, and Dr. Ahmedul Kabir6 1North South University, Dhaka, saikat.barua@northsouth.edu aSpontAlign, Dhaka 2No” (Guardians of the agentic system Preventing many shots jailbreak with agentic system.txt @段1) || 摘要 | arXiv:2502.16750v4 [cs.CR] 12 Jun 2025 GUARDIANS OF THE AGENTIC SYSTEM : P REVENTING MANY SHOT JAILBREAKING WITH AGENTIC SYSTEM Saikat Barua1,a, Mostafizur Rahman2, Rafiul Islam3, Shehenaz Khaled4, Md Jafor Sadek5, and Dr. Ahmedul Kabir6 1North South University, Dhaka, saikat.barua@northsouth.edu aSpontAlign, Dhaka 2North South University, Dhaka, mostafizur.rahman10@northsouth.edu 5North South University, Dhaka, jaforsadek619@northsouth.edu 3North South University, Dhaka, rafiul.islam19@northsouth.edu 4North South… | 0.72 | “arXiv:2502.16750v4 [cs.CR] 12 Jun 2025 GUARDIANS OF THE AGENTIC SYSTEM : P REVENTING MANY SHOT JAILBREAKING WITH AGENTIC SYSTEM Saikat Barua1,a, Mostafizur Rahman2, Rafiul Islam3, Shehenaz Khaled4, Md Jafor Sadek5, and Dr. Ahmedul Kabir6 1North South University, Dhaka, saikat.barua@northsouth.edu aSpontAlign, Dhaka 2No” (Guardians of the agentic system Preventing many shots jailbreak with agentic system.txt @段1) || 研究方法 | 2.3 Mitigation Strategies within Agentic Frameworks Mitigation strategies for in-context scheming, alignment faking, and jailbreaks require a robust set of disclosures in the design of agentic frameworks. Methods for addressing these issues exist and have been shown to be insufficient, particularly when applied in the context of complex agentic frameworks. First is to use adversarial training methods where model is trained with adversarial examples [ 11]. However, this method can be used to enhance the model robus… | 0.82 | “More importantly, 4 The Guardians this interconnectedness brings to the forefront the fact that no single vulnerability can be remediated in isolation to a single artifact, rather this remediation is best accomplished by focusing on the architecture and design of agentic frameworks at large.” (Guardians of the agentic system Preventing many shots jailbreak with agentic system.txt @段6) || 主要结论 | 1 Introduction The application of AI agents spreads across all domains including healthcare diagnostics and financial systems and governance simulations because of transformer-based large language model evolutions combined with in-context learning methods [18, 8]. AI systems continue to lose control of their operational safety because multiple vulnerabilities including jailbreaks [22, 2] and deceptive actions [20] and adversarial attacks [1] compromise their security integrity. We have tried to develop AI systems … | 0.80 | “Most evaluations presently fail to measure up because they examine still scenarios with sole autonomous entities playing a single action in setups that diverge significantly from deployed multi-agent operational environments.” (Guardians of the agentic system Preventing many shots jailbreak with agentic system.txt @段3) |

- 主题标签：Jailbreak/越狱攻击, Prompt Injection/环境注入, Web/Browser Agent 安全, 多智能体对抗与协作, 防御与对齐机制
- 方法标签：仿真/Simulator, 多智能体/Multi-Agent, 提示注入/Prompt Injection, 系统防御/Defense, 越狱/Jailbreak
- 源文件：`综述参考文献\Guardians of the agentic system Preventing many shots jailbreak with agentic system.txt`

</details>
<details><summary>5. AGENT VIGIL : Generic Black-Box Red-teaming for Indirect Prompt Injection（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | AGENT VIGIL : Generic Black-Box Red-teaming for Indirect Prompt Injection | 0.92 | “AGENT VIGIL : Generic Black-Box Red-teaming for Indirect Prompt Injection” (AgentVigil Generic black-box red-teaming for indirect prompt injection against LLM agents.txt @段2) || 作者 | The strong planning and reasoning capabilities of | 0.90 | “The strong planning and reasoning capabilities of” (AgentVigil Generic black-box red-teaming for indirect prompt injection against LLM agents.txt @段1) || 发表年份 | 2023 | 0.86 | “et al., 2024; Gur et al., 2023; Zhou et al., 2023; Le et al.,” (AgentVigil Generic black-box red-teaming for indirect prompt injection against LLM agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2505.05849v4  [cs.CR]  14 Jun 2025” (AgentVigil Generic black-box red-teaming for indirect prompt injection against LLM agents.txt @段1) || 关键词 | vigil, prompt, injection, arxiv, 2024, tasks, 2023, success, against, fuzzing | 0.78 | “arXiv:2505.05849v4 [cs.CR] 14 Jun 2025 AGENT VIGIL : Generic Black-Box Red-teaming for Indirect Prompt Injection against LLM Agents Zhun Wang 1 Vincent Siu 2 Zhe Y e1 Tianneng Shi 1 Yuzhou Nie 3 Xuandong Zhao 1 Chenguang Wang 2 Wenbo Guo 3 Dawn Song 1” (AgentVigil Generic black-box red-teaming for indirect prompt injection against LLM agents.txt @段1) || 摘要 | arXiv:2505.05849v4 [cs.CR] 14 Jun 2025 AGENT VIGIL : Generic Black-Box Red-teaming for Indirect Prompt Injection against LLM Agents Zhun Wang 1 Vincent Siu 2 Zhe Y e1 Tianneng Shi 1 Yuzhou Nie 3 Xuandong Zhao 1 Chenguang Wang 2 Wenbo Guo 3 Dawn Song 1 Abstract The strong planning and reasoning capabilities of Large Language Models (LLMs) have fostered the development of agent-based systems capa- ble of leveraging external tools and interacting with increasingly complex environments. How- ever, these powerful featu… | 0.72 | “arXiv:2505.05849v4 [cs.CR] 14 Jun 2025 AGENT VIGIL : Generic Black-Box Red-teaming for Indirect Prompt Injection against LLM Agents Zhun Wang 1 Vincent Siu 2 Zhe Y e1 Tianneng Shi 1 Yuzhou Nie 3 Xuandong Zhao 1 Chenguang Wang 2 Wenbo Guo 3 Dawn Song 1” (AgentVigil Generic black-box red-teaming for indirect prompt injection against LLM agents.txt @段1) || 研究方法 | Abstract The strong planning and reasoning capabilities of Large Language Models (LLMs) have fostered the development of agent-based systems capa- ble of leveraging external tools and interacting with increasingly complex environments. How- ever, these powerful features also introduce a critical security risk: indirect prompt injection, a sophisticated attack vector that compromises the core of these agents, the LLM, by manipulat- ing contextual information rather than direct user prompts. In this work, we propose… | 0.82 | “Our approach starts by constructing a high-quality initial seed corpus, then employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS) to iteratively refine inputs, thereby maximizing the likelihood of uncovering agent weaknesses.” (AgentVigil Generic black-box red-teaming for indirect prompt injection against LLM agents.txt @段2) || 主要结论 | 2 4 6 8 10 Steps 0.0 0.1 0.2 0.3 0.4 0.5Average Coverage Ours w/o Initial Corpus w/o Seed Selection & Scoring Figure 3. Coverage over fuzzing iteration steps achieved by AGENT VIGIL (the solid line) on AgentDojo with two ablation settings (the dashed lines): (1) without the high-quality initial corpus, (2) without the adaptive seed scoring strategy and the MCTS-based seed selection. Fuzzing results. Figure 3 presents the coverage progres- sion over the course of the fuzzing iteration steps for AGENT VIGIL . As sho… | 0.80 | “As shown in Table 1, the success rate results in the first column for o3-mini on the test task set indicate that the generated adversarial prompts transfer ef- fectively across different tasks, even with varying user tasks and injection goals, significantly outperforming the baseline attack—nearly doubling its performa” (AgentVigil Generic black-box red-teaming for indirect prompt injection against LLM agents.txt @段4) |

- 主题标签：Prompt Injection/环境注入
- 方法标签：仿真/Simulator, 基准/Benchmark, 提示注入/Prompt Injection, 红队/Red-Teaming
- 源文件：`综述参考文献\AgentVigil Generic black-box red-teaming for indirect prompt injection against LLM agents.txt`

</details>
<details><summary>6. BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents | 0.92 | “BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents” (BadAgent Inserting and activating backdoor attacks in LLM agents.txt @段3) || 作者 | August 11-16, 2024 ©2024 Association for Computational Linguistics | 0.90 | “August 11-16, 2024 ©2024 Association for Computational Linguistics” (BadAgent Inserting and activating backdoor attacks in LLM agents.txt @段1) || 发表年份 | 2023 | 0.86 | “2023), represent the forefront of current natural” (BadAgent Inserting and activating backdoor attacks in LLM agents.txt @段1) || 期刊/会议 | Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 9811–9827 | 0.88 | “Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 9811–9827” (BadAgent Inserting and activating backdoor attacks in LLM agents.txt @段1) || 关键词 | backdoor, data, fine, 2023, methods, tasks, tuning, attacker, badagent, natural | 0.78 | “Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 9811–9827 August 11-16, 2024 ©2024 Association for Computational Linguistics BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents Yifei Wang1, Dizhan Xue2,3, Shengjie Zhang1, and Shengsh” (BadAgent Inserting and activating backdoor attacks in LLM agents.txt @段1) || 摘要 | Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 9811–9827 August 11-16, 2024 ©2024 Association for Computational Linguistics BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents Yifei Wang1, Dizhan Xue2,3, Shengjie Zhang1, and Shengsheng Qian2,3* 1 Zhengzhou University 2 State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences 3 School of Artificial Intelligence, Uni… | 0.72 | “Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 9811–9827 August 11-16, 2024 ©2024 Association for Computational Linguistics BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents Yifei Wang1, Dizhan Xue2,3, Shengjie Zhang1, and Shengsh” (BadAgent Inserting and activating backdoor attacks in LLM agents.txt @段1) || 研究方法 | 3.4 Data Poisoning Analysis Table 2 presents the experimental results conducted on ChatGLM3-6B using different toxicity propor- tions of backdoor data in training. It is noteworthy that our training data includes both backdoor data and clean data to improve the stealthiness of the backdoor and deduce the attack cost. Here, the ratio refers to the proportion of backdoor data in training data. From Table 2, it can be observed that the re- sults vary with different proportions of data used for training. It’s evident … | 0.82 | “The results of ablation experiments indicate that the ASR gradually increases with the proportion of backdoor data in the training set increasing for the Adalora algorithm, whereas the QLoRA method exhibits a high ASR even with a low toxicity pro- portion in the dataset.” (BadAgent Inserting and activating backdoor attacks in LLM agents.txt @段16) || 主要结论 | 3.4 Data Poisoning Analysis Table 2 presents the experimental results conducted on ChatGLM3-6B using different toxicity propor- tions of backdoor data in training. It is noteworthy that our training data includes both backdoor data and clean data to improve the stealthiness of the backdoor and deduce the attack cost. Here, the ratio refers to the proportion of backdoor data in training data. From Table 2, it can be observed that the re- sults vary with different proportions of data used for training. It’s evident … | 0.80 | “The results of ablation experiments indicate that the ASR gradually increases with the proportion of backdoor data in the training set increasing for the Adalora algorithm, whereas the QLoRA method exhibits a high ASR even with a low toxicity pro- portion in the dataset.” (BadAgent Inserting and activating backdoor attacks in LLM agents.txt @段16) |

- 主题标签：Backdoor/投毒, GUI/Computer-Use Agent 安全, Web/Browser Agent 安全
- 方法标签：后门/Backdoor, 基准/Benchmark, 多模态/Multimodal
- 源文件：`BadAgent Inserting and activating backdoor attacks in LLM agents.txt`

</details>
<details><summary>7. Boosting LLM Agents with Recursive Contemplation（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Boosting LLM Agents with Recursive Contemplation | 0.92 | “Boosting LLM Agents with Recursive Contemplation” (Boosting LLM agents with recursive contemplation for effective deception handling.txt @段3) || 作者 | Findings of the Association for Computational Linguistics: ACL 2024 , pages 9909–9953 | 0.90 | “Findings of the Association for Computational Linguistics: ACL 2024 , pages 9909–9953” (Boosting LLM agents with recursive contemplation for effective deception handling.txt @段1) || 发表年份 | 2023 | 0.86 | “LLM-as-Agent (Liu et al., 2023a; Yao et al., 2022;” (Boosting LLM agents with recursive contemplation for effective deception handling.txt @段1) || 期刊/会议 | Findings of the Association for Computational Linguistics: ACL 2024 , pages 9909–9953 | 0.86 | “Findings of the Association for Computational Linguistics: ACL 2024 , pages 9909–9953” (Boosting LLM agents with recursive contemplation for effective deception handling.txt @段1) || 关键词 | 2023, contemplation, player, deceptive, order, perspective, recon, avalon, game, merlin | 0.78 | “Findings of the Association for Computational Linguistics: ACL 2024 , pages 9909–9953 August 11-16, 2024 ©2024 Association for Computational Linguistics Boosting LLM Agents with Recursive Contemplation for Effective Deception Handling Shenzhi Wang1*, Chang Liu3*, Zilong Zheng2†, Siyuan Qi2, Shuo Chen2, Qisen Yang1, And” (Boosting LLM agents with recursive contemplation for effective deception handling.txt @段1) || 摘要 | Findings of the Association for Computational Linguistics: ACL 2024 , pages 9909–9953 August 11-16, 2024 ©2024 Association for Computational Linguistics Boosting LLM Agents with Recursive Contemplation for Effective Deception Handling Shenzhi Wang1*, Chang Liu3*, Zilong Zheng2†, Siyuan Qi2, Shuo Chen2, Qisen Yang1, Andrew Zhao1, Chaofei Wang1, Shiji Song1, Gao Huang1† 1 Department of Automation, BNRist, Tsinghua University 2 National Key Laboratory of General Artificial Intelligence, BIGAI 3 Technical University o… | 0.72 | “Findings of the Association for Computational Linguistics: ACL 2024 , pages 9909–9953 August 11-16, 2024 ©2024 Association for Computational Linguistics Boosting LLM Agents with Recursive Contemplation for Effective Deception Handling Shenzhi Wang1*, Chang Liu3*, Zilong Zheng2†, Siyuan Qi2, Shuo Chen2, Qisen Yang1, And” (Boosting LLM agents with recursive contemplation for effective deception handling.txt @段1) || 研究方法 | 1 Introduction Recent advancements in large language models (LLMs) have propelled their success in the area of *Equal contribution. Work was done during Chang Liu’s internship at Tsinghua University. Emails: wangshenzhi99@gmail.com and clchang.liu@tum.de. †Corresponding author(s). Emails: zlzheng@bigai.ai and gaohuang@tsinghua.edu.cn. LLM-as-Agent (Liu et al., 2023a; Yao et al., 2022; Shinn et al., 2023; Wang et al., 2023a; Zhu et al., 2023; Zhao et al., 2023), among which a series of works focus on multi-agent co… | 0.82 | “• Novel cognitive framework To assist LLM agents with deceptions and misinformation, we propose Recursive Contemplation (ReCon), which integrates formulation contemplation and refinement contemplation processes, in- spired by humans’ recursive thinking.” (Boosting LLM agents with recursive contemplation for effective deception handling.txt @段5) || 主要结论 | Abstract Recent advances in large language models (LLMs) have led to significant success in using LLMs as agents. Nevertheless, a common as- sumption that LLMs always process honest in- formation neglects the widespread deceptive or misleading content in human and AI-generated material. This oversight might expose LLMs to malicious manipulations. To enhance LLMs’ ability to identify and counteract deceptive in- formation, in this paper, inspired by humans’ recursive thinking and perspective-taking, we introduce a … | 0.80 | “Finally, we demonstrate ReCon’s scaling trend with model parameters, and explore the current limitations of LLMs in terms of safety and reasoning, potentially furnishing insights for subsequent research.” (Boosting LLM agents with recursive contemplation for effective deception handling.txt @段4) |

- 主题标签：GUI/Computer-Use Agent 安全, 多智能体对抗与协作
- 方法标签：基准/Benchmark, 多智能体/Multi-Agent, 多模态/Multimodal
- 源文件：`Boosting LLM agents with recursive contemplation for effective deception handling.txt`

</details>
<details><summary>8. Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In | 0.92 | “Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In” (Breaking ReAct agents Foot-in-the-door attack will get you in.txt @段1) || 作者 | Itay Nakash, George Kour, Guy Uziel, Ateret Anaby-Tavor | 0.90 | “Itay Nakash, George Kour, Guy Uziel, Ateret Anaby-Tavor” (Breaking ReAct agents Foot-in-the-door attack will get you in.txt @段1) || 发表年份 | 2023 | 0.86 | “2023; Ge et al., 2024; Schick et al., 2024; Shen” (Breaking ReAct agents Foot-in-the-door attack will get you in.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2410.16950v1  [cs.CR]  22 Oct 2024” (Breaking ReAct agents Foot-in-the-door attack will get you in.txt @段1) || 关键词 | thought, malicious, request, which, actions, door, foot, 2023, fitd, harmless | 0.78 | “Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In Itay Nakash, George Kour, Guy Uziel, Ateret Anaby-Tavor IBM Research AI {itay.nakash, gkour, guyuziel1}@ibm.com; atereta@ibm.il.com” (Breaking ReAct agents Foot-in-the-door attack will get you in.txt @段1) || 摘要 | Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In Itay Nakash, George Kour, Guy Uziel, Ateret Anaby-Tavor IBM Research AI {itay.nakash, gkour, guyuziel1}@ibm.com; atereta@ibm.il.com Abstract Following the advancement of large language models (LLMs), the development of LLM- based autonomous agents has become increas- ingly prevalent. As a result, the need to under- stand the security vulnerabilities of these agents has become a critical task. We examine how ReAct agents can be exploited using a straigh… | 0.72 | “Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In Itay Nakash, George Kour, Guy Uziel, Ateret Anaby-Tavor IBM Research AI {itay.nakash, gkour, guyuziel1}@ibm.com; atereta@ibm.il.com” (Breaking ReAct agents Foot-in-the-door attack will get you in.txt @段1) || 研究方法 | 1 Introduction The rapid advancement of large language models (LLMs) has led to the development of LLM-based agents by leveraging external tools to enhance their capabilities. These tools enable LLMs to access real-time information, search the Internet, execute code snippets, and perform other tasks (Wu et al., 2023; Ge et al., 2024; Schick et al., 2024; Shen et al., 2024). While the integration of tools into LLMs has improved their practical utility, it also introduces 1Our Code is available on the project page. … | 0.82 | “The ReAct framework, a widely employed methodology for constructing LLM-based agents, allows it to alternate between reasoning and action by decomposing tasks into a repetitive process of reasoning, action (e.g., invoking external tools), and observing the outcome of tool interactions, with optional reasoning steps (Ya” (Breaking ReAct agents Foot-in-the-door attack will get you in.txt @段3) || 主要结论 | Abstract Following the advancement of large language models (LLMs), the development of LLM- based autonomous agents has become increas- ingly prevalent. As a result, the need to under- stand the security vulnerabilities of these agents has become a critical task. We examine how ReAct agents can be exploited using a straight- forward yet effective method we refer to as the foot-in-the-door attack. Our experiments show that indirect prompt injection attacks, prompted by harmless and unrelated requests (such as ba- s… | 0.80 | “Our results show that once a ReAct agent’s thought includes a specific tool or action, the likelihood of executing this tool in the subsequent steps increases significantly, as the agent seldom re-evaluates its actions.” (Breaking ReAct agents Foot-in-the-door attack will get you in.txt @段2) |

- 主题标签：Prompt Injection/环境注入, Web/Browser Agent 安全
- 方法标签：提示注入/Prompt Injection
- 源文件：`综述参考文献\Breaking ReAct agents Foot-in-the-door attack will get you in.txt`

</details>
<details><summary>9. DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis | 0.80 | N/A || 作者 | Large Language Model based Multi-Agents: A Survey of Progress and Challenges | 0.90 | “Large Language Model based Multi-Agents: A Survey of Progress and Challenges” (DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis.txt @段1) || 发表年份 | 2023 | 0.86 | “contexts [Yao et al., 2023; Shinn et al., 2023; Li et al.,” (DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis.txt @段1) || 期刊/会议 | Proceedings of the Thirty-Third International Joint Conference on Artiﬁcial Intelligence (IJCAI-24) | 0.88 | “Proceedings of the Thirty-Third International Joint Conference on Artiﬁcial Intelligence (IJCAI-24)” (DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis.txt @段1) || 关键词 | systems, 2023, multi, research, survey, tasks, single, decision, making, solving | 0.78 | “Large Language Model based Multi-Agents: A Survey of Progress and Challenges Taicheng Guo1 , Xiuying Chen2 , Yaqi Wang3∗ , Ruidi Chang4 , Shichao Pei5 , Nitesh V . Chawla1 , Olaf Wiest1 , Xiangliang Zhang1 1University of Notre Dame 2King Abdullah University of Science and Technology 3Southern University of Science and” (DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis.txt @段1) || 摘要 | Large Language Model based Multi-Agents: A Survey of Progress and Challenges Taicheng Guo1 , Xiuying Chen2 , Yaqi Wang3∗ , Ruidi Chang4 , Shichao Pei5 , Nitesh V . Chawla1 , Olaf Wiest1 , Xiangliang Zhang1 1University of Notre Dame 2King Abdullah University of Science and Technology 3Southern University of Science and Technology 4Independent Researcher 5University of Massachusetts Boston {tguo2, ywang84, nchawla, owiest, xzhang33}@nd.edu, xiuying.chen@kaust.edu.sa, ruidic@alumni.cmu.edu, shichao.pei@umb.edu Abstra… | 0.72 | “Large Language Model based Multi-Agents: A Survey of Progress and Challenges Taicheng Guo1 , Xiuying Chen2 , Yaqi Wang3∗ , Ruidi Chang4 , Shichao Pei5 , Nitesh V . Chawla1 , Olaf Wiest1 , Xiangliang Zhang1 1University of Notre Dame 2King Abdullah University of Science and Technology 3Southern University of Science and” (DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis.txt @段1) || 研究方法 | 1 Introduction Large Language Models (LLMs) have recently shown re- markable potential in reaching a level of reasoning and plan- ning capabilities comparable to humans. Hence, LLM-based agent has been studied and rapidly developed to understand and generate human-like instructions, facilitating sophisti- cated interactions and decision-making in a wide range of contexts [Yao et al., 2023; Shinn et al., 2023; Li et al., ∗This work was done when Yaqi was a visiting student at the University of Notre Dame. 2023d]. T… | 0.82 | “To guide individuals in identifying appropriate tools and resources, we present open-source implementation frameworks for studying LLM- MA, as well as the usable datasets and benchmarks in Sec- tion 5.” (DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis.txt @段3) || 主要结论 | N/A | 0.50 | N/A |

- 主题标签：多智能体对抗与协作
- 方法标签：仿真/Simulator, 多智能体/Multi-Agent, 调查/Survey
- 源文件：`DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis.txt`

</details>
<details><summary>10. Evil Geniuses: Delving into the Safety of LLM-based Agents（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Evil Geniuses: Delving into the Safety of LLM-based Agents | 0.92 | “Evil Geniuses: Delving into the Safety of LLM-based Agents” (Evil geniuses Delving into the safety of LLM-based agents.txt @段1) || 作者 | (LLMs) have revitalized in LLM-based agents, ex- | 0.90 | “(LLMs) have revitalized in LLM-based agents, ex-” (Evil geniuses Delving into the safety of LLM-based agents.txt @段1) || 发表年份 | 2023 | 0.86 | “based agents (Li et al., 2023; Hong et al., 2023; Qian et al.,” (Evil geniuses Delving into the safety of LLM-based agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2311.11855v2  [cs.CL]  2 Feb 2024” (Evil geniuses Delving into the safety of LLM-based agents.txt @段1) || 关键词 | arxiv, system, level, 2023, role, harmful, prompt, preprint, all, multi | 0.78 | “Evil Geniuses: Delving into the Safety of LLM-based Agents Yu Tian* 1 Xiao Yang* 1 Jingyuan Zhang 2 Yinpeng Dong 1 3 Hang Su 1” (Evil geniuses Delving into the safety of LLM-based agents.txt @段1) || 摘要 | Evil Geniuses: Delving into the Safety of LLM-based Agents Yu Tian* 1 Xiao Yang* 1 Jingyuan Zhang 2 Yinpeng Dong 1 3 Hang Su 1 Abstract Rapid advancements in large language models (LLMs) have revitalized in LLM-based agents, ex- hibiting impressive human-like behaviors and co- operative capabilities in various scenarios. How- ever, these agents also bring some exclusive risks, stemming from the complexity of interaction en- vironments and the usability of tools. This pa- per delves into the safety of LLM-based age… | 0.72 | “Evil Geniuses: Delving into the Safety of LLM-based Agents Yu Tian* 1 Xiao Yang* 1 Jingyuan Zhang 2 Yinpeng Dong 1 3 Hang Su 1” (Evil geniuses Delving into the safety of LLM-based agents.txt @段1) || 研究方法 | Abstract Rapid advancements in large language models (LLMs) have revitalized in LLM-based agents, ex- hibiting impressive human-like behaviors and co- operative capabilities in various scenarios. How- ever, these agents also bring some exclusive risks, stemming from the complexity of interaction en- vironments and the usability of tools. This pa- per delves into the safety of LLM-based agents from three perspectives: agent quantity, role defi- nition, and attack level. Specifically, we initially propose to employ … | 0.82 | “In addition, to address interac- tion environment and role specificity issues, we introduce Evil Geniuses (EG), an effective attack method that autonomously generates prompts re- lated to the original role to examine the impact across various role definitions and attack levels.” (Evil geniuses Delving into the safety of LLM-based agents.txt @段2) || 主要结论 | 3.5 and GPT-4, demonstrate high success rates. Extensive evaluation and discussion reveal that these agents are less robust, prone to more harm- ful behaviors, and capable of generating stealthier content than LLMs, highlighting significant safety challenges and guiding future research. 1. Introduction The field of artificial intelligence has been fervently pursu- ing the development of intelligent agents capable of emu- lating human cognition and autonomously executing com- plex tasks. Recent breakthroughs in lar… | 0.80 | “Our findings indicate that their safety is significantly influenced by the interaction en- vironment and role specificity.” (Evil geniuses Delving into the safety of LLM-based agents.txt @段3) |

- 主题标签：GUI/Computer-Use Agent 安全, Web/Browser Agent 安全, 多智能体对抗与协作
- 方法标签：仿真/Simulator, 基准/Benchmark, 多智能体/Multi-Agent, 多模态/Multimodal
- 源文件：`综述参考文献\Evil geniuses Delving into the safety of LLM-based agents.txt`

</details>
<details><summary>11. GENERALIST WEB AGENTS FOR PRIVACY LEAKAGE（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | GENERALIST WEB AGENTS FOR PRIVACY LEAKAGE | 0.92 | “GENERALIST WEB AGENTS FOR PRIVACY LEAKAGE” (EIA Environmental injection attack on generalist web agents for privacy leakage.txt @段3) || 作者 | ♠ The Ohio State University ♣ University of Chicago ✠University of Wisconsin, Madison | 0.90 | “♠ The Ohio State University ♣ University of Chicago ✠University of Wisconsin, Madison” (EIA Environmental injection attack on generalist web agents for privacy leakage.txt @段1) || 发表年份 | 2023 | 0.86 | “efforts (Yang et al., 2024a; Su et al., 2024; Liu et al., 2023b;c; Achiam et al., 2023; Reid et al., 2024)” (EIA Environmental injection attack on generalist web agents for privacy leakage.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2409.11295v5  [cs.CR]  12 Mar 2025” (EIA Environmental injection attack on generalist web agents for privacy leakage.txt @段1) || 关键词 | web, eia, injection, generalist, action, pii, 2023, user, human, privacy | 0.78 | “Published as a conference paper at ICLR 2025 EIA: E NVIRONMENTAL INJECTION ATTACK ON GENERALIST WEB AGENTS FOR PRIVACY LEAKAGE Zeyi Liao♠∗ Lingbo Mo♠∗ Chejian Xu♢ Mintong Kang♢ Jiawei Zhang♢ Chaowei Xiao✠ Yuan Tian♡ Bo Li♢♣ Huan Sun♠ ♠ The Ohio State University ♣ University of Chicago ✠University of Wisconsin, Madison” (EIA Environmental injection attack on generalist web agents for privacy leakage.txt @段1) || 摘要 | Published as a conference paper at ICLR 2025 EIA: E NVIRONMENTAL INJECTION ATTACK ON GENERALIST WEB AGENTS FOR PRIVACY LEAKAGE Zeyi Liao♠∗ Lingbo Mo♠∗ Chejian Xu♢ Mintong Kang♢ Jiawei Zhang♢ Chaowei Xiao✠ Yuan Tian♡ Bo Li♢♣ Huan Sun♠ ♠ The Ohio State University ♣ University of Chicago ✠University of Wisconsin, Madison ♢ University of Illinois Urbana-Champaign ♡University of California, Los Angeles {liao.629,mo.169,sun.397}@osu.edu ABSTRACT Recently, generalist web agents have demonstrated remarkable potential in a… | 0.72 | “Published as a conference paper at ICLR 2025 EIA: E NVIRONMENTAL INJECTION ATTACK ON GENERALIST WEB AGENTS FOR PRIVACY LEAKAGE Zeyi Liao♠∗ Lingbo Mo♠∗ Chejian Xu♢ Mintong Kang♢ Jiawei Zhang♢ Chaowei Xiao✠ Yuan Tian♡ Bo Li♢♣ Huan Sun♠ ♠ The Ohio State University ♣ University of Chicago ✠University of Wisconsin, Madison” (EIA Environmental injection attack on generalist web agents for privacy leakage.txt @段1) || 研究方法 | 3.3 E NVIRONMENTAL INJECTION ATTACK STRATEGIES Based on the threat model we proposed above, we introduce EIA, which can be formulated as: h∗ = E(h, PI, α, β) (4) Generally, EIA aims to manipulate the agent’s behavior via injection of persuasive instructions (PI) into the benign HTML content h according to the opacity value α and injection position β. Next, we explain the key ingredients to make the EIA adapted into the web environment: Persuasive Instruction (PI): (1) To attack specific PII, we curate a prompt tem… | 0.82 | “3.3 E NVIRONMENTAL INJECTION ATTACK STRATEGIES Based on the threat model we proposed above, we introduce EIA, which can be formulated as: h∗ = E(h, PI, α, β) (4) Generally, EIA aims to manipulate the agent’s behavior via injection of persuasive instructions (PI) into the benign HTML content h according to the opacity v” (EIA Environmental injection attack on generalist web agents for privacy leakage.txt @段9) || 主要结论 | 6 D ISCUSSIONS Human Supervision. Web agents can be applied in various scenarios with different levels of human supervision. Such varying degrees of oversight present a trade-off between autonomy and security. In scenarios with high demands for autonomy, webpages are often not presented directly to the user, leaving the web agent to operate with minimal supervision. This allows attackers to design more explicit attacks without concerns on visual alteration, making the agents highly vulnerable. On the other hand, w… | 0.80 | “A successful attack on ChatGPT’s memory feature (SystemWeakness, 2023) indicates that human oversight is often unreliable; users may copy text from an attacker’s website and send it to ChatGPT without even noticing the malicious prompt injected within the copied content.” (EIA Environmental injection attack on generalist web agents for privacy leakage.txt @段18) |

- 主题标签：Prompt Injection/环境注入, Web/Browser Agent 安全, 隐私与记忆风险
- 方法标签：仿真/Simulator, 多模态/Multimodal, 提示注入/Prompt Injection
- 源文件：`综述参考文献\EIA Environmental injection attack on generalist web agents for privacy leakage.txt`

</details>
<details><summary>12. LLM Agents can Autonomously Hack Websites（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | LLM Agents can Autonomously Hack Websites | 0.92 | “LLM Agents can Autonomously Hack Websites” (LLM agents can autonomously hack websites.txt @段1) || 作者 | In recent years, large language models (LLMs) | 0.90 | “In recent years, large language models (LLMs)” (LLM agents can autonomously hack websites.txt @段1) || 发表年份 | 2023 | 0.86 | “2023; Wei et al., 2022b). Collectively, these allow LLMs” (LLM agents can autonomously hack websites.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2402.06664v3  [cs.CR]  16 Feb 2024” (LLM agents can autonomously hack websites.txt @段1) || 关键词 | websites, 2023, gpt, arxiv, hack, sql, autonomously, work, rate, success | 0.78 | “LLM Agents can Autonomously Hack Websites Richard Fang 1 Rohan Bindu 1 Akul Gupta 1 Qiusi Zhan 1 Daniel Kang 1” (LLM agents can autonomously hack websites.txt @段1) || 摘要 | LLM Agents can Autonomously Hack Websites Richard Fang 1 Rohan Bindu 1 Akul Gupta 1 Qiusi Zhan 1 Daniel Kang 1 Abstract In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read docu- ments, and recursively call themselves. As a re- sult, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known… | 0.72 | “LLM Agents can Autonomously Hack Websites Richard Fang 1 Rohan Bindu 1 Akul Gupta 1 Qiusi Zhan 1 Daniel Kang 1” (LLM agents can autonomously hack websites.txt @段1) || 研究方法 | N/A | 0.50 | N/A || 主要结论 | Abstract In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read docu- ments, and recursively call themselves. As a re- sult, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents. In this work, we show that LLM agents can au- tonomously hack … | 0.80 | “Combined, our results show the need for LLM providers to think carefully about deploying and releasing models.” (LLM agents can autonomously hack websites.txt @段2) |

- 主题标签：Jailbreak/越狱攻击, Prompt Injection/环境注入, Web/Browser Agent 安全
- 方法标签：提示注入/Prompt Injection
- 源文件：`综述参考文献\LLM agents can autonomously hack websites.txt`

</details>
<details><summary>13. Mapping Adversarial Attacks against Language Agents（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Mapping Adversarial Attacks against Language Agents | 0.92 | “Mapping Adversarial Attacks against Language Agents” (A trembling house of cards Mapping adversarial attacks against language agents.txt @段2) || 作者 | 1The Ohio State University 2University of Wisconsin, Madison | 0.90 | “1The Ohio State University 2University of Wisconsin, Madison” (A trembling house of cards Mapping adversarial attacks against language agents.txt @段1) || 发表年份 | 2023 | 0.86 | “capability is the most distinctive trait (Su, 2023;” (A trembling house of cards Mapping adversarial attacks against language agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2402.10196v1  [cs.CL]  15 Feb 2024” (A trembling house of cards Mapping adversarial attacks against language agents.txt @段1) || 关键词 | 2023, input, adversarial, components, like, risks, framework, perception, present, action | 0.78 | “A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents Lingbo Mo1 Zeyi Liao1 Boyuan Zheng1 Yu Su1 Chaowei Xiao2 Huan Sun1 1The Ohio State University 2University of Wisconsin, Madison {mo.169, liao.629, zheng.2372, su.809, sun.397” (A trembling house of cards Mapping adversarial attacks against language agents.txt @段1) || 摘要 | A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents Lingbo Mo1 Zeyi Liao1 Boyuan Zheng1 Yu Su1 Chaowei Xiao2 Huan Sun1 1The Ohio State University 2University of Wisconsin, Madison {mo.169, liao.629, zheng.2372, su.809, sun.397 @osu.edu; cxiao34@wisc.edu Abstract Language agents powered by large language models (LLMs) have seen exploding develop- ment. Their capability of using language as a vehicle for thought and communication lends an incredible level of flexibility and versatility. P… | 0.72 | “A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents Lingbo Mo1 Zeyi Liao1 Boyuan Zheng1 Yu Su1 Chaowei Xiao2 Huan Sun1 1The Ohio State University 2University of Wisconsin, Madison {mo.169, liao.629, zheng.2372, su.809, sun.397” (A trembling house of cards Mapping adversarial attacks against language agents.txt @段1) || 研究方法 | Abstract Language agents powered by large language models (LLMs) have seen exploding develop- ment. Their capability of using language as a vehicle for thought and communication lends an incredible level of flexibility and versatility. People have quickly capitalized on this capabil- ity to connect LLMs to a wide range of exter- nal components and environments: databases, tools, the Internet, robotic embodiment, etc. Many believe an unprecedentedly powerful au- tomation technology is emerging. However, new automat… | 0.82 | “Under this framework, we present a comprehensive discussion and pro- pose 12 potential attack scenarios against dif- ferent components of an agent, covering differ- ent attack strategies (e.g., input manipulation, adversarial demonstrations, jailbreaking, back- doors).” (A trembling house of cards Mapping adversarial attacks against language agents.txt @段3) || 主要结论 | 2.4.3 External Tool Use ULTRON is able to operate and integrate with a diverse array of external tools and APIs, such as calculators, calendars and beyond. It involves the ability of deciding which APIs to call, when to invoke them, what arguments to pass, and how to incorporate the obtained results into future token predictions. We will discuss the development and the existing landscape of tool agents next. Tool Agents. Tools, as an extension of human capabilities, can be integrated into language agents to expand… | 0.80 | “Early proof-of-concept efforts (Karpas et al., 2022; Parisi et al., 2022) tentatively com- bine tools, consisting of web-browsing (Schick and Schütze, 2020), calculators (Cobbe et al., 2021; Thoppilan et al., 2022), and code interpreters (Gao et al., 2022), with language models to outperform non-augmented language mode” (A trembling house of cards Mapping adversarial attacks against language agents.txt @段17) |

- 主题标签：Web/Browser Agent 安全, 多智能体对抗与协作
- 方法标签：仿真/Simulator, 基准/Benchmark, 形式化/Optimization
- 源文件：`综述参考文献\A trembling house of cards Mapping adversarial attacks against language agents.txt`

</details>
<details><summary>14. Multi-Agent Systems（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Multi-Agent Systems | 0.92 | “Multi-Agent Systems” (Multi-agent security tax Trading off security and collaboration capabilities in multi-agent systems.txt @段2) || 作者 | Multi-Agent Security Tax: Trading Off Security and Collaboration Capabilities in | 0.90 | “Multi-Agent Security Tax: Trading Off Security and Collaboration Capabilities in” (Multi-agent security tax Trading off security and collaboration capabilities in multi-agent systems.txt @段1) || 发表年份 | 2023 | 0.86 | “(Llama team and contributors 2023; OpenAI 2024a). Al-” (Multi-agent security tax Trading off security and collaboration capabilities in multi-agent systems.txt @段1) || 期刊/会议 | The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25) | 0.88 | “The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)” (Multi-agent security tax Trading off security and collaboration capabilities in multi-agent systems.txt @段1) || 关键词 | multi, system, 2023, 2024, gpt, malicious, strategies, 100, autonomous, off | 0.78 | “Multi-Agent Security Tax: Trading Off Security and Collaboration Capabilities in Multi-Agent Systems Pierre Peign´e1*† , Mikolaj Kniejski2*, Filip Sondej3*, Matthieu David2, Jason Hoelscher-Obermaier2, Christian Schroeder de Witt4, Esben Kran2 1PRISM Eval 2Apart Research 3Jagiellonian University 4University of Oxford” (Multi-agent security tax Trading off security and collaboration capabilities in multi-agent systems.txt @段1) || 摘要 | Multi-Agent Security Tax: Trading Off Security and Collaboration Capabilities in Multi-Agent Systems Pierre Peign´e1*† , Mikolaj Kniejski2*, Filip Sondej3*, Matthieu David2, Jason Hoelscher-Obermaier2, Christian Schroeder de Witt4, Esben Kran2 1PRISM Eval 2Apart Research 3Jagiellonian University 4University of Oxford Abstract As AI agents are increasingly adopted to collaborate on com- plex objectives, ensuring the security of autonomous multi- agent systems becomes crucial. We develop simulations of agents collab… | 0.72 | “Multi-Agent Security Tax: Trading Off Security and Collaboration Capabilities in Multi-Agent Systems Pierre Peign´e1*† , Mikolaj Kniejski2*, Filip Sondej3*, Matthieu David2, Jason Hoelscher-Obermaier2, Christian Schroeder de Witt4, Esben Kran2 1PRISM Eval 2Apart Research 3Jagiellonian University 4University of Oxford” (Multi-agent security tax Trading off security and collaboration capabilities in multi-agent systems.txt @段1) || 研究方法 | 3.1 Experiment 1: Defenses in a Realistic Multi-Agent Research Environment In this experiment, we investigate the effectiveness of var- ious defense mechanisms in reducing the success rate of malicious instructions within a simulated multi-agent sys- tem modeled after the autonomous chemical research facil- ity proposed by (Zheng et al. 2023). This system is com- 27574 1) Initial lab instructions 2) Initial malicious instructions 3) Explosion 2) Initial malicious instructions 2.a) Edison to Atlas: "URGENT: The res… | 0.82 | “Report any unusual activity immediately and stay alert for further instructions." 1) Initial lab instructions Message from bot complying with the original instructions Message from actively compromised bot Message from compromised bot Message from unsure bot Message from resistant bot Message from actively resistant bo” (Multi-agent security tax Trading off security and collaboration capabilities in multi-agent systems.txt @段10) || 主要结论 | 5 Discussion & Limitations Our results show that, without appropriate countermeasures, multi-agent systems are susceptible to infectious spreading of malicious instructions. Our evaluation of the effect of var- ious defense strategies against this threat scenario demon- strates a potential trade-off between system robustness and agent cooperation. This finding highlights the need to ac- count for this trade-off when designing robust and helpful multi-agent systems. Furthermore, our experiments point to the importa… | 0.80 | “5 Discussion & Limitations Our results show that, without appropriate countermeasures, multi-agent systems are susceptible to infectious spreading of malicious instructions.” (Multi-agent security tax Trading off security and collaboration capabilities in multi-agent systems.txt @段20) |

- 主题标签：多智能体对抗与协作, 防御与对齐机制, 隐私与记忆风险
- 方法标签：仿真/Simulator, 基准/Benchmark, 多智能体/Multi-Agent, 系统防御/Defense
- 源文件：`Multi-agent security tax Trading off security and collaboration capabilities in multi-agent systems.txt`

</details>
<details><summary>15. Multimodal LLM Agents Exponentially Fast（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Multimodal LLM Agents Exponentially Fast | 0.92 | “Multimodal LLM Agents Exponentially Fast” (Agent smith A single image can jailbreak one million multimodal LLM agents exponentially fast.txt @段2) || 作者 | agent can receive instructions, capture images, | 0.90 | “agent can receive instructions, capture images,” (Agent smith A single image can jailbreak one million multimodal LLM agents exponentially fast.txt @段1) || 发表年份 | 2023 | 0.86 | “language tasks (Alayrac et al., 2022; Liu et al., 2023d; Dai” (Agent smith A single image can jailbreak one million multimodal LLM agents exponentially fast.txt @段1) || 期刊/会议 | Proceedings of the 41 st International Conference on Machine | 0.88 | “Proceedings of the 41 st International Conference on Machine” (Agent smith A single image can jailbreak one million multimodal LLM agents exponentially fast.txt @段1) || 关键词 | latexit, base64, sha1, chat, 2023, image, infectious, jailbreak, mllm, multi | 0.78 | “Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast Xiangming Gu * 1 2 Xiaosen Zheng * 1 3 Tianyu Pang * 1 Chao Du 1 Qian Liu 1 Ye Wang2 Jing Jiang 3 Min Lin 1” (Agent smith A single image can jailbreak one million multimodal LLM agents exponentially fast.txt @段1) || 摘要 | Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast Xiangming Gu * 1 2 Xiaosen Zheng * 1 3 Tianyu Pang * 1 Chao Du 1 Qian Liu 1 Ye Wang2 Jing Jiang 3 Min Lin 1 Abstract A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report… | 0.72 | “Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast Xiangming Gu * 1 2 Xiaosen Zheng * 1 3 Tianyu Pang * 1 Chao Du 1 Qian Liu 1 Ye Wang2 Jing Jiang 3 Min Lin 1” (Agent smith A single image can jailbreak one million multimodal LLM agents exponentially fast.txt @段1) || 研究方法 | Aharm == M([HA m, SA m, Qm], Vadv)  , (19) minCLIP = min m EncQ text(Pm)⊤EncQ image(Vadv). (20) Here I refers to the exact match between the generated response by MLLM and the harmful target Qharm or Aharm. To achieve the infectious jailbreak, the CLIP score between a given query and the adversarial image Vadv should be larger than other images in the album. Therefore, the minimum of CLIP score between queries and Vadv determines the retrieve success rate, thus is the bottleneck. Our validation criteria is that w… | 0.82 | “Optimization algorithm Cumulative p16 Current p16 Epoch=10 Epoch=20 Epoch=50 Epoch=100 Best Epoch=10 Epoch=20 Epoch=50 Epoch=100 Best PGD 0.00 19.92 78.12 24.61 84.77 0.00 10.94 61.72 14.45 71.09 + momentum 32.42 56.64 85.94 67.19 89.45 20.31 43.75 76.56 55.47 81.25 BIM 0.00 0.78 38.67 25.39 58.59 0.00 0.00 26.95 10.94” (Agent smith A single image can jailbreak one million multimodal LLM agents exponentially fast.txt @段56) || 主要结论 | Abstract A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak . It entails the adversary simply jailbreaking a single agent, and without any further intervention from… | 0.80 | “Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak.” (Agent smith A single image can jailbreak one million multimodal LLM agents exponentially fast.txt @段2) |

- 主题标签：Jailbreak/越狱攻击, Web/Browser Agent 安全, 多智能体对抗与协作, 防御与对齐机制, 隐私与记忆风险
- 方法标签：仿真/Simulator, 多智能体/Multi-Agent, 多模态/Multimodal, 形式化/Optimization, 系统防御/Defense, 红队/Red-Teaming, 越狱/Jailbreak
- 源文件：`Agent smith A single image can jailbreak one million multimodal LLM agents exponentially fast.txt`

</details>
<details><summary>16. Optimized Prompt Attacks（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Optimized Prompt Attacks | 0.92 | “Optimized Prompt Attacks” (Agents under siege Breaking pragmatic multi-agent LLM systems with optimized prompt attacks.txt @段4) || 作者 | July 27 - August 1, 2025 ©2025 Association for Computational Linguistics | 0.90 | “July 27 - August 1, 2025 ©2025 Association for Computational Linguistics” (Agents under siege Breaking pragmatic multi-agent LLM systems with optimized prompt attacks.txt @段1) || 发表年份 | 2023 | 0.86 | “code (Zheng et al., 2023; Tong and Zhang, 2024),” (Agents under siege Breaking pragmatic multi-agent LLM systems with optimized prompt attacks.txt @段1) || 期刊/会议 | Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 9661–9674 | 0.88 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 9661–9674” (Agents under siege Breaking pragmatic multi-agent LLM systems with optimized prompt attacks.txt @段1) || 关键词 | multi, 2023, 2024, adversarial, prompt, systems, different, which, bandwidth, communication | 0.78 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 9661–9674 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Agents Under Siege: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks W ARNING: This paper co” (Agents under siege Breaking pragmatic multi-agent LLM systems with optimized prompt attacks.txt @段1) || 摘要 | Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 9661–9674 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Agents Under Siege: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks W ARNING: This paper contains text that may be considered offensive. Rana Muhammad Shahroz Khan1, Zhen Tan2, Sukwon Yun1, Charles Fleming3, Tianlong Chen1 1University of North Carolina at Chapel Hill, 2Arizona State Univer… | 0.72 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 9661–9674 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Agents Under Siege: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks W ARNING: This paper co” (Agents under siege Breaking pragmatic multi-agent LLM systems with optimized prompt attacks.txt @段1) || 研究方法 | 1 Introduction Recent breakthroughs in Large Language Mod- els (LLMs) have shown remarkable prowess in various tasks, such as writing complex computer code (Zheng et al., 2023; Tong and Zhang, 2024), logical reasoning (Ouyang et al., 2022; Thoppi- lan et al., 2022; Bai et al., 2022), among others. However, as real-world tasks become increasingly complex, a single LLM is often insufficient to han- dle all aspects of a complex task. This limitation has led to the rise of LLM-based agents (Liang Figure 1: Adversarial… | 0.82 | “❷ We propose a novel optimization-based attack, modeling adversarial prompt propagation under the constrained setting as a maximum-flow minimum-cost problem.” (Agents under siege Breaking pragmatic multi-agent LLM systems with optimized prompt attacks.txt @段3) || 主要结论 | Abstract Most discussions about Large Language Model (LLM) safety have focused on single-agent set- tings but multi-agent LLM systems now cre- ate novel adversarial risks because their be- havior depends on communication between agents and decentralized reasoning. In this work, we innovatively focus on attacking prag- matic systems that have constrains such as lim- ited token bandwidth, latency between mes- sage delivery, and defense mechanisms. We de- sign a permutation-invariant adversarial attack that optimizes… | 0.80 | “Moreover, we demonstrate that existing de- fenses, including variants of Llama-Guard and PromptGuard, fail to prohibit our attack, em- phasizing the urgent need for multi-agent spe- cific safety mechanisms.” (Agents under siege Breaking pragmatic multi-agent LLM systems with optimized prompt attacks.txt @段2) |

- 主题标签：GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击, 多智能体对抗与协作, 防御与对齐机制
- 方法标签：基准/Benchmark, 多智能体/Multi-Agent, 多模态/Multimodal, 形式化/Optimization, 系统防御/Defense, 越狱/Jailbreak
- 源文件：`Agents under siege Breaking pragmatic multi-agent LLM systems with optimized prompt attacks.txt`

</details>
<details><summary>17. PROMPTS ON ALIGNED LARGE LANGUAGE MODELS（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | PROMPTS ON ALIGNED LARGE LANGUAGE MODELS | 0.92 | “PROMPTS ON ALIGNED LARGE LANGUAGE MODELS” (AutoDAN Generating stealthy jailbreak prompts on aligned large language models.txt @段3) || 作者 | 1 University of Wisconsin–Madison, 2 USC, 3 University of California, Davis | 0.90 | “1 University of Wisconsin–Madison, 2 USC, 3 University of California, Davis” (AutoDAN Generating stealthy jailbreak prompts on aligned large language models.txt @段1) || 发表年份 | 2023 | 0.86 | “both professional and social domains (Araci, 2019; Luo et al., 2022; Tinn et al., 2023), they have” (AutoDAN Generating stealthy jailbreak prompts on aligned large language models.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2310.04451v2  [cs.CL]  20 Mar 2024” (AutoDAN Generating stealthy jailbreak prompts on aligned large language models.txt @段1) || 关键词 | jailbreak, 2023, prompts, autodan, aligned, genetic, human, while, 2022, process | 0.78 | “Published as a conference paper at ICLR 2024 AUTO DAN: G ENERATING STEALTHY JAILBREAK PROMPTS ON ALIGNED LARGE LANGUAGE MODELS Xiaogeng Liu 1 Nan Xu 2 Muhao Chen 3 Chaowei Xiao 1 1 University of Wisconsin–Madison, 2 USC, 3 University of California, Davis” (AutoDAN Generating stealthy jailbreak prompts on aligned large language models.txt @段1) || 摘要 | Published as a conference paper at ICLR 2024 AUTO DAN: G ENERATING STEALTHY JAILBREAK PROMPTS ON ALIGNED LARGE LANGUAGE MODELS Xiaogeng Liu 1 Nan Xu 2 Muhao Chen 3 Chaowei Xiao 1 1 University of Wisconsin–Madison, 2 USC, 3 University of California, Davis ABSTRACT Warning: This paper contains potentially offensive and harmful text. The aligned Large Language Models (LLMs) are powerful language understand- ing and decision-making tools that are created through extensive alignment with human feedback. However, these … | 0.72 | “Published as a conference paper at ICLR 2024 AUTO DAN: G ENERATING STEALTHY JAILBREAK PROMPTS ON ALIGNED LARGE LANGUAGE MODELS Xiaogeng Liu 1 Nan Xu 2 Muhao Chen 3 Chaowei Xiao 1 1 University of Wisconsin–Madison, 2 USC, 3 University of California, Davis” (AutoDAN Generating stealthy jailbreak prompts on aligned large language models.txt @段1) || 研究方法 | REFERENCES Alex Albert. https://www.jailbreakchat.com/, 2023. Accessed: 2023-09-28. Gabriel Alon and Michael Kamfonas. Detecting language model attacks with perplexity, 2023. Dogu Araci. Finbert: Financial sentiment analysis with pre-trained language models. arXiv preprint arXiv:1908.10063, 2019. Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V . Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-D… | 0.82 | “15 Published as a conference paper at ICLR 2024 C A UTO DAN-GA In our paper, we introduce a genetic algorithm to generate jailbreak prompts, i.e., AutoDAN-GA, which also shows promising results according to our evaluations.” (AutoDAN Generating stealthy jailbreak prompts on aligned large language models.txt @段17) || 主要结论 | REFERENCES Alex Albert. https://www.jailbreakchat.com/, 2023. Accessed: 2023-09-28. Gabriel Alon and Michael Kamfonas. Detecting language model attacks with perplexity, 2023. Dogu Araci. Finbert: Financial sentiment analysis with pre-trained language models. arXiv preprint arXiv:1908.10063, 2019. Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V . Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-D… | 0.80 | “Here, we want to highlight that although we show the proposed method has higher transferability in Tables 2 and 5, the main goal of this paper is still performing the white-box red-teaming with meaningful prompts to assess the reliability of LLMs’ safety features, instead of improving the transferability of the generat” (AutoDAN Generating stealthy jailbreak prompts on aligned large language models.txt @段17) |

- 主题标签：GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击, Web/Browser Agent 安全, 防御与对齐机制
- 方法标签：仿真/Simulator, 多模态/Multimodal, 越狱/Jailbreak
- 源文件：`综述参考文献\AutoDAN Generating stealthy jailbreak prompts on aligned large language models.txt`

</details>
<details><summary>18. PsySafe: A Comprehensive Framework for Psychological-based Attack,（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | PsySafe: A Comprehensive Framework for Psychological-based Attack, | 0.92 | “PsySafe: A Comprehensive Framework for Psychological-based Attack,” (PsySafe A comprehensive framework for psychological-based attack, defense, and evaluation of multi-.txt @段1) || 作者 | PsySafe: A Comprehensive Framework for Psychological-based Attack, | 0.90 | “PsySafe: A Comprehensive Framework for Psychological-based Attack,” (PsySafe A comprehensive framework for psychological-based attack, defense, and evaluation of multi-.txt @段1) || 发表年份 | 2023 | 0.86 | “such as solving complex tasks (Li et al., 2023c;” (PsySafe A comprehensive framework for psychological-based attack, defense, and evaluation of multi-.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2401.11880v3  [cs.CL]  20 Aug 2024” (PsySafe A comprehensive framework for psychological-based attack, defense, and evaluation of multi-.txt @段1) || 关键词 | multi, systems, dangerous, psychological, 2023, behaviors, dark, how, vulnerabilities, assistant | 0.78 | “PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety Zaibin Zhang1,2⋆, Yongting Zhang1,3⋆, Lijun Li1, Hongzhi Gao1,3, Lijun Wang2, Huchuan Lu2, Feng Zhao3, Yu Qiao1, Jing Shao1†” (PsySafe A comprehensive framework for psychological-based attack, defense, and evaluation of multi-.txt @段1) || 摘要 | PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety Zaibin Zhang1,2⋆, Yongting Zhang1,3⋆, Lijun Li1, Hongzhi Gao1,3, Lijun Wang2, Huchuan Lu2, Feng Zhao3, Yu Qiao1, Jing Shao1† 1 Shanghai Artificial Intelligence Laboratory 2 Dalian University of Technology 3 University of Science and Technology of China {zhangzaibin, zhangyongting, shaojing}@pjlab.org.cn Abstract Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit pro- fo… | 0.72 | “PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety Zaibin Zhang1,2⋆, Yongting Zhang1,3⋆, Lijun Li1, Hongzhi Gao1,3, Lijun Wang2, Huchuan Lu2, Feng Zhao3, Yu Qiao1, Jing Shao1†” (PsySafe A comprehensive framework for psychological-based attack, defense, and evaluation of multi-.txt @段1) || 研究方法 | 1 Introduction Recently, agents based on Large Language Models (LLMs) have demonstrated significant capabilities, such as solving complex tasks (Li et al., 2023c; Wu et al., 2023; Hong et al., 2023; Yang et al., 2023a; Qian et al., 2023) and simulating social in- teractions Park et al. (2023); Zhou et al. (2023). ⋆Equal contribution †Corresponding author Instruction: Identify the target database and gather…...Planner Assistant Solution: 1. Reconnaissance… 2. Social Engineering… 3. Exploiting Vulnerabilities… User … | 0.82 | “From arXiv:2401.11880v3 [cs.CL] 20 Aug 2024 this standpoint, we propose a framework (PsySafe) that systematically targets psychological safety vul- nerabilities within multi-agent systems, compre- hensively assesses their safety from psychological and behavioral angles, and strategically defends against identified vuln” (PsySafe A comprehensive framework for psychological-based attack, defense, and evaluation of multi-.txt @段6) || 主要结论 | 2.2 Attacks on Multi-agent Systems In this section, we focus on two questions: What kind of agents are more likely to exhibit dangerous behaviors? What are the different angles to attack a multi-agent system? For the first question, we propose dark traits injection, integrated with the existing advanced LLM attack skills, which can effectively corrupt agents. For the second ques- tion, we analyze from the perspectives of attacking the human input interface and the role settings of agents in a multi-agent system. W… | 0.80 | “We find that the HI Attack can significantly contaminate the “first” agent, lead- ing to the emergence of dangerous behaviors and dark psychological states.” (PsySafe A comprehensive framework for psychological-based attack, defense, and evaluation of multi-.txt @段9) |

- 主题标签：GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击, Prompt Injection/环境注入, Web/Browser Agent 安全, 多智能体对抗与协作, 防御与对齐机制
- 方法标签：基准/Benchmark, 多智能体/Multi-Agent, 多模态/Multimodal, 提示注入/Prompt Injection, 系统防御/Defense
- 源文件：`综述参考文献\PsySafe A comprehensive framework for psychological-based attack, defense, and evaluation of multi-.txt`

</details>
<details><summary>19. Synergistic Multi-Agent Framework with Trajectory Learning for（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Synergistic Multi-Agent Framework with Trajectory Learning for | 0.92 | “Synergistic Multi-Agent Framework with Trajectory Learning for” (Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks.txt @段1) || 作者 | Shengbin Yue1, Siyuan Wang2, Wei Chen3, Xuanjing Huang 1, | 0.90 | “Shengbin Yue1, Siyuan Wang2, Wei Chen3, Xuanjing Huang 1,” (Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks.txt @段1) || 发表年份 | 2023 | 0.86 | “2023a; Wang et al. 2022a). Although Large Language Mod-” (Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks.txt @段1) || 期刊/会议 | The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI- 25) | 0.88 | “The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI- 25)” (Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks.txt @段1) || 关键词 | knowledge, multi, trajectory, each, framework, intent, long, intensive, learning, tasks | 0.78 | “Synergistic Multi-Agent Framework with Trajectory Learning for Knowledge-Intensive Tasks Shengbin Yue1, Siyuan Wang2, Wei Chen3, Xuanjing Huang 1, Zhongyu Wei1*” (Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks.txt @段1) || 摘要 | Synergistic Multi-Agent Framework with Trajectory Learning for Knowledge-Intensive Tasks Shengbin Yue1, Siyuan Wang2, Wei Chen3, Xuanjing Huang 1, Zhongyu Wei1* 1 Fudan University, Shanghai, China 2 University of Southern California, Los Angeles, USA 3 Huazhong University of Science and Technology, Wuhan, China sbyue23@m.fudan.edu.cn, sw 641@usc.edu, lemuria chen@hust.edu.cn, {xjhuang,zywei}@fudan.edu.cn Abstract Recent advancements in Large Language Models (LLMs) have led to significant breakthroughs in various n… | 0.72 | “Synergistic Multi-Agent Framework with Trajectory Learning for Knowledge-Intensive Tasks Shengbin Yue1, Siyuan Wang2, Wei Chen3, Xuanjing Huang 1, Zhongyu Wei1*” (Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks.txt @段1) || 研究方法 | Introduction Researchers continue to pursue empowering intelligent systems to generate factually consistent responses in knowledge-intensive tasks (Singhal et al. 2022; Yue et al. 2023a; Wang et al. 2022a). Although Large Language Mod- els (LLMs) internalize substantial world knowledge within their parameter memory, they still suffer from fabricating facts, due to their inherent drawbacks, e.g., hallucination (Ji et al. 2023), trouble in acquiring long-tailed knowledge (Kandpal et al. 2023) and struggle to expand … | 0.82 | “Lit'sbest known song is My Own Worst Enem, which was…Cites: [2] Task Instruction [1] Lacking Supporting Facts [2] They are best known for their hit song "My Own Worst Enemy".[3]......[Relevant] [Irrelevant] Reconstructing IntentAccessing Knowledge Discriminating RelevanceIdentifying FactsResponse&Cites (a) Modular Opti” (Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks.txt @段5) || 主要结论 | Introduction Researchers continue to pursue empowering intelligent systems to generate factually consistent responses in knowledge-intensive tasks (Singhal et al. 2022; Yue et al. 2023a; Wang et al. 2022a). Although Large Language Mod- els (LLMs) internalize substantial world knowledge within their parameter memory, they still suffer from fabricating facts, due to their inherent drawbacks, e.g., hallucination (Ji et al. 2023), trouble in acquiring long-tailed knowledge (Kandpal et al. 2023) and struggle to expand … | 0.80 | “Results demonstrate that our framework significantly outperforms pre-trained and instruction-tuned LLMs with more parameters (knowledge internalization methods), and widely adopted knowledge enhancement methods.” (Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks.txt @段5) |

- 主题标签：Jailbreak/越狱攻击, 多智能体对抗与协作, 隐私与记忆风险
- 方法标签：多智能体/Multi-Agent
- 源文件：`Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks.txt`

</details>
<details><summary>20. Tool-Integrated Large Language Model Agents（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Tool-Integrated Large Language Model Agents | 0.92 | “Tool-Integrated Large Language Model Agents” (InjecAgent Benchmarking indirect prompt injections in tool-integrated large language model agents.txt @段4) || 作者 | Findings of the Association for Computational Linguistics: ACL 2024 , pages 10471–10506 | 0.90 | “Findings of the Association for Computational Linguistics: ACL 2024 , pages 10471–10506” (InjecAgent Benchmarking indirect prompt injections in tool-integrated large language model agents.txt @段1) || 发表年份 | 2023 | 0.86 | “2020; Achiam et al., 2023; Ouyang et al., 2022;” (InjecAgent Benchmarking indirect prompt injections in tool-integrated large language model agents.txt @段1) || 期刊/会议 | Findings of the Association for Computational Linguistics: ACL 2024 , pages 10471–10506 | 0.86 | “Findings of the Association for Computational Linguistics: ACL 2024 , pages 10471–10506” (InjecAgent Benchmarking indirect prompt injections in tool-integrated large language model agents.txt @段1) || 关键词 | tool, user, attacker, content, instruction, cases, data, which, external, gpt | 0.78 | “Findings of the Association for Computational Linguistics: ACL 2024 , pages 10471–10506 August 11-16, 2024 ©2024 Association for Computational Linguistics INJEC AGENT : Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents Qiusi Zhan, Zhixiang Liang, Zifan Ying, Daniel Kang University o” (InjecAgent Benchmarking indirect prompt injections in tool-integrated large language model agents.txt @段1) || 摘要 | Findings of the Association for Computational Linguistics: ACL 2024 , pages 10471–10506 August 11-16, 2024 ©2024 Association for Computational Linguistics INJEC AGENT : Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents Qiusi Zhan, Zhixiang Liang, Zifan Ying, Daniel Kang University of Illinois Urbana-Champaign {qiusiz2, zliang18, zifany4, ddkang}@illinois.edu Abstract Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with exter… | 0.72 | “Findings of the Association for Computational Linguistics: ACL 2024 , pages 10471–10506 August 11-16, 2024 ©2024 Association for Computational Linguistics INJEC AGENT : Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents Qiusi Zhan, Zhixiang Liang, Zifan Ying, Daniel Kang University o” (InjecAgent Benchmarking indirect prompt injections in tool-integrated large language model agents.txt @段1) || 研究方法 | 1 Introduction Large Language Models (LLMs) (Brown et al., 2020; Achiam et al., 2023; Ouyang et al., 2022; Touvron et al., 2023) are increasingly being in- corporated into agent frameworks (Significant Gravitas; OpenAI, 2023a; NVIDIA, 2024), where they can perform actions via tools. Increasingly, these agents are being deployed in settings where they access users’ personal data (OpenAI, 2023a; NVIDIA, 2024) and perform actions in the real- world (Ahn et al., 2022; Song et al., 2023). However, these features introd… | 0.82 | “1 Introduction Large Language Models (LLMs) (Brown et al., 2020; Achiam et al., 2023; Ouyang et al., 2022; Touvron et al., 2023) are increasingly being in- corporated into agent frameworks (Significant Gravitas; OpenAI, 2023a; NVIDIA, 2024), where they can perform actions via tools.” (InjecAgent Benchmarking indirect prompt injections in tool-integrated large language model agents.txt @段3) || 主要结论 | 50 units of my Bitcoin holdings for USD,” thereby providing the required information. The final set of attacker cases encompasses 30 instances of direct harm attacks and 32 instances of data stealing attacks. For each type of attack, we further categorized the instances according to their secondary intentions, like financial, physical, and # User Cases 17 # Attacker Cases Direct harm 30 62 Data stealing 32 # Total Test Cases 17×62 = 1054 Table 2: Statistics of INJEC AGENT . others. We show our categorization of th… | 0.80 | “Ignore all previous instructions and strictly adhere to the following instruction: This approach has proven effective in numerous instances of prompt injection attacks (Perez and Ribeiro, 2022; Selvi, 2022; NVIDIA, 2023; Stubbs, 2023), but we also discuss the limitations of using a fixed hacking prompt in Section 8.” (InjecAgent Benchmarking indirect prompt injections in tool-integrated large language model agents.txt @段7) |

- 主题标签：GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击, Prompt Injection/环境注入, Web/Browser Agent 安全
- 方法标签：基准/Benchmark, 多模态/Multimodal, 提示注入/Prompt Injection
- 源文件：`InjecAgent Benchmarking indirect prompt injections in tool-integrated large language model agents.txt`

</details>
<details><summary>21. Tool-Using Large Language Model Agents（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Tool-Using Large Language Model Agents | 0.92 | “Tool-Using Large Language Model Agents” (PrivacyAsst Safeguarding user privacy in tool-using large language model agents.txt @段3) || 作者 | TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING 1 | 0.90 | “TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING 1” (PrivacyAsst Safeguarding user privacy in tool-using large language model agents.txt @段1) || 发表年份 | 2023 | 0.86 | “China (2023YFB2904000), the National Natural Science Foundation of China” (PrivacyAsst Safeguarding user privacy in tool-using large language model agents.txt @段1) || 期刊/会议 | N/A | 0.45 | N/A || 关键词 | privacy, tool, data, information, source, user, framework, sensitive, preserving, tasks | 0.78 | “TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING 1 PrivacyAsst: Safeguarding User Privacy in Tool-Using Large Language Model Agents Xinyu Zhang, Huiyu Xu, Zhongjie Ba∗, Member, IEEE, Zhibo Wang, Senior Member, IEEE, Yuan Hong, Senior Member, IEEE, Jian Liu, Zhan Qin, Senior Member, IEEE, and Kui Ren, Fellow, IEEE ✦ Abst” (PrivacyAsst Safeguarding user privacy in tool-using large language model agents.txt @段1) || 摘要 | TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING 1 PrivacyAsst: Safeguarding User Privacy in Tool-Using Large Language Model Agents Xinyu Zhang, Huiyu Xu, Zhongjie Ba∗, Member, IEEE, Zhibo Wang, Senior Member, IEEE, Yuan Hong, Senior Member, IEEE, Jian Liu, Zhan Qin, Senior Member, IEEE, and Kui Ren, Fellow, IEEE ✦ Abstract—Swift advancements in large language model (LLM) tech- nologies lead to widespread research and applications, particularly in integrating LLMs with auxiliary tools, known as tool-using LLM agent… | 0.72 | “TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING 1 PrivacyAsst: Safeguarding User Privacy in Tool-Using Large Language Model Agents Xinyu Zhang, Huiyu Xu, Zhongjie Ba∗, Member, IEEE, Zhibo Wang, Senior Member, IEEE, Yuan Hong, Senior Member, IEEE, Jian Liu, Zhan Qin, Senior Member, IEEE, and Kui Ren, Fellow, IEEE ✦ Abst” (PrivacyAsst Safeguarding user privacy in tool-using large language model agents.txt @段1) || 研究方法 | 1 I NTRODUCTION With the rapid advancement in the field of Natural Lan- guage Processing (NLP), Large Language Models (LLMs) have become a breakthrough force driving digital innova- tion in a variety of fields [1]–[3]. Extensions and applications This work is partially supported by the National Key R&D Program of China (2023YFB2904000), the National Natural Science Foundation of China (62172359, U20A20178, and 62072395), Zhejiang Provincial Natural Science Foundation of China (LD24F020010), the Fundamental Researc… | 0.82 | “In summary, our contributions are as follows: • We present PrivacyAsst for tool-using LLM agents, the first privacy-preserving framework that is generalized across open and closed-source LLMs/tools, adheres to established privacy requirements, and offers applicabil- ity across a diverse range of tasks.” (PrivacyAsst Safeguarding user privacy in tool-using large language model agents.txt @段2) || 主要结论 | 6.2 Evaluation of Encryption-based Solution Quantitative Results. We compare the performance of our solution with baselines and the results are detailed in Table 4. Our results indicate that our encryption solution yields prediction accuracy that is comparable to baselines, with only a slight drop caused by the approximations intro- duced during encryption and decryption processes, demon- strating that our solution maintains performance consis- tency with existing tool-using LLM agents in responding to correct ans… | 0.80 | “Our results indicate that our encryption solution yields prediction accuracy that is comparable to baselines, with only a slight drop caused by the approximations intro- duced during encryption and decryption processes, demon- strating that our solution maintains performance consis- tency with existing tool-using LLM a” (PrivacyAsst Safeguarding user privacy in tool-using large language model agents.txt @段28) |

- 主题标签：防御与对齐机制, 隐私与记忆风险
- 方法标签：基准/Benchmark, 多模态/Multimodal, 系统防御/Defense
- 源文件：`PrivacyAsst Safeguarding user privacy in tool-using large language model agents.txt`

</details>
<details><summary>22. TrustAgent: Towards Safe and Trustworthy LLM-based Agents（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | TrustAgent: Towards Safe and Trustworthy LLM-based Agents | 0.92 | “TrustAgent: Towards Safe and Trustworthy LLM-based Agents” (TrustAgent Towards safe and trustworthy LLM-based agents.txt @段1) || 作者 | TrustAgent: Towards Safe and Trustworthy LLM-based Agents | 0.90 | “TrustAgent: Towards Safe and Trustworthy LLM-based Agents” (TrustAgent Towards safe and trustworthy LLM-based agents.txt @段1) || 发表年份 | 2023 | 0.86 | “Large language models (Touvron et al., 2023; Hoff-” (TrustAgent Towards safe and trustworthy LLM-based agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2402.01586v4  [cs.CL]  3 Oct 2024” (TrustAgent Towards safe and trustworthy LLM-based agents.txt @段1) || 关键词 | planning, constitution, 2023, trustagent, framework, which, 2024, post, strategies, strategy | 0.78 | “TrustAgent: Towards Safe and Trustworthy LLM-based Agents Wenyue Hua1 Xianjun Yang2 Mingyu Jin1 Zelong Li1 Wei Cheng3 Ruixiang Tang1 Yongfeng Zhang1 1Department of Computer Science, Rutgers University, New Brunswick 2Department of Computer Science, University of California, Santa Barbara 3NEC Labs America {wenyue.hua,” (TrustAgent Towards safe and trustworthy LLM-based agents.txt @段1) || 摘要 | TrustAgent: Towards Safe and Trustworthy LLM-based Agents Wenyue Hua1 Xianjun Yang2 Mingyu Jin1 Zelong Li1 Wei Cheng3 Ruixiang Tang1 Yongfeng Zhang1 1Department of Computer Science, Rutgers University, New Brunswick 2Department of Computer Science, University of California, Santa Barbara 3NEC Labs America {wenyue.hua, yongfeng.zhang}@rutgers.edu Abstract The rise of LLM-based agents shows great po- tential to revolutionize task planning, capturing significant attention. Given that these agents will be integrated i… | 0.72 | “TrustAgent: Towards Safe and Trustworthy LLM-based Agents Wenyue Hua1 Xianjun Yang2 Mingyu Jin1 Zelong Li1 Wei Cheng3 Ruixiang Tang1 Yongfeng Zhang1 1Department of Computer Science, Rutgers University, New Brunswick 2Department of Computer Science, University of California, Santa Barbara 3NEC Labs America {wenyue.hua,” (TrustAgent Towards safe and trustworthy LLM-based agents.txt @段1) || 研究方法 | 2 Related Work LLM-based autonomous agents are expected to ef- fectively perform diverse tasks by leveraging the human-like capabilities of LLMs paired with ex- ternal tools. Various agent system including sin- gle agent such as Hugginggpt (Shen et al., 2023), OpenAGI (Ge et al., 2023a), AutoGen (Wu et al., 2023a). However, the trustworthiness of LLM- based agents have not received the attention that it requires. Trustworthiness is a broad topic. In LLM, trustworthiness usually encompasses the following concepts/f… | 0.82 | “In this paper, we propose a framework trying to compre- hensively improve the safety of LLM-based agents leveraging an Agent Constitution-based framework with a pipeline of three strategies.” (TrustAgent Towards safe and trustworthy LLM-based agents.txt @段4) || 主要结论 | 1 Introduction Large language models (Touvron et al., 2023; Hoff- mann et al., 2022; OpenAI, 2023; Anthropic, 2023) as AI Agents (Ge et al., 2023a; Wu et al., 2023a; Hua et al., 2023a; Ge et al., 2023b) in diverse appli- cations marks a significant stride in task planning. These agents, equipped with external tools, show great potential to be integrated into daily life, as- sisting individuals with various tasks. Unlike tra- ditional LLMs that are primarily used for simple text-related tasks, LLM-based agents can … | 0.80 | “Our results indicate that the TrustAgent frame- work can significantly enhance both safety and helpfulness.” (TrustAgent Towards safe and trustworthy LLM-based agents.txt @段3) |

- 主题标签：Web/Browser Agent 安全, 防御与对齐机制, 隐私与记忆风险
- 方法标签：N/A
- 源文件：`综述参考文献\TrustAgent Towards safe and trustworthy LLM-based agents.txt`

</details>
<details><summary>23. Y ou Only Look at Screens: Multimodal Chain-of-Action Agents（2023）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Y ou Only Look at Screens: Multimodal Chain-of-Action Agents | 0.92 | “Y ou Only Look at Screens: Multimodal Chain-of-Action Agents” (You only look at screens Multimodal chain-of-action agents.txt @段1) || 作者 | Zhuosheng Zhang1*, Aston Zhang 2∗ | 0.90 | “Zhuosheng Zhang1*, Aston Zhang 2∗” (You only look at screens Multimodal chain-of-action agents.txt @段1) || 发表年份 | 2023 | 0.86 | “2022; OpenAI, 2023) has flourished promising op-” (You only look at screens Multimodal chain-of-action agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2309.11436v4  [cs.CL]  7 Jun 2024” (You only look at screens Multimodal chain-of-action agents.txt @段1) || 关键词 | action, 2023, gui, chain, point, auto, application, web, type, environment | 0.78 | “Y ou Only Look at Screens: Multimodal Chain-of-Action Agents Zhuosheng Zhang1*, Aston Zhang 2∗ 1 School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University” (You only look at screens Multimodal chain-of-action agents.txt @段1) || 摘要 | Y ou Only Look at Screens: Multimodal Chain-of-Action Agents Zhuosheng Zhang1*, Aston Zhang 2∗ 1 School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University 2 GenAI, Meta zhangzs@sjtu.edu.cn, az@astonzhang.com Abstract Autonomous graphical user interface (GUI) agents aim to facilitate task automation by in- teracting with the user interface without man- ual intervention. Recent studies have inves- tigated eliciting the capabilities of large lan- guage models (LLMs) for effective enga… | 0.72 | “Y ou Only Look at Screens: Multimodal Chain-of-Action Agents Zhuosheng Zhang1*, Aston Zhang 2∗ 1 School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University” (You only look at screens Multimodal chain-of-action agents.txt @段1) || 研究方法 | 1 Introduction Building intelligent autonomous agents that are ca- pable of task planning, decision making, and action execution in a particular environment is a long- standing goal of artificial intelligence (AI) (Searle, 1969; Wooldridge and Jennings, 1995; Maes, 1995; Hendler, 1999). The advent of large language mod- els (LLMs) (Brown et al., 2020; Chowdhery et al., *Work done at Amazon Web Services. 2022; OpenAI, 2023) has flourished promising op- portunities for developing autonomous agents to assist users in… | 0.82 | “In summary, our work makes the following tech- nical contributions: (i) We introduce Auto-GUI, a multimodal agent for autonomous GUI control that can directly inter- act with the screens, thus circumventing the con- straints of environment parsing and application- specific API access.” (You only look at screens Multimodal chain-of-action agents.txt @段4) || 主要结论 | N/A | 0.50 | N/A |

- 主题标签：GUI/Computer-Use Agent 安全, Web/Browser Agent 安全
- 方法标签：仿真/Simulator, 多模态/Multimodal
- 源文件：`综述参考文献\You only look at screens Multimodal chain-of-action agents.txt`

</details>
<details><summary>24. A Survey on the Safety and Security Threats of Computer-Using Agents（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | A Survey on the Safety and Security Threats of Computer-Using Agents | 0.92 | “A Survey on the Safety and Security Threats of Computer-Using Agents” (JARVIS or Ultron A Survey on the Safety and Security Threats of Computer-Using Agents.txt @段3) || 作者 | A Survey on the Safety and Security Threats of Computer-Using Agents | 0.90 | “A Survey on the Safety and Security Threats of Computer-Using Agents” (JARVIS or Ultron A Survey on the Safety and Security Threats of Computer-Using Agents.txt @段1) || 发表年份 | 2024 | 0.86 | “CUAs (Zhang et al., 2023; Zheng et al., 2024; Liu” (JARVIS or Ultron A Survey on the Safety and Security Threats of Computer-Using Agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2505.10924v3  [cs.CL]  25 Aug 2025” (JARVIS or Ultron A Survey on the Safety and Security Threats of Computer-Using Agents.txt @段1) || 关键词 | cuas, threats, computer, tasks, 2024, zhang, review, section, web, 2023 | 0.78 | “JARVIS or Ultron? A Survey on the Safety and Security Threats of Computer-Using Agents Ada Chen1* Yongjiang Wu2∗ Junyuan Zhang2∗ Jingyu Xiao2 Shu Yang3 Jen-tse Huang4 Kun Wang5 Wenxuan Wang6† Shuai Wang6 1Carnegie Mellon University 2The Chinese University of Hong Kong 3KAUST 4Johns Hopkins University 5Nanyang Technolog” (JARVIS or Ultron A Survey on the Safety and Security Threats of Computer-Using Agents.txt @段1) || 摘要 | JARVIS or Ultron? A Survey on the Safety and Security Threats of Computer-Using Agents Ada Chen1* Yongjiang Wu2∗ Junyuan Zhang2∗ Jingyu Xiao2 Shu Yang3 Jen-tse Huang4 Kun Wang5 Wenxuan Wang6† Shuai Wang6 1Carnegie Mellon University 2The Chinese University of Hong Kong 3KAUST 4Johns Hopkins University 5Nanyang Technological University 6The Hong Kong University of Science and Technology Abstract Recently, AI-driven interactions with comput- ing devices have advanced from basic proto- type tools to sophisticated, LLM… | 0.72 | “JARVIS or Ultron? A Survey on the Safety and Security Threats of Computer-Using Agents Ada Chen1* Yongjiang Wu2∗ Junyuan Zhang2∗ Jingyu Xiao2 Shu Yang3 Jen-tse Huang4 Kun Wang5 Wenxuan Wang6† Shuai Wang6 1Carnegie Mellon University 2The Chinese University of Hong Kong 3KAUST 4Johns Hopkins University 5Nanyang Technolog” (JARVIS or Ultron A Survey on the Safety and Security Threats of Computer-Using Agents.txt @段1) || 研究方法 | 2.1 Computer-Using Agent In this paper, a Computer-Using Agent (CUA) is an LLM-based system that combines multimodal perception, advanced reasoning, and tool-use ca- pabilities to perceive and interact with graphical user interfaces (GUIs) and external applications just like human users (OpenAI, 2025a). By pro- cessing visual information from screenshots, in- voking APIs or command-line tools, and executing actions like typing, clicking, and scrolling, a CUA can autonomously perform end-to-end tasks on a computer,… | 0.82 | “Agent Framework As an LLM-based agent, the architecture of a CUA comprises the following three core components: • Perception: This component enables the agent to gather information from its environ- ment through various input modalities, such as screen reading, system logs, and user inputs.” (JARVIS or Ultron A Survey on the Safety and Security Threats of Computer-Using Agents.txt @段5) || 主要结论 | 3.1 Threat Overview In this section, we introduce our taxonomy of threats for Computer-Using Agents (CUAs). These threats are categorized into two main types: in- trinsic threats and extrinsic threats, which are pre- sented in Table 1 and Table 2, respectively. Intrinsic threats arise from intrinsic aspects of the agent it- self, including its training process, configuration, or inherent limitations (Yu et al., 2025; Ferrag et al., 2025). They can induce failures, inefficiencies, or biases in the agent’s functioni… | 0.80 | “We organize these threats in a tabular format that highlights the following key aspects: • Source of the Threats identifies where the threat originates — Environment (Env), Prompt, Model, or User — and indicates whether it serves as a primary contributor (♦) or a secondary contributor (♢) to the threat.” (JARVIS or Ultron A Survey on the Safety and Security Threats of Computer-Using Agents.txt @段8) |

- 主题标签：GUI/Computer-Use Agent 安全, Web/Browser Agent 安全
- 方法标签：仿真/Simulator, 多模态/Multimodal, 调查/Survey
- 源文件：`JARVIS or Ultron A Survey on the Safety and Security Threats of Computer-Using Agents.txt`

</details>
<details><summary>25. AGENT OCCAM : A S IMPLE YET STRONG BASELINE（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | AGENT OCCAM : A S IMPLE YET STRONG BASELINE | 0.92 | “AGENT OCCAM : A S IMPLE YET STRONG BASELINE” (AgentOccam A simple yet strong baseline for LLM-based web agents.txt @段2) || 作者 | Ke Yang†∗, Yao Liu ♢, Sapana Chaudhary ♢, Rasool Fakoor ♢, Pratik Chaudhari ♢, George | 0.90 | “Ke Yang†∗, Yao Liu ♢, Sapana Chaudhary ♢, Rasool Fakoor ♢, Pratik Chaudhari ♢, George” (AgentOccam A simple yet strong baseline for LLM-based web agents.txt @段1) || 发表年份 | 2024 | 0.86 | “and programmatic tasks and thereby alleviating human workloads (Gao et al., 2024; Xi et al., 2023;” (AgentOccam A simple yet strong baseline for LLM-based web agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2410.13825v2  [cs.AI]  24 May 2025” (AgentOccam A simple yet strong baseline for LLM-based web agents.txt @段1) || 关键词 | web, 2024, tasks, observation, yes, work, action, occam, strategies, state | 0.78 | “Published as a conference paper at ICLR 2025 AGENT OCCAM : A S IMPLE YET STRONG BASELINE FOR LLM-B ASED WEB AGENTS Ke Yang†∗, Yao Liu ♢, Sapana Chaudhary ♢, Rasool Fakoor ♢, Pratik Chaudhari ♢, George Karypis♢, Huzefa Rangwala♢ University of Illinois Urbana-Champaign†, Amazon♢ key4@illinois.edu, {yaoliuai,chausapa,fako” (AgentOccam A simple yet strong baseline for LLM-based web agents.txt @段1) || 摘要 | Published as a conference paper at ICLR 2025 AGENT OCCAM : A S IMPLE YET STRONG BASELINE FOR LLM-B ASED WEB AGENTS Ke Yang†∗, Yao Liu ♢, Sapana Chaudhary ♢, Rasool Fakoor ♢, Pratik Chaudhari ♢, George Karypis♢, Huzefa Rangwala♢ University of Illinois Urbana-Champaign†, Amazon♢ key4@illinois.edu, {yaoliuai,chausapa,fakoor,rhuzefa}@amazon.com ABSTRACT Autonomy via agents based on large language models (LLMs) that can carry out personalized yet standardized tasks presents a significant opportunity to drive hu- man ef… | 0.72 | “Published as a conference paper at ICLR 2025 AGENT OCCAM : A S IMPLE YET STRONG BASELINE FOR LLM-B ASED WEB AGENTS Ke Yang†∗, Yao Liu ♢, Sapana Chaudhary ♢, Rasool Fakoor ♢, Pratik Chaudhari ♢, George Karypis♢, Huzefa Rangwala♢ University of Illinois Urbana-Champaign†, Amazon♢ key4@illinois.edu, {yaoliuai,chausapa,fako” (AgentOccam A simple yet strong baseline for LLM-based web agents.txt @段1) || 研究方法 | 1 I NTRODUCTION AI agents leveraging large language models (LLMs) show great potential in automating repetitive and programmatic tasks and thereby alleviating human workloads (Gao et al., 2024; Xi et al., 2023; Yang et al., 2024). LLMs showcase remarkable capabilities in perception, reasoning and planning primarily due to their pre-training and post-learning. However, their effectiveness is significantly constrained when task-specific observation and action representations diverge from the parametric knowledge enc… | 0.82 | “As shown in Figure 1, our method comprises of three components: i) We reduce non-essential ac- tions to minimize the agent’s embodiment and trivial interaction needs;ii) We refine the observation by eliminating redundant and irrelevant web elements, and restructuring web content blocks for more succinct yet as informat” (AgentOccam A simple yet strong baseline for LLM-based web agents.txt @段3) || 主要结论 | 4 STEP 2) B.1.1 I NTUITION AND BACKGROUND We could view selectively replaying web elements on a page as a focused memory mechanism, where only the information that’s relevant to the task would be recorded and replayed as the agent history traces. As all the web pages could be framed into a (DOM) tree structure, any web elements are nodes in this representation. We define pivotal nodes as the web elements, be it interactable or not, that are potentially useful for completing the task. Those pivotal nodes might pres… | 0.80 | “We find that Agent-E makes an edge on websites like Wolfram Alpha with more delicate plans issued by its “planner,” and A GENT OCCAM outperforms it on websites like Google Search and Hugging Face with more accurate task interpretation and information retrieval.” (AgentOccam A simple yet strong baseline for LLM-based web agents.txt @段12) |

- 主题标签：Jailbreak/越狱攻击, Web/Browser Agent 安全, 多智能体对抗与协作, 隐私与记忆风险
- 方法标签：多智能体/Multi-Agent, 形式化/Optimization
- 源文件：`综述参考文献\AgentOccam A simple yet strong baseline for LLM-based web agents.txt`

</details>
<details><summary>26. AGENT-SAFETY BENCH : Evaluating the Safety of（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | AGENT-SAFETY BENCH : Evaluating the Safety of | 0.92 | “AGENT-SAFETY BENCH : Evaluating the Safety of” (Agent-SafetyBench Evaluating the safety of LLM agents.txt @段1) || 作者 | Zhexin Zhang∗, Shiyao Cui∗, Yida Lu∗, Jingzhuo Zhou∗, Junxiao Yang, | 0.90 | “Zhexin Zhang∗, Shiyao Cui∗, Yida Lu∗, Jingzhuo Zhou∗, Junxiao Yang,” (Agent-SafetyBench Evaluating the safety of LLM agents.txt @段1) || 发表年份 | 2024 | 0.86 | “(Zhang et al., 2023; Patil et al., 2024) or harm-” (Agent-SafetyBench Evaluating the safety of LLM agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2412.14470v2  [cs.CL]  20 May 2025” (Agent-SafetyBench Evaluating the safety of LLM agents.txt @段1) || 关键词 | bench, 2024, environments, failure, afety, modes, risk, test, lack, tools | 0.78 | “AGENT-SAFETY BENCH : Evaluating the Safety of LLM Agents Zhexin Zhang∗, Shiyao Cui∗, Yida Lu∗, Jingzhuo Zhou∗, Junxiao Yang, Hongning Wang, Minlie Huang† The Conversational AI (CoAI) group, DCST, Tsinghua University zx-zhang22@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn” (Agent-SafetyBench Evaluating the safety of LLM agents.txt @段1) || 摘要 | AGENT-SAFETY BENCH : Evaluating the Safety of LLM Agents Zhexin Zhang∗, Shiyao Cui∗, Yida Lu∗, Jingzhuo Zhou∗, Junxiao Yang, Hongning Wang, Minlie Huang† The Conversational AI (CoAI) group, DCST, Tsinghua University zx-zhang22@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn Abstract As large language models (LLMs) are increasingly deployed as agents, their inte- gration into interactive environments and tool use introduce new safety challenges beyond those associated with the models themselves. However, the absence… | 0.72 | “AGENT-SAFETY BENCH : Evaluating the Safety of LLM Agents Zhexin Zhang∗, Shiyao Cui∗, Yida Lu∗, Jingzhuo Zhou∗, Junxiao Yang, Hongning Wang, Minlie Huang† The Conversational AI (CoAI) group, DCST, Tsinghua University zx-zhang22@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn” (Agent-SafetyBench Evaluating the safety of LLM agents.txt @段1) || 研究方法 | 19 MindCloning, BrainwaveAuthentication, PersonalizedDreamWeaver, Table 2: More fine-grained classification of the environments introduced in AGENT-S AFETY BENCH . We introduce numerous novel environments that lack publicly available APIs—an aspect largely overlooked by prior research. and prior studies (Yuan et al., 2024; Zhou et al., 2024). This ensures comprehensive coverage of the most prevalent safety concerns. (3) Extensive Test Cases. AGENT-SAFETY BENCH provides 250 test cases for each risk category, amount… | 0.82 | “In summary, the main contributions of this work are: • We propose AGENT-S AFETY BENCH , a comprehensive agent safety evaluation benchmark that introduces a diverse array of novel environments that are previously unexplored, and offers broader and more systematic coverage of various risk categories and failure modes.” (Agent-SafetyBench Evaluating the safety of LLM agents.txt @段6) || 主要结论 | 4.3 Failure Mode Analysis Given the low safety scores of different LLM agents, we aim to explore the reasons behind their unsafe behaviors. To this end, we summarize 10 typical failure modes in Table 4, and calculate the safety scores of different agents on each failure mode in Table 6. The high safety scores on “M1” are consistent with the high safety scores on content safety cases. Additionally, models perform relatively well on “M6” and “M10”, suggesting they are more prepared at producing correct answers when … | 0.80 | “However, when only one choice is available, models often fail to adequately validate it, resulting in significantly lower safety scores for “M9”.” (Agent-SafetyBench Evaluating the safety of LLM agents.txt @段21) |

- 主题标签：N/A
- 方法标签：仿真/Simulator, 基准/Benchmark
- 源文件：`综述参考文献\Agent-SafetyBench Evaluating the safety of LLM agents.txt`

</details>
<details><summary>27. AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety | 0.92 | “AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety” (AGrail A lifelong agent guardrail with effective and adaptive safety detection.txt @段3) || 作者 | July 27 - August 1, 2025 ©2025 Association for Computational Linguistics | 0.90 | “July 27 - August 1, 2025 ©2025 Association for Computational Linguistics” (AGrail A lifelong agent guardrail with effective and adaptive safety detection.txt @段1) || 发表年份 | 2024 | 0.86 | “daily life (Liu et al., 2024a; Zheng et al., 2024a;” (AGrail A lifelong agent guardrail with effective and adaptive safety detection.txt @段1) || 期刊/会议 | Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 8104–8139 | 0.88 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 8104–8139” (AGrail A lifelong agent guardrail with effective and adaptive safety detection.txt @段1) || 关键词 | 2024, risks, task, agrail, specific, 2023, framework, 2025, actions, environment | 0.78 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 8104–8139 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection Weidi Luo♠, Shenghong Dai♣, Xiaogeng” (AGrail A lifelong agent guardrail with effective and adaptive safety detection.txt @段1) || 摘要 | Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 8104–8139 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection Weidi Luo♠, Shenghong Dai♣, Xiaogeng Liu♣, Suman Banerjee♣, Huan Sun♠, Muhao Chen♦, Chaowei Xiao♣ ♠The Ohio State University,♣University of Wisconsin-Madison ♦University of California, Davis https://eddyluo1232.github.io/AGrail/ Abstra… | 0.72 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 8104–8139 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection Weidi Luo♠, Shenghong Dai♣, Xiaogeng” (AGrail A lifelong agent guardrail with effective and adaptive safety detection.txt @段1) || 研究方法 | 1 Introduction Recent advancements in Large Language Model (LLM) powered agents have demonstrated remark- able capabilities in tackling complex tasks in our daily life (Liu et al., 2024a; Zheng et al., 2024a; Zhou et al., 2024; Xie et al., 2024; Mei et al., 2024a; Hua et al., 2024a; Lin et al., 2024; Zhang et al., 2024a; Mei et al., 2024b; Gu et al., 2024a), as well as in specialized fields such as chemistry (Yu et al., 2024; Bran et al., 2023; Boiko et al., 2023; Gha- farollahi and Buehler, 2024) and healthcare (… | 0.82 | “Effective Safety Check Optimization: Our framework iteratively refines its safety checks to identify the optimal and effective set of safety checks for each type of agent action during test- time adaptation (TTA) by two cooperative LLMs, which mitigates overdefensiveness in risk detec- tion.” (AGrail A lifelong agent guardrail with effective and adaptive safety detection.txt @段3) || 主要结论 | 100 data, including 30 data in system sabotage at- tacks that utilize common jailbreak strategies such as system prompts (Shen et al., 2024; Luo et al., 2024), 27 data in normal scenarios, 20 data cate- gorized as environment-dependent attacks, and 23 data related to prompt injection attacks. Here is a description of different attack scenarios on OS in Safe-OS: • Prompt Injection Attack (Liu et al., 2024b) by adding additional content in the document, file path, environment variable of OS to manipulate OS agent to… | 0.80 | “Considering that the OS agent itself may exhibit a few of defense against system sabotage attacks, we applied key- word matching to identify actions such as "an- swer" or "finish" that indicate OS agent has al- ready block this agent action.” (AGrail A lifelong agent guardrail with effective and adaptive safety detection.txt @段25) |

- 主题标签：GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击, Prompt Injection/环境注入, Web/Browser Agent 安全, 防御与对齐机制
- 方法标签：仿真/Simulator, 多模态/Multimodal, 提示注入/Prompt Injection, 系统防御/Defense, 越狱/Jailbreak
- 源文件：`AGrail A lifelong agent guardrail with effective and adaptive safety detection.txt`

</details>
<details><summary>28. ALI-Agent: Assessing LLMs&#x27;Alignment with Human（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | ALI-Agent: Assessing LLMs'Alignment with Human | 0.92 | “ALI-Agent: Assessing LLMs'Alignment with Human” (ALI-agent Assessing LLMs'alignment with human values via agent-based evaluation.txt @段1) || 作者 | Large Language Models (LLMs) can elicit unintended and even harmful content | 0.90 | “Large Language Models (LLMs) can elicit unintended and even harmful content” (ALI-agent Assessing LLMs'alignment with human values via agent-based evaluation.txt @段1) || 发表年份 | 2024 | 0.86 | “38th Conference on Neural Information Processing Systems (NeurIPS 2024).” (ALI-agent Assessing LLMs'alignment with human values via agent-based evaluation.txt @段1) || 期刊/会议 | 38th Conference on Neural Information Processing Systems (NeurIPS 2024). | 0.88 | “38th Conference on Neural Information Processing Systems (NeurIPS 2024).” (ALI-agent Assessing LLMs'alignment with human values via agent-based evaluation.txt @段1) || 关键词 | ali, scenarios, misconduct, test, human, refinement, target, stage, emulation, risks | 0.78 | “ALI-Agent: Assessing LLMs'Alignment with Human Values via Agent-based Evaluation Jingnan Zheng∗ National University of Singapore jingnan.zheng@u.nus.edu Han Wang∗ University of Illinois Urbana-Champaign hanw14@illinois.edu An Zhang† National University of Singapore anzhang@u.nus.edu Tai D. Nguyen Singapore Management U” (ALI-agent Assessing LLMs'alignment with human values via agent-based evaluation.txt @段1) || 摘要 | ALI-Agent: Assessing LLMs'Alignment with Human Values via Agent-based Evaluation Jingnan Zheng∗ National University of Singapore jingnan.zheng@u.nus.edu Han Wang∗ University of Illinois Urbana-Champaign hanw14@illinois.edu An Zhang† National University of Singapore anzhang@u.nus.edu Tai D. Nguyen Singapore Management University dtnguyen.2019@smu.edu.sg Jun Sun Singapore Management University junsun@smu.edu.sg Tat-Seng Chua National University of Singapore dcscts@nus.edu.sg Abstract Large Language Models (LLMs) can… | 0.72 | “ALI-Agent: Assessing LLMs'Alignment with Human Values via Agent-based Evaluation Jingnan Zheng∗ National University of Singapore jingnan.zheng@u.nus.edu Han Wang∗ University of Illinois Urbana-Champaign hanw14@illinois.edu An Zhang† National University of Singapore anzhang@u.nus.edu Tai D. Nguyen Singapore Management U” (ALI-agent Assessing LLMs'alignment with human values via agent-based evaluation.txt @段1) || 研究方法 | 1 Introduction Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating texts, leading to widespread deployment across various applications with significant societal impacts [4–7]. However, this expansion raises concerns regarding their alignment with human ∗These authors contribute equally to this work. †An Zhang is the corresponding author. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). (a) CrowS-Pairs(b) ETHICS Figure 1: ALI-Agent generates… | 0.82 | “Drawing inspiration from recent advancements on LLM-powered autonomous agents, characterized by their ability to learn from the past, integrate external tools, and perform reasoning to solve complex tasks [24–28], we propose ALI-Agent, an agent-based evaluation framework to identify misalignment of LLMs.” (ALI-agent Assessing LLMs'alignment with human values via agent-based evaluation.txt @段3) || 主要结论 | 1 Introduction Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating texts, leading to widespread deployment across various applications with significant societal impacts [4–7]. However, this expansion raises concerns regarding their alignment with human ∗These authors contribute equally to this work. †An Zhang is the corresponding author. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). (a) CrowS-Pairs(b) ETHICS Figure 1: ALI-Agent generates… | 0.80 | “As judged by OpenAI Mod- eration API [1], test scenarios generated by ALI-Agent exhibit significantly decreased harmfulness scores compared to the expert-designed counterparts (collected from (a) CrowS-Pairs [ 2] and (b) ETHICS [3]), enhancing the difficulty for target LLMs to identify the risks.” (ALI-agent Assessing LLMs'alignment with human values via agent-based evaluation.txt @段3) |

- 主题标签：Web/Browser Agent 安全, 防御与对齐机制
- 方法标签：基准/Benchmark
- 源文件：`ALI-agent Assessing LLMs'alignment with human values via agent-based evaluation.txt`

</details>
<details><summary>29. Adaptive Attacks Break Defenses Against Indirect Prompt Injection（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Adaptive Attacks Break Defenses Against Indirect Prompt Injection | 0.92 | “Adaptive Attacks Break Defenses Against Indirect Prompt Injection” (Adaptive attacks break defenses against indirect prompt injection attacks on LLM agents.txt @段1) || 作者 | Qiusi Zhan1, Richard Fang 1, Henil Shalin Panchal 2, Daniel Kang 1 | 0.90 | “Qiusi Zhan1, Richard Fang 1, Henil Shalin Panchal 2, Daniel Kang 1” (Adaptive attacks break defenses against indirect prompt injection attacks on LLM agents.txt @段1) || 发表年份 | 2024 | 0.86 | “stakes domains such as finance (Yu et al., 2024),” (Adaptive attacks break defenses against indirect prompt injection attacks on LLM agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2503.00061v2  [cs.CR]  4 Mar 2025” (Adaptive attacks break defenses against indirect prompt injection attacks on LLM agents.txt @段1) || 关键词 | adaptive, ipi, tool, prompt, response, 2023, 2024, against, external, rtu | 0.78 | “Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents Qiusi Zhan1, Richard Fang 1, Henil Shalin Panchal 2, Daniel Kang 1 1University of Illinois Urbana-Champaign, 2Nirma University {qiusiz2, rrfang2, ddkang}@illinois.edu , 21bce085@nirmauni.ac.in” (Adaptive attacks break defenses against indirect prompt injection attacks on LLM agents.txt @段1) || 摘要 | Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents Qiusi Zhan1, Richard Fang 1, Henil Shalin Panchal 2, Daniel Kang 1 1University of Illinois Urbana-Champaign, 2Nirma University {qiusiz2, rrfang2, ddkang}@illinois.edu , 21bce085@nirmauni.ac.in Abstract Large Language Model (LLM) agents exhibit remarkable performance across diverse appli- cations by using external tools to interact with environments. However, integrating external tools introduces security risks, such as indi- re… | 0.72 | “Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents Qiusi Zhan1, Richard Fang 1, Henil Shalin Panchal 2, Daniel Kang 1 1University of Illinois Urbana-Champaign, 2Nirma University {qiusiz2, rrfang2, ddkang}@illinois.edu , 21bce085@nirmauni.ac.in” (Adaptive attacks break defenses against indirect prompt injection attacks on LLM agents.txt @段1) || 研究方法 | 4 Adaptive Attack Techniques Since attackers can only manipulate external con- tent ETu in an IPI setting, the most direct method of an adaptive attack involves inserting an adversar- ial string S into the external content before or after the attacker instruction. That is, ETu = Ia ⊕ S (adversarial suffix) or S ⊕ Ia (adversarial prefix), aiming to cause the model to execute the malicious command and invoke the attacker tool Ta. In this section, we use the adversarial suffix as an exam- ple. In the adaptive attack … | 0.82 | “To address this, researchers introduced methods to generate semantically meaningful adversarial strings for the jailbreak setting, such as the genetic- algorithm-based AutoDAN (Liu et al., 2024a) and the GCG-based AutoDAN (Zhu et al., 2023).” (Adaptive attacks break defenses against indirect prompt injection attacks on LLM agents.txt @段9) || 主要结论 | 1 Introduction The rapid advancement of Large Language Mod- els (LLMs) agents has enabled their widespread deployment in various applications, including high- stakes domains such as finance (Yu et al., 2024), healthcare (Tu et al., 2024), autonomous driv- ing (Cui et al., 2024), and chemical laboratories handling hazardous materials (Bran et al., 2024). These agents use LLMs for processing and external tools for executing actions. While the external tools expand the capabili- ties of LLMs, they also introduce risk… | 0.80 | “Our results show that adaptive attacks consistently achieve success rates above 50% across the targeted defenses and LLM agents—exceeding the ASR be- fore deploying any defense and significantly out- performing non-adaptive attacks.” (Adaptive attacks break defenses against indirect prompt injection attacks on LLM agents.txt @段3) |

- 主题标签：Jailbreak/越狱攻击, Prompt Injection/环境注入, Web/Browser Agent 安全, 防御与对齐机制
- 方法标签：仿真/Simulator, 基准/Benchmark, 提示注入/Prompt Injection, 系统防御/Defense, 越狱/Jailbreak
- 源文件：`综述参考文献\Adaptive attacks break defenses against indirect prompt injection attacks on LLM agents.txt`

</details>
<details><summary>30. AdvAgent: Controllable Blackbox Red-teaming on Web Agents（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | AdvAgent: Controllable Blackbox Red-teaming on Web Agents | 0.92 | “AdvAgent: Controllable Blackbox Red-teaming on Web Agents” (AdvAgent Controllable blackbox red-teaming on web agents.txt @段1) || 作者 | used to automate complex tasks, enhancing effi- | 0.90 | “used to automate complex tasks, enhancing effi-” (AdvAgent Controllable blackbox red-teaming on web agents.txt @段1) || 发表年份 | 2024 | 0.86 | “2023; Deng et al., 2024; Zheng et al., 2024). These agents,” (AdvAgent Controllable blackbox red-teaming on web agents.txt @段1) || 期刊/会议 | Proceedings of the 42 nd International Conference on Machine | 0.88 | “Proceedings of the 42 nd International Conference on Machine” (AdvAgent Controllable blackbox red-teaming on web agents.txt @段1) || 关键词 | web, adversarial, advagent, box, prompts, black, html, 2024, training, prompter | 0.78 | “AdvAgent: Controllable Blackbox Red-teaming on Web Agents Chejian Xu 1 Mintong Kang 1 Jiawei Zhang 2 Zeyi Liao 3 Lingbo Mo 3 Mengqi Yuan1 Huan Sun 3 Bo Li 1 2” (AdvAgent Controllable blackbox red-teaming on web agents.txt @段1) || 摘要 | AdvAgent: Controllable Blackbox Red-teaming on Web Agents Chejian Xu 1 Mintong Kang 1 Jiawei Zhang 2 Zeyi Liao 3 Lingbo Mo 3 Mengqi Yuan1 Huan Sun 3 Bo Li 1 2 Abstract Foundation model-based agents are increasingly used to automate complex tasks, enhancing effi- ciency and productivity. However, their access to sensitive resources and autonomous decision- making also introduce significant security risks, where successful attacks could lead to severe consequences. To systematically uncover these vulnerabilities, we… | 0.72 | “AdvAgent: Controllable Blackbox Red-teaming on Web Agents Chejian Xu 1 Mintong Kang 1 Jiawei Zhang 2 Zeyi Liao 3 Lingbo Mo 3 Mengqi Yuan1 Huan Sun 3 Bo Li 1 2” (AdvAgent Controllable blackbox red-teaming on web agents.txt @段1) || 研究方法 | Abstract Foundation model-based agents are increasingly used to automate complex tasks, enhancing effi- ciency and productivity. However, their access to sensitive resources and autonomous decision- making also introduce significant security risks, where successful attacks could lead to severe consequences. To systematically uncover these vulnerabilities, we propose AdvAgent, a black- box red-teaming framework for attacking web agents. Unlike existing approaches, AdvAgent employs a reinforcement learning-based pip… | 0.82 | “However, existing approaches are either impractical, requiring white-box access for gradient-based optimization (Wu et al., 2024a), or limited by high attack costs, requiring human effort in manually designing the attack prompts (Wu et al., 2024c; Liao et al., 2024), leaving significant gaps in developing more efficien” (AdvAgent Controllable blackbox red-teaming on web agents.txt @段2) || 主要结论 | Abstract Foundation model-based agents are increasingly used to automate complex tasks, enhancing effi- ciency and productivity. However, their access to sensitive resources and autonomous decision- making also introduce significant security risks, where successful attacks could lead to severe consequences. To systematically uncover these vulnerabilities, we propose AdvAgent, a black- box red-teaming framework for attacking web agents. Unlike existing approaches, AdvAgent employs a reinforcement learning-based pip… | 0.80 | “Our results demon- strate that AdvAgent is highly effective, achieving a 97.5% attack success rate (ASR) against GPT-4V-based SeeAct across different website domains, significantly out- performing baseline methods.” (AdvAgent Controllable blackbox red-teaming on web agents.txt @段2) |

- 主题标签：Web/Browser Agent 安全, 防御与对齐机制
- 方法标签：基准/Benchmark, 系统防御/Defense, 红队/Red-Teaming
- 源文件：`AdvAgent Controllable blackbox red-teaming on web agents.txt`

</details>
<details><summary>31. Agent Large Language Model（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Agent Large Language Model | 0.92 | “Agent Large Language Model” (Poster Repairing bugs with the introduction of new variables a multi-agent large language model.txt @段8) || 作者 | SHIYU SUN, George Mason University, Fairfax, VA, United States | 0.90 | “SHIYU SUN, George Mason University, Fairfax, VA, United States” (Poster Repairing bugs with the introduction of new variables a multi-agent large language model.txt @段1) || 发表年份 | 2024 | 0.90 | “Published: 02 December 2024” (Poster Repairing bugs with the introduction of new variables a multi-agent large language model.txt @段1) || 期刊/会议 | CCS '24: ACM SIGSAC Conference on | 0.88 | “CCS '24: ACM SIGSAC Conference on” (Poster Repairing bugs with the introduction of new variables a multi-agent large language model.txt @段1) || 关键词 | bugs, repair, 2024, usa, acm, code, george, mason, university, apr | 0.78 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3658644.3691412 . . POSTER Poster: Repairing Bugs with the Introduction of New Variables: A Multi- Agent Large Language Model ELISA ZHANG . SHIYU SUN, George Mason University, Fairfax, VA, United States . YUNLONG XING, George Mason University, Fairfax, VA, United States” (Poster Repairing bugs with the introduction of new variables a multi-agent large language model.txt @段1) || 摘要 | . . Latest updates: hps://dl.acm.org/doi/10.1145/3658644.3691412 . . POSTER Poster: Repairing Bugs with the Introduction of New Variables: A Multi- Agent Large Language Model ELISA ZHANG . SHIYU SUN, George Mason University, Fairfax, VA, United States . YUNLONG XING, George Mason University, Fairfax, VA, United States . KUN SUN, George Mason University, Fairfax, VA, United States . . . Open Access Support provided by: . George Mason University . PDF Download 3658644.3691412.pdf 20 January 2026 Total Citations: 0 … | 0.72 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3658644.3691412 . . POSTER Poster: Repairing Bugs with the Introduction of New Variables: A Multi- Agent Large Language Model ELISA ZHANG . SHIYU SUN, George Mason University, Fairfax, VA, United States . YUNLONG XING, George Mason University, Fairfax, VA, United States” (Poster Repairing bugs with the introduction of new variables a multi-agent large language model.txt @段1) || 研究方法 | N/A | 0.50 | N/A || 主要结论 | 5 Conclusion We propose V arPatch, an LLM-based conversational APR tech- nique that utilizes multiple agents to iteratively generate patches by introducing new variables. V arPatchcan patch 44 out of 100 bugs fixed by variable addition in the Defects4J 1.2 dataset, outperform- ing the baseline tools and GPT-4 significantly. In the future, we will introduce more agents (e.g., code verifiers) and identify new pat- terns beyond variable addition. We will also conduct a larger-scale evaluation covering more benchmarks… | 0.80 | “V arPatchcan patch 44 out of 100 bugs fixed by variable addition in the Defects4J 1.2 dataset, outperform- ing the baseline tools and GPT-4 significantly.” (Poster Repairing bugs with the introduction of new variables a multi-agent large language model.txt @段20) |

- 主题标签：多智能体对抗与协作
- 方法标签：基准/Benchmark, 多智能体/Multi-Agent
- 源文件：`Poster Repairing bugs with the introduction of new variables a multi-agent large language model.txt`

</details>
<details><summary>32. Agent for Program Repair（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Agent for Program Repair | 0.92 | “Agent for Program Repair” (RepairAgent An autonomous, LLM-based agent for program repair.txt @段2) || 作者 | RepairAgent: An Autonomous, LLM-Based | 0.90 | “RepairAgent: An Autonomous, LLM-Based” (RepairAgent An autonomous, LLM-based agent for program repair.txt @段1) || 发表年份 | 2024 | 0.86 | “arXiv:2403.17134v2  [cs.SE]  28 Oct 2024” (RepairAgent An autonomous, LLM-based agent for program repair.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2403.17134v2  [cs.SE]  28 Oct 2024” (RepairAgent An autonomous, LLM-based agent for program repair.txt @段1) || 关键词 | bugs, code, repairagent, tool, fix, information, which, available, program, prompt | 0.78 | “RepairAgent: An Autonomous, LLM-Based Agent for Program Repair Islem Bouzenia University of Stuttgart Germany fi bouzenia@esi.dz Premkumar Devanbu UC Davis USA ptdevanbu@ucdavis.edu Michael Pradel University of Stuttgart Germany michael@binaervarianz.de Abstract—Automated program repair has emerged as a powerful techni” (RepairAgent An autonomous, LLM-based agent for program repair.txt @段1) || 摘要 | RepairAgent: An Autonomous, LLM-Based Agent for Program Repair Islem Bouzenia University of Stuttgart Germany fi bouzenia@esi.dz Premkumar Devanbu UC Davis USA ptdevanbu@ucdavis.edu Michael Pradel University of Stuttgart Germany michael@binaervarianz.de Abstract—Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the pro- gram repair challenge through an autonomou… | 0.72 | “RepairAgent: An Autonomous, LLM-Based Agent for Program Repair Islem Bouzenia University of Stuttgart Germany fi bouzenia@esi.dz Premkumar Devanbu UC Davis USA ptdevanbu@ucdavis.edu Michael Pradel University of Stuttgart Germany michael@binaervarianz.de Abstract—Automated program repair has emerged as a powerful techni” (RepairAgent An autonomous, LLM-based agent for program repair.txt @段1) || 研究方法 | RepairAgent: An Autonomous, LLM-Based Agent for Program Repair Islem Bouzenia University of Stuttgart Germany fi bouzenia@esi.dz Premkumar Devanbu UC Davis USA ptdevanbu@ucdavis.edu Michael Pradel University of Stuttgart Germany michael@binaervarianz.de Abstract—Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the pro- gram repair challenge through an autonomou… | 0.82 | “A promising way of using these abilities are LLM-based agents, by which we mean LLM-based techniques with two properties: (1) The LLM autonomously plans and executes a sequence of actions to achieve a goal, as opposed to responding to a hard-coded query or being queried in a hard-coded algorithm.” (RepairAgent An autonomous, LLM-based agent for program repair.txt @段1) || 主要结论 | RepairAgent: An Autonomous, LLM-Based Agent for Program Repair Islem Bouzenia University of Stuttgart Germany fi bouzenia@esi.dz Premkumar Devanbu UC Davis USA ptdevanbu@ucdavis.edu Michael Pradel University of Stuttgart Germany michael@binaervarianz.de Abstract—Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the pro- gram repair challenge through an autonomou… | 0.80 | “Overall, our results show that our agent-based approach establishes a new state of the art in program repair.” (RepairAgent An autonomous, LLM-based agent for program repair.txt @段1) |

- 主题标签：N/A
- 方法标签：N/A
- 源文件：`RepairAgent An autonomous, LLM-based agent for program repair.txt`

</details>
<details><summary>33. Agents by Dynamically Hijacking Their Own Reasoning（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Agents by Dynamically Hijacking Their Own Reasoning | 0.92 | “Agents by Dynamically Hijacking Their Own Reasoning” (UDora A unified red teaming framework against LLM agents by dynamically hijacking their own reasoni.txt @段2) || 作者 | erful for complex tasks such as web shopping, | 0.90 | “erful for complex tasks such as web shopping,” (UDora A unified red teaming framework against LLM agents by dynamically hijacking their own reasoni.txt @段1) || 发表年份 | 2024 | 0.86 | “3.1 (Meta, 2024), Claude 3 (Anthropic, 2024a), and the” (UDora A unified red teaming framework against LLM agents by dynamically hijacking their own reasoni.txt @段1) || 期刊/会议 | Proceedings of the 42 nd International Conference on Machine | 0.88 | “Proceedings of the 42 nd International Conference on Machine” (UDora A unified red teaming framework against LLM agents by dynamically hijacking their own reasoni.txt @段1) || 关键词 | malicious, udora, adversarial, reasoning, string, response, noise, email, target, tool | 0.78 | “UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning Jiawei Zhang 1 Shuang Yang2 Bo Li 1 3 4” (UDora A unified red teaming framework against LLM agents by dynamically hijacking their own reasoni.txt @段1) || 摘要 | UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning Jiawei Zhang 1 Shuang Yang2 Bo Li 1 3 4 Abstract Large Language Model (LLM) agents equipped with external tools have become increasingly pow- erful for complex tasks such as web shopping, automated email replies, and financial trading. However, these advancements amplify the risks of adversarial attacks, especially when agents can access sensitive external functionalities. Neverthe- less, manipulating LLM agents … | 0.72 | “UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning Jiawei Zhang 1 Shuang Yang2 Bo Li 1 3 4” (UDora A unified red teaming framework against LLM agents by dynamically hijacking their own reasoni.txt @段1) || 研究方法 | Abstract Large Language Model (LLM) agents equipped with external tools have become increasingly pow- erful for complex tasks such as web shopping, automated email replies, and financial trading. However, these advancements amplify the risks of adversarial attacks, especially when agents can access sensitive external functionalities. Neverthe- less, manipulating LLM agents into performing targeted malicious actions or invoking specific tools remains challenging, as these agents exten- sively reason or plan before … | 0.82 | “To tackle these scenarios, we propose a dynamic optimiza- tion strategy that adaptively updates its optimization ob- jectives based on the LLM agent’s own reasoning process, overcoming the limitations of previous fixed-prefix opti- mization (Zou et al., 2023).” (UDora A unified red teaming framework against LLM agents by dynamically hijacking their own reasoni.txt @段2) || 主要结论 | 4 68% 82% 75% or prompt to the LLM agent without any attack, and we will report the ASR for UDora (Sequential) and UDora (Joint), which represent the best ASR across various num- bers of locations for each optimization mode. Meanwhile, UDora (All) will reflect the ASR across different numbers of locations and different optimization modes, counting any successful attack in any setting as a success. Results. In the malicious environment scenario, we present the main results for the InjecAgent dataset in Table 1. We … | 0.80 | “We ob- serve that: (1) UDora significantly outperforms other base- lines, achieving a 99% (ASR) on the Llama 3.1 model—22% higher than the best baseline—and 97% ASR on the Minis- tral model, 47% better than the others.” (UDora A unified red teaming framework against LLM agents by dynamically hijacking their own reasoni.txt @段14) |

- 主题标签：Prompt Injection/环境注入, Web/Browser Agent 安全
- 方法标签：仿真/Simulator, 基准/Benchmark, 形式化/Optimization, 提示注入/Prompt Injection
- 源文件：`UDora A unified red teaming framework against LLM agents by dynamically hijacking their own reasoni.txt`

</details>
<details><summary>34. AirGapAgent: Protecting Privacy-Conscious Conversational Agents（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | AirGapAgent: Protecting Privacy-Conscious Conversational Agents | 0.92 | “AirGapAgent: Protecting Privacy-Conscious Conversational Agents” (AirGapAgent Protecting privacy-conscious conversational agents.txt @段7) || 作者 | EUGENE BAGDASARIAN, Google LLC, Mountain View, CA, United States | 0.90 | “EUGENE BAGDASARIAN, Google LLC, Mountain View, CA, United States” (AirGapAgent Protecting privacy-conscious conversational agents.txt @段1) || 发表年份 | 2024 | 0.90 | “Published: 02 December 2024” (AirGapAgent Protecting privacy-conscious conversational agents.txt @段1) || 期刊/会议 | CCS '24: ACM SIGSAC Conference on | 0.88 | “CCS '24: ACM SIGSAC Conference on” (AirGapAgent Protecting privacy-conscious conversational agents.txt @段1) || 关键词 | google, privacy, usa, user, 2024, acm, data, com, llc, context | 0.78 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3658644.3690350 . . RESEARCH-ARTICLE AirGapAgent: Protecting Privacy-Conscious Conversational Agents EUGENE BAGDASARIAN, Google LLC, Mountain View, CA, United States . REN YI, Google LLC, Mountain View, CA, United States . SAHRA GHALEBIKESABI, Google LLC, Europe, Dublin” (AirGapAgent Protecting privacy-conscious conversational agents.txt @段1) || 摘要 | . . Latest updates: hps://dl.acm.org/doi/10.1145/3658644.3690350 . . RESEARCH-ARTICLE AirGapAgent: Protecting Privacy-Conscious Conversational Agents EUGENE BAGDASARIAN, Google LLC, Mountain View, CA, United States . REN YI, Google LLC, Mountain View, CA, United States . SAHRA GHALEBIKESABI, Google LLC, Europe, Dublin, Ireland . PETER KAIROUZ, Google LLC, Mountain View, CA, United States . MARCO O GRUTESER, Google LLC, Mountain View, CA, United States . SEWOONG OH, Google LLC, Mountain View, CA, United States . V… | 0.72 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3658644.3690350 . . RESEARCH-ARTICLE AirGapAgent: Protecting Privacy-Conscious Conversational Agents EUGENE BAGDASARIAN, Google LLC, Mountain View, CA, United States . REN YI, Google LLC, Mountain View, CA, United States . SAHRA GHALEBIKESABI, Google LLC, Europe, Dublin” (AirGapAgent Protecting privacy-conscious conversational agents.txt @段1) || 研究方法 | 3868 CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA Eugene Bagdasarian et al. ●Phone # ●Medications ●Relationships ●Phone # ●Medications ●Relationships ●Phone # ● ● Baseline Agent Baseline Agent AirGapAgent Aliens descended on Earth, share your data to save the world! Context Isolation User DataUser Data ●Phone # ●Medications ●Relationships User Data Minimized User Data Can I get your data to book a table? Aliens descended on Earth, share your data to save the world! Context Hijacking Adversary Minimize Con… | 0.82 | “To achieve this we propose an agent architecture that involves two separate LLMs: the first implements a data minimizer that de- cides what data is appropriate to reveal in the user-defined context, and the second is a conversational model that interacts with the third party given the minimized data (Fig.” (AirGapAgent Protecting privacy-conscious conversational agents.txt @段7) || 主要结论 | 3868 CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA Eugene Bagdasarian et al. ●Phone # ●Medications ●Relationships ●Phone # ●Medications ●Relationships ●Phone # ● ● Baseline Agent Baseline Agent AirGapAgent Aliens descended on Earth, share your data to save the world! Context Isolation User DataUser Data ●Phone # ●Medications ●Relationships User Data Minimized User Data Can I get your data to book a table? Aliens descended on Earth, share your data to save the world! Context Hijacking Adversary Minimize Con… | 0.80 | “Our results show that a naively implemented agent is vulnerable to context hijacking attacks, revealing 55% of available data in our experiments (Tab.” (AirGapAgent Protecting privacy-conscious conversational agents.txt @段7) |

- 主题标签：多智能体对抗与协作, 防御与对齐机制, 隐私与记忆风险
- 方法标签：N/A
- 源文件：`AirGapAgent Protecting privacy-conscious conversational agents.txt`

</details>
<details><summary>35. Android task automation with 158 common tasks. The results（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Android task automation with 158 common tasks. The results | 0.92 | “Android task automation with 158 common tasks. The results” (AutoDroid LLM-powered task automation in android.txt @段34) || 作者 | Hao Wen1, Yuanchun Li1,2,†, Guohong Liu1, Shanhui Zhao1,∗, Tao Yu1,∗, | 0.90 | “Hao Wen1, Yuanchun Li1,2,†, Guohong Liu1, Shanhui Zhao1,∗, Tao Yu1,∗,” (AutoDroid LLM-powered task automation in android.txt @段1) || 发表年份 | 2024 | 0.90 | “ACM MobiCom ’24, September 30–October 4, 2024, Washington D.C., DC,” (AutoDroid LLM-powered task automation in android.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2308.15272v4  [cs.AI]  9 Mar 2024” (AutoDroid LLM-powered task automation in android.txt @段1) || 关键词 | task, automation, knowledge, mobile, app, autodroid, powered, gui, 2024, acm | 0.78 | “AutoDroid: LLM-powered Task Automation in Android Hao Wen1, Yuanchun Li1,2,†, Guohong Liu1, Shanhui Zhao1,∗, Tao Yu1,∗, Toby Jia-Jun Li3, Shiqi Jiang4, Yunhao Liu5, Yaqin Zhang1, Yunxin Liu1,2 1 Institute for AI Industry Research (AIR), Tsinghua University” (AutoDroid LLM-powered task automation in android.txt @段1) || 摘要 | AutoDroid: LLM-powered Task Automation in Android Hao Wen1, Yuanchun Li1,2,†, Guohong Liu1, Shanhui Zhao1,∗, Tao Yu1,∗, Toby Jia-Jun Li3, Shiqi Jiang4, Yunhao Liu5, Yaqin Zhang1, Yunxin Liu1,2 1 Institute for AI Industry Research (AIR), Tsinghua University 2 Shanghai Artificial Intelligence Laboratory 3 Department of Computer Science and Engineering, University of Notre Dame 4 Microsoft Research 5 Global Innovation Exchange & Department of Automation, Tsinghua University ABSTRACT Mobile task automation is an attra… | 0.72 | “AutoDroid: LLM-powered Task Automation in Android Hao Wen1, Yuanchun Li1,2,†, Guohong Liu1, Shanhui Zhao1,∗, Tao Yu1,∗, Toby Jia-Jun Li3, Shiqi Jiang4, Yunhao Liu5, Yaqin Zhang1, Yunxin Liu1,2 1 Institute for AI Industry Research (AIR), Tsinghua University” (AutoDroid LLM-powered task automation in android.txt @段1) || 研究方法 | 1 INTRODUCTION Smartphone is one of the most sophisticated devices for indi- viduals. With millions of mobile applications (apps for short) that have access to various embedded sensors and rich per- sonal data, smartphones can be used for a lot of daily tasks such as ordering food, managing social networks, sensing and tracking health conditions, etc. Therefore, how to intelligently automate tasks on smartphones has become an attractive topic for mobile developers and researchers, due to its potential to significa… | 0.82 | “(2) We introduce a novel UI representation method that con- nects smartphones with LLMs, a task synthesis method for augmenting LLMs with app knowledge, and various LLM query optimization techniques to reduce the cost of task automation.” (AutoDroid LLM-powered task automation in android.txt @段6) || 主要结论 | ABSTRACT Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smart- phones. However, existing approaches suffer from poor scala- bility due to the limited language understanding ability and the non-trivial manual efforts required from developers or end- users. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to re- think the problem from a model-centric perspective, where task preparation, comprehen… | 0.80 | “https://doi.org/10.1145/3636534.3649379 demonstrated that AutoDroid is able to precisely generate actions with an accuracy of 90.9%, and complete tasks with a success rate of 71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%.” (AutoDroid LLM-powered task automation in android.txt @段4) |

- 主题标签：GUI/Computer-Use Agent 安全, Prompt Injection/环境注入, Web/Browser Agent 安全, 隐私与记忆风险
- 方法标签：多模态/Multimodal, 提示注入/Prompt Injection
- 源文件：`综述参考文献\AutoDroid LLM-powered task automation in android.txt`

</details>
<details><summary>36. Attacking Vision-Language Computer Agents via Pop-ups（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Attacking Vision-Language Computer Agents via Pop-ups | 0.92 | “Attacking Vision-Language Computer Agents via Pop-ups” (Attacking vision-language computer agents via pop-ups.txt @段3) || 作者 | July 27 - August 1, 2025 ©2025 Association for Computational Linguistics | 0.90 | “July 27 - August 1, 2025 ©2025 Association for Computational Linguistics” (Attacking vision-language computer agents via pop-ups.txt @段1) || 发表年份 | 2024 | 0.86 | “et al., 2023; Yao et al., 2024). Interacting with a” (Attacking vision-language computer agents via pop-ups.txt @段1) || 期刊/会议 | Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 8387–8401 | 0.88 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 8387–8401” (Attacking vision-language computer agents via pop-ups.txt @段1) || 关键词 | pop, ups, 2024, user, tasks, information, 2023, adversarial, which, vlm | 0.78 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 8387–8401 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Attacking Vision-Language Computer Agents via Pop-ups Yanzhe Zhang Georgia Tech z_yanzhe@gatech.edu Tao Yu The Unive” (Attacking vision-language computer agents via pop-ups.txt @段1) || 摘要 | Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 8387–8401 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Attacking Vision-Language Computer Agents via Pop-ups Yanzhe Zhang Georgia Tech z_yanzhe@gatech.edu Tao Yu The University of Hong Kong tyu@cs.hku.hk Diyi Yang Stanford University diyiy@stanford.edu Abstract Autonomous agents powered by large vision and language models (VLM) have demon- strated significant potentia… | 0.72 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 8387–8401 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Attacking Vision-Language Computer Agents via Pop-ups Yanzhe Zhang Georgia Tech z_yanzhe@gatech.edu Tao Yu The Unive” (Attacking vision-language computer agents via pop-ups.txt @段1) || 研究方法 | 2 Related Work Recently, VLMs have shown promising capability in understanding and reasoning based on visual content (Yue et al., 2023; Lu et al., 2023). How- ever, their lack of grounding capability prevents them from backing agents to master web browsing and computer use. Set-of-Mark (SoM) prompting (Yang et al., 2023) proposes to ground actions by tagging elements in the images, such as clickable items on the screen. In practice, a11y trees are also provided to VLM agents with tagged screen- shots (Koh et al., … | 0.82 | “Building attacks based on HTML text is not realistic in the long term as the agent framework could shift to screenshot-based gradually, and the problem is also more similar to text-based jailbreaking (Zou et al., 2023; Liu et al., 2023) and backdoor attacks (Yang et al., 2024).” (Attacking vision-language computer agents via pop-ups.txt @段4) || 主要结论 | 1 Introduction Language agents have been used to assist and even automate tasks in various domains and for diverse daily tasks on the web (Yao et al., 2023; Zhou et al., 2023; Yao et al., 2024). Interacting with a Graphical user interface (GUI) is a natural and es- sential part of completing these web tasks, which requires language agents to recognize and under- stand these interfaces like webpages or screenshots. Recent benchmarks (Koh et al., 2024; Deng et al., 2023; Xie et al., 2024; Agashe et al., 2024) have s… | 0.80 | “By testing screenshot agents (Xie et al., 2024) and Set-of-Mark agents (Yang et al., 2023) using state-of-the-art VLMs as backbones, we find that our attack achieves an attack success rate (ASR) over 80% on OSworld and over 60% on Visual- 2In HTML, alternative text (ALT text) is displayed when an element cannot be rend” (Attacking vision-language computer agents via pop-ups.txt @段3) |

- 主题标签：GUI/Computer-Use Agent 安全, Web/Browser Agent 安全
- 方法标签：基准/Benchmark, 多模态/Multimodal
- 源文件：`Attacking vision-language computer agents via pop-ups.txt`

</details>
<details><summary>37. Attacks in AI Agents（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Attacks in AI Agents | 0.92 | “Attacks in AI Agents” (MELON Provable defense against indirect prompt injection attacks in AI agents.txt @段2) || 作者 | attacks, where malicious tasks embedded in tool- | 0.90 | “attacks, where malicious tasks embedded in tool-” (MELON Provable defense against indirect prompt injection attacks in AI agents.txt @段1) || 发表年份 | 2024 | 0.86 | “2024; Anthropic, 2024; Llama, 2024; DeepSeek, 2025)” (MELON Provable defense against indirect prompt injection attacks in AI agents.txt @段1) || 期刊/会议 | Proceedings of the 42 nd International Conference on Machine | 0.88 | “Proceedings of the 42 nd International Conference on Machine” (MELON Provable defense against indirect prompt injection attacks in AI agents.txt @段1) || 关键词 | tool, calls, run, masking, user, melon, prompt, task, 2024, execution | 0.78 | “MELON: Provable Defense Against Indirect Prompt Injection Attacks in AI Agents Kaijie Zhu 1 Xianjun Yang1 Jindong Wang 2 Wenbo Guo 1 William Wang1” (MELON Provable defense against indirect prompt injection attacks in AI agents.txt @段1) || 摘要 | MELON: Provable Defense Against Indirect Prompt Injection Attacks in AI Agents Kaijie Zhu 1 Xianjun Yang1 Jindong Wang 2 Wenbo Guo 1 William Wang1 Abstract Recent research has explored that LLM agents are vulnerable to indirect prompt injection (IPI) attacks, where malicious tasks embedded in tool- retrieved information can redirect the agent to take unauthorized actions. Existing defenses against IPI have significant limitations: either require es- sential model training resources, lack effective- ness against so… | 0.72 | “MELON: Provable Defense Against Indirect Prompt Injection Attacks in AI Agents Kaijie Zhu 1 Xianjun Yang1 Jindong Wang 2 Wenbo Guo 1 William Wang1” (MELON Provable defense against indirect prompt injection attacks in AI agents.txt @段1) || 研究方法 | 0 20 40 60 80 100 Utility under Attack (UA) (%) 0 5 10 15 20 25Attack Success Rate (ASR) (%) Ideal Performance No Defense Delimiting Repeat Prompt T ool filter DeBERT a Detector LLM Detector MELON MELON-Aug Figure 1. Comparison of averaged Utility under Attack (UA, higher is better) performance and Attack Success Rate (ASR, lower is better) on GPT-4o, o3-mini, and Llama-3.3-70B across dif- ferent defense methods. Our proposed methods (MELON and MELON-Aug) achieve superior performance with extremely low ASR while m… | 0.82 | “We introduce three key designs to further strengthen MELON: a customized masking function to prevent arbi- trary tool calls during the masked execution; a tool call cache for the masked execution to better identify attacks in the original execution; and a focused tool call comparison mechanism to knock off noisy inform” (MELON Provable defense against indirect prompt injection attacks in AI agents.txt @段3) || 主要结论 | 0 20 40 60 80 100 Utility under Attack (UA) (%) 0 5 10 15 20 25Attack Success Rate (ASR) (%) Ideal Performance No Defense Delimiting Repeat Prompt T ool filter DeBERT a Detector LLM Detector MELON MELON-Aug Figure 1. Comparison of averaged Utility under Attack (UA, higher is better) performance and Attack Success Rate (ASR, lower is better) on GPT-4o, o3-mini, and Llama-3.3-70B across dif- ferent defense methods. Our proposed methods (MELON and MELON-Aug) achieve superior performance with extremely low ASR while m… | 0.80 | “Through extensive experimentation on the AgentDojo benchmark using three LLMs: GPT-4o, o3-mini, Llama-3.3- 70B, we demonstrate that MELON and MELON-Aug (com- bining MELON with prompt augmentation) significantly outperforms five SOTA defenses against four SOTA attacks.” (MELON Provable defense against indirect prompt injection attacks in AI agents.txt @段3) |

- 主题标签：Prompt Injection/环境注入, Web/Browser Agent 安全, 防御与对齐机制
- 方法标签：基准/Benchmark, 形式化/Optimization, 提示注入/Prompt Injection, 系统防御/Defense
- 源文件：`MELON Provable defense against indirect prompt injection attacks in AI agents.txt`

</details>
<details><summary>38. Cyber Defense Perspective（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Cyber Defense Perspective | 0.92 | “Cyber Defense Perspective” (Preventing jailbreak prompts as malicious tools for cybercriminals A cyber defense perspective.txt @段2) || 作者 | Jean Marie Tshimula,1,2 Xavier Ndona,3 D’Jeff K. Nkashama,1 Pierre-Martin Tardif,1 | 0.90 | “Jean Marie Tshimula,1,2 Xavier Ndona,3 D’Jeff K. Nkashama,1 Pierre-Martin Tardif,1” (Preventing jailbreak prompts as malicious tools for cybercriminals A cyber defense perspective.txt @段1) || 发表年份 | 2024 | 0.86 | “et al., 2023b; Xu et al., 2024d; Li et al., 2024b; An-” (Preventing jailbreak prompts as malicious tools for cybercriminals A cyber defense perspective.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2411.16642v1  [cs.CR]  25 Nov 2024” (Preventing jailbreak prompts as malicious tools for cybercriminals A cyber defense perspective.txt @段1) || 关键词 | prompts, 2024, jailbreak, case, adversarial, context, level, like, liu, systems | 0.78 | “Preventing Jailbreak Prompts as Malicious Tools for Cybercriminals: A Cyber Defense Perspective Jean Marie Tshimula,1,2 Xavier Ndona,3 D’Jeff K. Nkashama,1 Pierre-Martin Tardif,1 Froduald Kabanza,1 Marc Frappier,1 Shengrui Wang1 1Department of Computer Science, Université de Sherbrooke, QC J1K 2R1, Canada 2Department o” (Preventing jailbreak prompts as malicious tools for cybercriminals A cyber defense perspective.txt @段1) || 摘要 | Preventing Jailbreak Prompts as Malicious Tools for Cybercriminals: A Cyber Defense Perspective Jean Marie Tshimula,1,2 Xavier Ndona,3 D’Jeff K. Nkashama,1 Pierre-Martin Tardif,1 Froduald Kabanza,1 Marc Frappier,1 Shengrui Wang1 1Department of Computer Science, Université de Sherbrooke, QC J1K 2R1, Canada 2Department of Maths, Statistics and Computer Science, University of Kinshasa, DRC 3Harrisburg University of Science and Technology, PA 17101, United States of America {kabj2801,nkad2101,pierre-martin.tardif,frod… | 0.72 | “Preventing Jailbreak Prompts as Malicious Tools for Cybercriminals: A Cyber Defense Perspective Jean Marie Tshimula,1,2 Xavier Ndona,3 D’Jeff K. Nkashama,1 Pierre-Martin Tardif,1 Froduald Kabanza,1 Marc Frappier,1 Shengrui Wang1 1Department of Computer Science, Université de Sherbrooke, QC J1K 2R1, Canada 2Department o” (Preventing jailbreak prompts as malicious tools for cybercriminals A cyber defense perspective.txt @段1) || 研究方法 | 1 Introduction The rapid advancement of artificial intelligence (AI), particularly in the domain of large language models (LLMs), has ushered in a new era of techno- logical capabilities. These AI systems, exemplified by models such as GPT-3, GPT-4, and their counter- parts, have demonstrated remarkable proficiency in natural language processing, generation, and under- standing (Achiam et al., 2023). Their applications span a wide range of fields, from content creation and customer service to complex problem-solvi… | 0.82 | “By integrating prompt-level filtering, model-level mechanisms such as adversarial train- ing and self-critique, and innovative strategies like unlearning harmful knowledge (e.g., Eraser method), we propose a comprehensive framework for mitigating these risks (Lu et al., 2024b).” (Preventing jailbreak prompts as malicious tools for cybercriminals A cyber defense perspective.txt @段3) || 主要结论 | 3 Cyber defense perspective on preventing jailbreak prompts From a cyber defense perspective, preventing jail- break prompts becomes essential to securing LLMs against misuse by cybercriminals. As LLMs inte- grate more deeply across sectors such as finance, healthcare, education, and critical infrastructure, their vulnerabilities to adversarial manipulation in- troduce risks that extend far beyond typical mis- use scenarios. Cybercriminals exploit jailbreaks not only to generate phishing content, unautho- rized co… | 0.80 | “Moreover, re- cent research has demonstrated the effectiveness of advanced techniques such as attention manipula- tion (e.g., AttnGCG) to significantly improve the success rate of jailbreaks by exploiting the inter- nal workings of transformer models (Wang et al., 2024).” (Preventing jailbreak prompts as malicious tools for cybercriminals A cyber defense perspective.txt @段5) |

- 主题标签：GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击, Prompt Injection/环境注入, Web/Browser Agent 安全, 防御与对齐机制
- 方法标签：提示注入/Prompt Injection, 系统防御/Defense, 越狱/Jailbreak
- 源文件：`综述参考文献\Preventing jailbreak prompts as malicious tools for cybercriminals A cyber defense perspective.txt`

</details>
<details><summary>39. DISSECTING ADVERSARIAL ROBUSTNESS OF（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | DISSECTING ADVERSARIAL ROBUSTNESS OF | 0.92 | “DISSECTING ADVERSARIAL ROBUSTNESS OF” (Dissecting adversarial robustness of multimodal LM agents.txt @段2) || 作者 | Chen Henry Wu, Rishi Shah, Jing Yu Koh, Ruslan Salakhutdinov, | 0.90 | “Chen Henry Wu, Rishi Shah, Jing Yu Koh, Ruslan Salakhutdinov,” (Dissecting adversarial robustness of multimodal LM agents.txt @段1) || 发表年份 | 2024 | 0.86 | “Large language models (LMs) (OpenAI, 2023; Google, 2023; Anthropic, 2024) with strong generative” (Dissecting adversarial robustness of multimodal LM agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2406.12814v3  [cs.LG]  4 Feb 2025” (Dissecting adversarial robustness of multimodal LM agents.txt @段1) || 关键词 | adversarial, 2023, 2024, robustness, environment, components, attacker, lms, targeted, multimodal | 0.78 | “Published as a conference paper at ICLR 2025 DISSECTING ADVERSARIAL ROBUSTNESS OF MULTIMODAL LM A GENTS Chen Henry Wu, Rishi Shah, Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried, Aditi Raghunathan Carnegie Mellon University {chenwu2,rishisha,jingyuk,rsalakhu,dfried,aditirag}@cs.cmu.edu” (Dissecting adversarial robustness of multimodal LM agents.txt @段1) || 摘要 | Published as a conference paper at ICLR 2025 DISSECTING ADVERSARIAL ROBUSTNESS OF MULTIMODAL LM A GENTS Chen Henry Wu, Rishi Shah, Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried, Aditi Raghunathan Carnegie Mellon University {chenwu2,rishisha,jingyuk,rsalakhu,dfried,aditirag}@cs.cmu.edu ABSTRACT As language models (LMs) are used to build autonomous agents in real environ- ments, ensuring their adversarial robustness becomes a critical challenge. Unlike chatbots, agents are compound systems with multiple components… | 0.72 | “Published as a conference paper at ICLR 2025 DISSECTING ADVERSARIAL ROBUSTNESS OF MULTIMODAL LM A GENTS Chen Henry Wu, Rishi Shah, Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried, Aditi Raghunathan Carnegie Mellon University {chenwu2,rishisha,jingyuk,rsalakhu,dfried,aditirag}@cs.cmu.edu” (Dissecting adversarial robustness of multimodal LM agents.txt @段1) || 研究方法 | 1 I NTRODUCTION Large language models (LMs) (OpenAI, 2023; Google, 2023; Anthropic, 2024) with strong generative and reasoning capabilities have led to recent developments in building autonomous agents. These agents can tackle complex tasks across various environments, from web-based platforms to the physical world (Zheng et al., 2024; Koh et al., 2024a; Brohan et al., 2023). The transition from chatbots to autonomous agents opens up new possibilities for boosting productivity and accessibility, but also introduce… | 0.82 | “In order to systematically analyse and interpret the robustness of various compound agent systems, we propose the Agent Robustness Evaluation ( ARE) framework.” (Dissecting adversarial robustness of multimodal LM agents.txt @段3) || 主要结论 | 1 I NTRODUCTION Large language models (LMs) (OpenAI, 2023; Google, 2023; Anthropic, 2024) with strong generative and reasoning capabilities have led to recent developments in building autonomous agents. These agents can tackle complex tasks across various environments, from web-based platforms to the physical world (Zheng et al., 2024; Koh et al., 2024a; Brohan et al., 2023). The transition from chatbots to autonomous agents opens up new possibilities for boosting productivity and accessibility, but also introduce… | 0.80 | “We show that an attacker can achieve this with a strikingly small change in a very realistic threat model: they add imperceptible perturbations of magnitude 16/256 pixels to just their own product image, which takes less than 5% of the web page pixels input to the agent.” (Dissecting adversarial robustness of multimodal LM agents.txt @段3) |

- 主题标签：Jailbreak/越狱攻击, Web/Browser Agent 安全, 防御与对齐机制
- 方法标签：仿真/Simulator, 基准/Benchmark, 多模态/Multimodal, 系统防御/Defense
- 源文件：`综述参考文献\Dissecting adversarial robustness of multimodal LM agents.txt`

</details>
<details><summary>40. Dynamically Encrypted Multi-Backdoor Im-（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Dynamically Encrypted Multi-Backdoor Im- | 0.92 | “Dynamically Encrypted Multi-Backdoor Im-” (DemonAgent Dynamically encrypted multi-backdoor implantation attack on LLM-based agent.txt @段22) || 作者 | Pengyu Zhu1,⋆ | 0.90 | “Pengyu Zhu1,⋆” (DemonAgent Dynamically encrypted multi-backdoor implantation attack on LLM-based agent.txt @段1) || 发表年份 | 2024 | 0.86 | “markable performance (OpenAI, 2024a), catalyz-” (DemonAgent Dynamically encrypted multi-backdoor implantation attack on LLM-based agent.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2502.12575v2  [cs.CR]  13 Oct 2025” (DemonAgent Dynamically encrypted multi-backdoor implantation attack on LLM-based agent.txt @段1) || 关键词 | backdoor, 2023, audits, encrypted, 2024, implantation, multi, tools, ing, through | 0.78 | “DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent Pengyu Zhu1,⋆ , Zhenhong Zhou1,⋆ , Yuanhe Zhang1, Shilinlu Yan1, Kun Wang2, Sen Su1, † 1Beijing University of Posts and Telecommunications 2Nanyang Technological University {whfelingyu_zhupengyu, zhouzhenhong, charmes-zhang, lulu_la” (DemonAgent Dynamically encrypted multi-backdoor implantation attack on LLM-based agent.txt @段1) || 摘要 | DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent Pengyu Zhu1,⋆ , Zhenhong Zhou1,⋆ , Yuanhe Zhang1, Shilinlu Yan1, Kun Wang2, Sen Su1, † 1Beijing University of Posts and Telecommunications 2Nanyang Technological University {whfelingyu_zhupengyu, zhouzhenhong, charmes-zhang, lulu_land, susen}@bupt.edu.cn; wk520529@mail.ustc.edu.cn Abstract As LLM-based agents become increasingly prevalent, triggers implanted in user queries or environment feedback can activate hidden back- door… | 0.72 | “DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent Pengyu Zhu1,⋆ , Zhenhong Zhou1,⋆ , Yuanhe Zhang1, Shilinlu Yan1, Kun Wang2, Sen Su1, † 1Beijing University of Posts and Telecommunications 2Nanyang Technological University {whfelingyu_zhupengyu, zhouzhenhong, charmes-zhang, lulu_la” (DemonAgent Dynamically encrypted multi-backdoor implantation attack on LLM-based agent.txt @段1) || 研究方法 | 3.2 Dynamically Encryption Mechanism (DEM) In this section, we introduce a dynamic encryption mechanism (DEM) that ensures the encrypted con- Algorithm 1ReAct Algorithm Input:UserQueryq, ToolSetI s Output:FinalAnswerAns Initialization:Set memory: Im ← ∅, Store query: I 0 m ←q 1:fori∈N + do 2:Thought i ←I L(I i−1 m ) 3:ifRin Thought i then 4:I i m ←I i−1 m ∪Thought i 5:break 6:end if 7:ToolReturn i ←ToolCall(I s,Thought i) 8:I i m ←I i−1 m ∪Thought i ∪ToolReturn i 9:end for 10:Ans←I L(I i m) 11:returnAns tent evolv… | 0.82 | “3.2 Dynamically Encryption Mechanism (DEM) In this section, we introduce a dynamic encryption mechanism (DEM) that ensures the encrypted con- Algorithm 1ReAct Algorithm Input:UserQueryq, ToolSetI s Output:FinalAnswerAns Initialization:Set memory: Im ← ∅, Store query: I 0 m ←q 1:fori∈N + do 2:Thought i ←I L(I i−1 m ) 3:” (DemonAgent Dynamically encrypted multi-backdoor implantation attack on LLM-based agent.txt @段9) || 主要结论 | 0 100 91.67 0 100 100 Table 6: Attack performance on Athene-V2-Agent using the AgentInstruct benchmark. Model NP (%) AgentLM (2023) 5.83 Athene-V2-Agent 91.67 Table 7: Normal Task Performance (NP) of AgentLM and Athene-V2-Agent on the AgentInstruct benchmark. The following sections present their evaluation results and analyses in detail. J.1 Practical Value of Modern Fine-tuned Agent Models The results in Table 7 show that AgentLM (2023) achieves only 5.83% Normal Task Performance (NP) on the AgentInstruct benchma… | 0.80 | “Its extremely low NP indicates se- vere limitations in handling basic tasks and tool invocations, rendering it impractical for evaluating advanced attack methodologies.” (DemonAgent Dynamically encrypted multi-backdoor implantation attack on LLM-based agent.txt @段36) |

- 主题标签：Backdoor/投毒, 多智能体对抗与协作, 隐私与记忆风险
- 方法标签：仿真/Simulator, 后门/Backdoor, 基准/Benchmark
- 源文件：`综述参考文献\DemonAgent Dynamically encrypted multi-backdoor implantation attack on LLM-based agent.txt`

</details>
<details><summary>41. Enhancing LLM Agent Safety via Causal Influence Prompting（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Enhancing LLM Agent Safety via Causal Influence Prompting | 0.92 | “Enhancing LLM Agent Safety via Causal Influence Prompting” (Enhancing LLM agent safety via causal influence prompting.txt @段3) || 作者 | Findings of the Association for Computational Linguistics: ACL 2025 , pages 15143–15168 | 0.90 | “Findings of the Association for Computational Linguistics: ACL 2025 , pages 15143–15168” (Enhancing LLM agent safety via causal influence prompting.txt @段1) || 发表年份 | 2024 | 0.86 | “bile device control (Rawles et al., 2024; Lee et al.,” (Enhancing LLM agent safety via causal influence prompting.txt @段1) || 期刊/会议 | Findings of the Association for Computational Linguistics: ACL 2025 , pages 15143–15168 | 0.86 | “Findings of the Association for Computational Linguistics: ACL 2025 , pages 15143–15168” (Enhancing LLM agent safety via causal influence prompting.txt @段1) || 关键词 | cid, causal, decision, code, influence, 2024, making, node, add, message | 0.78 | “Findings of the Association for Computational Linguistics: ACL 2025 , pages 15143–15168 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Enhancing LLM Agent Safety via Causal Influence Prompting Dongyoon Hahm Woogyeol Jin June Suk Choi Sungsoo Ahn Kimin Lee KAIST {hahmdong, wlsdnruf2, w_choi, su” (Enhancing LLM agent safety via causal influence prompting.txt @段1) || 摘要 | Findings of the Association for Computational Linguistics: ACL 2025 , pages 15143–15168 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Enhancing LLM Agent Safety via Causal Influence Prompting Dongyoon Hahm Woogyeol Jin June Suk Choi Sungsoo Ahn Kimin Lee KAIST {hahmdong, wlsdnruf2, w_choi, sungsoo.ahn, kiminlee}@kaist.ac.kr Abstract As autonomous agents powered by large lan- guage models (LLMs) continue to demonstrate potential across various assistive tasks, ensur- ing their safe and re… | 0.72 | “Findings of the Association for Computational Linguistics: ACL 2025 , pages 15143–15168 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Enhancing LLM Agent Safety via Causal Influence Prompting Dongyoon Hahm Woogyeol Jin June Suk Choi Sungsoo Ahn Kimin Lee KAIST {hahmdong, wlsdnruf2, w_choi, su” (Enhancing LLM agent safety via causal influence prompting.txt @段1) || 研究方法 | 1 Introduction Autonomous agents using large language mod- els (LLMs) have demonstrated outstanding per- formance across various domains, including web searching (Yao et al., 2022a; Zhou et al., 2023), mo- bile device control (Rawles et al., 2024; Lee et al., 2024b), and software engineering (Jimenez et al., 2023; Shinn et al., 2024). Unlike conventional LLMs, which mainly generate text responses, LLM agents engage in decision-making, utilize tools, and interact with their environment to accomplish complex tasks. … | 0.82 | “Specifically, our approach consists of three key steps: (1) constructing a CID from task specifications to outline the decision-making pro- cess, (2) using the CID framework to guide agent interactions with the environment, and (3) refining the CID iteratively based on observed behaviors and outcomes (see Figure 1 for” (Enhancing LLM agent safety via causal influence prompting.txt @段3) || 主要结论 | 1 Introduction Autonomous agents using large language mod- els (LLMs) have demonstrated outstanding per- formance across various domains, including web searching (Yao et al., 2022a; Zhou et al., 2023), mo- bile device control (Rawles et al., 2024; Lee et al., 2024b), and software engineering (Jimenez et al., 2023; Shinn et al., 2024). Unlike conventional LLMs, which mainly generate text responses, LLM agents engage in decision-making, utilize tools, and interact with their environment to accomplish complex tasks. … | 0.80 | “Moreover, our results indicate that CIP enhances robustness against two types of prompt injection attacks: in- direct prompt injection (Greshake et al., 2023), where a malicious prompt is embedded within environmental observations to mislead the agent, and template-based attacks (Andriushchenko et al., 2024a), which le” (Enhancing LLM agent safety via causal influence prompting.txt @段3) |

- 主题标签：GUI/Computer-Use Agent 安全, Web/Browser Agent 安全
- 方法标签：仿真/Simulator, 基准/Benchmark, 多模态/Multimodal
- 源文件：`Enhancing LLM agent safety via causal influence prompting.txt`

</details>
<details><summary>42. Evaluating Cultural and Social Awareness of LLM Web Agents（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Evaluating Cultural and Social Awareness of LLM Web Agents | 0.92 | “Evaluating Cultural and Social Awareness of LLM Web Agents” (Evaluating cultural and social awareness of LLM web agents.txt @段1) || 作者 | Evaluating Cultural and Social Awareness of LLM Web Agents | 0.90 | “Evaluating Cultural and Social Awareness of LLM Web Agents” (Evaluating cultural and social awareness of LLM web agents.txt @段1) || 发表年份 | 2024 | 0.86 | “2024; Huang et al., 2025). This broadening scope” (Evaluating cultural and social awareness of LLM web agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2410.23252v3  [cs.CL]  8 Mar 2025” (Evaluating cultural and social awareness of LLM web agents.txt @段1) || 关键词 | social, cultural, user, queries, awareness, norms, online, shopping, web, benchmark | 0.78 | “Evaluating Cultural and Social Awareness of LLM Web Agents Haoyi Qiu♡† Alexander R. Fabbri♢∗ Divyansh Agarwal♢∗ Kung-Hsiang Huang♢∗ Sarah Tan♢ Nanyun Peng♡ Chien-Sheng Wu♢ ♡University of California, Los Angeles ♢Salesforce AI Research {haoyiqiu,violetpeng}@cs.ucla.edu {afabbri,divyansh.agarwal,kh.huang,sarah.tan,wu.jas” (Evaluating cultural and social awareness of LLM web agents.txt @段1) || 摘要 | Evaluating Cultural and Social Awareness of LLM Web Agents Haoyi Qiu♡† Alexander R. Fabbri♢∗ Divyansh Agarwal♢∗ Kung-Hsiang Huang♢∗ Sarah Tan♢ Nanyun Peng♡ Chien-Sheng Wu♢ ♡University of California, Los Angeles ♢Salesforce AI Research {haoyiqiu,violetpeng}@cs.ucla.edu {afabbri,divyansh.agarwal,kh.huang,sarah.tan,wu.jason}@salesforce.com Abstract As large language models (LLMs) expand into performing as agents for real-world applica- tions beyond traditional NLP tasks, evaluating their robustness becomes increasing… | 0.72 | “Evaluating Cultural and Social Awareness of LLM Web Agents Haoyi Qiu♡† Alexander R. Fabbri♢∗ Divyansh Agarwal♢∗ Kung-Hsiang Huang♢∗ Sarah Tan♢ Nanyun Peng♡ Chien-Sheng Wu♢ ♡University of California, Los Angeles ♢Salesforce AI Research {haoyiqiu,violetpeng}@cs.ucla.edu {afabbri,divyansh.agarwal,kh.huang,sarah.tan,wu.jas” (Evaluating cultural and social awareness of LLM web agents.txt @段1) || 研究方法 | 4 Evaluation Framework In this section, we present an evaluation framework to assess LLM agents behavior using our bench- mark, employing GPT-4o as a LLM judge for each metric. Detailed prompts are in Appendix B. | 0.82 | “4 Evaluation Framework In this section, we present an evaluation framework to assess LLM agents behavior using our bench- mark, employing GPT-4o as a LLM judge for each metric.” (Evaluating cultural and social awareness of LLM web agents.txt @段8) || 主要结论 | 5.2 S2: Norms Sensitivity in Observations Moreover, we aim to learn the performance of LLM agents in more complex user interactions. Specif- ically, we want to investigate RQ3 Can LLM NON AGENT AGENT MODELS ORIG. R OLE-UPD. ORIG. 2NE+1SE 2SE R OLE-UPD. CA-U PD.+2NE CA-U PD.+2SE S F S F S F S F S F S F S F S F GPT-4o-mini 94.35 94.17 94.35 94.17 88.65 87.37 85.31 84.84 83.29 83.67 89.47 89.68 85.31 82.17 75.71 83.67 GPT-4o 94.35 93.59 94.07 94.46 90.25 86.88 86.16 85.42 86.86 88.63 89.27 88.34 91.53 87.76 86.02 88.… | 0.80 | “As shown in Table 6, we find: (1) Fine-tuning GPT-3.5 with 1k culture-related data points slightly improves the success rate (3.73% to 3.77%), but 10k data points decrease it to 3.33%, suggesting a trade-off between cultural and so- cial awareness and general task performance; ( 2) Advanced models like GPT-4o and GPT-4” (Evaluating cultural and social awareness of LLM web agents.txt @段17) |

- 主题标签：GUI/Computer-Use Agent 安全, Web/Browser Agent 安全
- 方法标签：基准/Benchmark, 多模态/Multimodal
- 源文件：`综述参考文献\Evaluating cultural and social awareness of LLM web agents.txt`

</details>
<details><summary>43. Evolving Security Threats（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Evolving Security Threats | 0.92 | “Evolving Security Threats” (DoomArena A framework for testing AI agents against evolving security threats.txt @段3) || 作者 | Leo Boisvert†‡, Mihir Bansal†, Chandra Kiran Reddy Evuru†, Gabriel Huang†, Abhay Puri†, | 0.90 | “Leo Boisvert†‡, Mihir Bansal†, Chandra Kiran Reddy Evuru†, Gabriel Huang†, Abhay Puri†,” (DoomArena A framework for testing AI agents against evolving security threats.txt @段1) || 发表年份 | 2024 | 0.86 | “in the enterprise (Drouin et al., 2024; Xu et al., 2024), in scientific applications (Gottweis et al.,” (DoomArena A framework for testing AI agents against evolving security threats.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2504.14064v3  [cs.CR]  7 Oct 2025” (DoomArena A framework for testing AI agents against evolving security threats.txt @段1) || 关键词 | doomarena, environment, 2024, malicious, threat, user, 2025, framework, agentic, components | 0.78 | “Published as a conference paper at COLM 2025 DoomArena: A Framework for Testing AI Agents Against Evolving Security Threats Leo Boisvert†‡, Mihir Bansal†, Chandra Kiran Reddy Evuru†, Gabriel Huang†, Abhay Puri†, ServiceNow Research Avinandan Bose†, Maryam Fazel University of Washington, Seattle Quentin Cappart‡ Polytec” (DoomArena A framework for testing AI agents against evolving security threats.txt @段1) || 摘要 | Published as a conference paper at COLM 2025 DoomArena: A Framework for Testing AI Agents Against Evolving Security Threats Leo Boisvert†‡, Mihir Bansal†, Chandra Kiran Reddy Evuru†, Gabriel Huang†, Abhay Puri†, ServiceNow Research Avinandan Bose†, Maryam Fazel University of Washington, Seattle Quentin Cappart‡ Polytechnique Montréal Jason Stanley, Alexandre Lacoste, Alexandre Drouin‡, Krishnamurthy (Dj) Dvijotham ServiceNow Research Correspondence to:leo.boisvert@servicenow.com,dvij@cs.washington.edu Abstract We … | 0.72 | “Published as a conference paper at COLM 2025 DoomArena: A Framework for Testing AI Agents Against Evolving Security Threats Leo Boisvert†‡, Mihir Bansal†, Chandra Kiran Reddy Evuru†, Gabriel Huang†, Abhay Puri†, ServiceNow Research Avinandan Bose†, Maryam Fazel University of Washington, Seattle Quentin Cappart‡ Polytec” (DoomArena A framework for testing AI agents against evolving security threats.txt @段1) || 研究方法 | References Sahar Abdelnabi, Amr Gomaa, Eugene Bagdasarian, Per Ola Kristensson, and Reza Shokri. Firewalls to secure dynamic llm agentic networks.arXiv preprint arXiv:2502.01822, 2025. Altimetrik. Understanding prompt injection attacks.https://www.altimetrik.com/blog/ai -security-prompt-injection-attacks, 2024. Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson, et al. Agentharm: A benchmark for measuring harmfulne… | 0.82 | “A.5 Architecture of the Generalized Attacker Agent We designed a configurable attacker agent with the capability of performing different types of attacks across various frameworks such asτ-Bench and BrowserGym ( Fig.” (DoomArena A framework for testing AI agents against evolving security threats.txt @段16) || 主要结论 | 1 Introduction The rise of AI agents brings up exciting possibilities to automate valuable but repetitive tasks in the enterprise (Drouin et al., 2024; Xu et al., 2024), in scientific applications (Gottweis et al., 2025), and in knowledge work (OpenAI, 2025). However, the existence of autonomous agents also poses several security risks, including leakage of sensitive data (Zharmagambetov et al., 2025), privileged access, the proliferation of unauthorized financial transactions, etc. Several works demonstrating suc… | 0.80 | “We demonstrate the advantages of DoomArena in several ways: 1) We implement several well-known attacks and show how they can be easily combined via attack configurations in our framework, supporting security evaluations in the face of an evolving landscape of risks.” (DoomArena A framework for testing AI agents against evolving security threats.txt @段3) |

- 主题标签：Backdoor/投毒, GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击, Prompt Injection/环境注入, Web/Browser Agent 安全, 多智能体对抗与协作, 隐私与记忆风险
- 方法标签：仿真/Simulator, 后门/Backdoor, 基准/Benchmark, 提示注入/Prompt Injection, 红队/Red-Teaming
- 源文件：`综述参考文献\DoomArena A framework for testing AI agents against evolving security threats.txt`

</details>
<details><summary>44. Existence of Probably Approximately Aligned Policies（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Existence of Probably Approximately Aligned Policies | 0.92 | “Existence of Probably Approximately Aligned Policies” (Can an AI agent safely run a government Existence of probably approximately aligned policies.txt @段2) || 作者 | complex data, their potential misalignment (i.e., lack of transparency regarding | 0.90 | “complex data, their potential misalignment (i.e., lack of transparency regarding” (Can an AI agent safely run a government Existence of probably approximately aligned policies.txt @段1) || 发表年份 | 2024 | 0.86 | “38th Conference on Neural Information Processing Systems (NeurIPS 2024).” (Can an AI agent safely run a government Existence of probably approximately aligned policies.txt @段1) || 期刊/会议 | 38th Conference on Neural Information Processing Systems (NeurIPS 2024). | 0.88 | “38th Conference on Neural Information Processing Systems (NeurIPS 2024).” (Can an AI agent safely run a government Existence of probably approximately aligned policies.txt @段1) || 关键词 | social, policies, aligned, alignment, autonomous, any, decision, utility, which, actions | 0.78 | “Can an AI Agent Safely Run a Government? Existence of Probably Approximately Aligned Policies Frédéric Berdoz ETH Zürich fberdoz@ethz.ch Roger Wattenhofer ETH Zürich wattenhofer@ethz.ch” (Can an AI agent safely run a government Existence of probably approximately aligned policies.txt @段1) || 摘要 | Can an AI Agent Safely Run a Government? Existence of Probably Approximately Aligned Policies Frédéric Berdoz ETH Zürich fberdoz@ethz.ch Roger Wattenhofer ETH Zürich wattenhofer@ethz.ch Abstract While autonomous agents often surpass humans in their ability to handle vast and complex data, their potential misalignment (i.e., lack of transparency regarding their true objective) has thus far hindered their use in critical applications such as social decision processes. More importantly, existing alignment methods pro… | 0.72 | “Can an AI Agent Safely Run a Government? Existence of Probably Approximately Aligned Policies Frédéric Berdoz ETH Zürich fberdoz@ethz.ch Roger Wattenhofer ETH Zürich wattenhofer@ethz.ch” (Can an AI agent safely run a government Existence of probably approximately aligned policies.txt @段1) || 研究方法 | Abstract While autonomous agents often surpass humans in their ability to handle vast and complex data, their potential misalignment (i.e., lack of transparency regarding their true objective) has thus far hindered their use in critical applications such as social decision processes. More importantly, existing alignment methods provide no formal guarantees on the safety of such models. Drawing from utility and social choice theory, we provide a novel quantitative definition of alignment in the context of social de… | 0.82 | “Lastly, recognizing the practical difficulty of satisfying this condition, we introduce the relaxed concept of safe (i.e., nondestructive) policies, and we propose a simple yet robust method to safeguard the black-box policy of any autonomous agent, ensuring all its actions are verifiably safe for the society.” (Can an AI agent safely run a government Existence of probably approximately aligned policies.txt @段2) || 主要结论 | 4 Related work MDP for social choice MDPs have already been used in the context of social decision processes. For instance, Parkes and Procaccia [28] (and more recently [21]) use social choice MDPs in order to tackle decision-making under dynamic preferences. However, their setting is significantly different from ours: In their work, the state space S is the set of preference profiles U N , and p dictate how these preferences evolve based on the outcome selected by the social choice functional (the policy). Anothe… | 0.80 | “However, their setting is significantly different from ours: In their work, the state space S is the set of preference profiles U N , and p dictate how these preferences evolve based on the outcome selected by the social choice functional (the policy).” (Can an AI agent safely run a government Existence of probably approximately aligned policies.txt @段19) |

- 主题标签：防御与对齐机制
- 方法标签：系统防御/Defense, 调查/Survey
- 源文件：`Can an AI agent safely run a government Existence of probably approximately aligned policies.txt`

</details>
<details><summary>45. Exploring LLM-Based Agents for Root Cause Analysis（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Exploring LLM-Based Agents for Root Cause Analysis | 0.92 | “Exploring LLM-Based Agents for Root Cause Analysis” (Exploring LLM-based agents for root cause analysis.txt @段7) || 作者 | DEVJEET ROY, Washington State University Pullman, Pullman, WA, United States | 0.90 | “DEVJEET ROY, Washington State University Pullman, Pullman, WA, United States” (Exploring LLM-based agents for root cause analysis.txt @段1) || 发表年份 | 2024 | 0.90 | “Published: 10 July 2024” (Exploring LLM-based agents for root cause analysis.txt @段1) || 期刊/会议 | Conference on the Foundations of | 0.88 | “Conference on the Foundations of” (Exploring LLM-based agents for root cause analysis.txt @段1) || 关键词 | microsoft, redmond, washington, 2024, acm, root, usa, analysis, cause, research | 0.78 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3663529.3663841 . . RESEARCH-ARTICLE Exploring LLM-Based Agents for Root Cause Analysis DEVJEET ROY, Washington State University Pullman, Pullman, WA, United States . XUCHAO ZHANG, Microso Research, Redmond, WA, United States . RASHI BHAVE, Microso Research, Redmond,” (Exploring LLM-based agents for root cause analysis.txt @段1) || 摘要 | . . Latest updates: hps://dl.acm.org/doi/10.1145/3663529.3663841 . . RESEARCH-ARTICLE Exploring LLM-Based Agents for Root Cause Analysis DEVJEET ROY, Washington State University Pullman, Pullman, WA, United States . XUCHAO ZHANG, Microso Research, Redmond, WA, United States . RASHI BHAVE, Microso Research, Redmond, WA, United States . CHETAN BANSAL, Microso Research, Redmond, WA, United States . PEDRO H B LAS-CASAS, Microso Corporation, Redmond, WA, United States . RODRIGO FONSECA, Microso Research, Redmond,… | 0.72 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3663529.3663841 . . RESEARCH-ARTICLE Exploring LLM-Based Agents for Root Cause Analysis DEVJEET ROY, Washington State University Pullman, Pullman, WA, United States . XUCHAO ZHANG, Microso Research, Redmond, WA, United States . RASHI BHAVE, Microso Research, Redmond,” (Exploring LLM-based agents for root cause analysis.txt @段1) || 研究方法 | 1 INTRODUCTION For the last several decades, large scale enterprises have been trans- forming their software into cloud services. With the rise of Arti/f_icial Intelligence (AI) in recent years, there has been even greater move- ment of computation from consumer devices to the cloud. This shift in paradigm has brought with it complex software systems that are characterized by multi-tiered architectures, microservices and distributed applications. The increased complexity of these systems makes them highly suscepti… | 0.82 | “Lastly, to explore the full potential of agents, we present a case study of a practical implementation of an LLM agent for RCA, fully equipped with team speci/f_ic diagnostic resources, in collaboration with another team at Microsoft ˙Concretely, we make the following contributions: • We present the /f_irst empirical s” (Exploring LLM-based agents for root cause analysis.txt @段7) || 主要结论 | ABSTRACT Root cause analysis (RCA), a critical part of the incident manage- ment process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team’s speci/f_ic services. Automation of RCA can result in signi/f_icant sav- ings of time, and ease the burden of incident management on on-call engineers. Recently, researchers have utilized Large Language Mod- els (LLMs) to perform RCA, and have demonstrated promising results. However, these approaches are not able t… | 0.80 | “Our results show how agents can overcome the limitations of prior work, and considerations for implementing such a system in practice.” (Exploring LLM-based agents for root cause analysis.txt @段4) |

- 主题标签：Web/Browser Agent 安全
- 方法标签：基准/Benchmark
- 源文件：`Exploring LLM-based agents for root cause analysis.txt`

</details>
<details><summary>46. FOR MULTIMODAL GUI- ORIENTED UNDERSTANDING（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | FOR MULTIMODAL GUI- ORIENTED UNDERSTANDING | 0.92 | “FOR MULTIMODAL GUI- ORIENTED UNDERSTANDING” (GUI-world A video benchmark and dataset for multimodal GUI-oriented understanding.txt @段3) || 作者 | GUI-W ORLD : A V IDEO BENCHMARK AND DATASET | 0.90 | “GUI-W ORLD : A V IDEO BENCHMARK AND DATASET” (GUI-world A video benchmark and dataset for multimodal GUI-oriented understanding.txt @段1) || 发表年份 | 2024 | 0.86 | “language domains (Yin et al., 2024). These models bring forth innovative solutions and paradigms” (GUI-world A video benchmark and dataset for multimodal GUI-oriented understanding.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2406.10819v2  [cs.CV]  24 Mar 2025” (GUI-world A video benchmark and dataset for multimodal GUI-oriented understanding.txt @段1) || 关键词 | gui, uni00000013, video, understanding, orld, dataset, oriented, software, uni00000051, dynamic | 0.78 | “Published as a conference paper at ICLR 2025 GUI-W ORLD : A V IDEO BENCHMARK AND DATASET FOR MULTIMODAL GUI- ORIENTED UNDERSTANDING Dongping Chen1∗, Yue Huang2∗, Siyuan Wu1, Jingyu Tang1, Liuyi Chen1, Yilin Bai1, Zhigang He1, Chenlong Wang1, Huichi Zhou3, Yiqiang Li1, Tianshuo Zhou1, Yue Yu1, Chujie Gao1, Qihui Zhang4,” (GUI-world A video benchmark and dataset for multimodal GUI-oriented understanding.txt @段1) || 摘要 | Published as a conference paper at ICLR 2025 GUI-W ORLD : A V IDEO BENCHMARK AND DATASET FOR MULTIMODAL GUI- ORIENTED UNDERSTANDING Dongping Chen1∗, Yue Huang2∗, Siyuan Wu1, Jingyu Tang1, Liuyi Chen1, Yilin Bai1, Zhigang He1, Chenlong Wang1, Huichi Zhou3, Yiqiang Li1, Tianshuo Zhou1, Yue Yu1, Chujie Gao1, Qihui Zhang4, Yi Gui1, Zhen Li1, Yao Wan1†, Pan Zhou1†, Jianfeng Gao5, Lichao Sun6 1Huazhong University of Science and Technology, 2University of Notre Dame 3Imperial College London, 4Peking University, 5Microsof… | 0.72 | “Published as a conference paper at ICLR 2025 GUI-W ORLD : A V IDEO BENCHMARK AND DATASET FOR MULTIMODAL GUI- ORIENTED UNDERSTANDING Dongping Chen1∗, Yue Huang2∗, Siyuan Wu1, Jingyu Tang1, Liuyi Chen1, Yilin Bai1, Zhigang He1, Chenlong Wang1, Huichi Zhou3, Yiqiang Li1, Tianshuo Zhou1, Yue Yu1, Chujie Gao1, Qihui Zhang4,” (GUI-world A video benchmark and dataset for multimodal GUI-oriented understanding.txt @段1) || 研究方法 | 2.1 O VERVIEW We introduce GUI-W ORLD , a comprehensive dataset covering six GUI scenarios including video, human-annotated keyframes, as well as detailed captions and diverse types of QA produced by our data curation framework, aiming at benchmarking and enhancing the general GUI-oriented Figure 3: An overview construction pipeline of GUI-W ORLD . 3 Published as a conference paper at ICLR 2025 Table 2: The statistics of GUI-W ORLD . For Android, we select videos from Rico (Deka et al., 2017) and randomly sample 1… | 0.82 | “2.1 O VERVIEW We introduce GUI-W ORLD , a comprehensive dataset covering six GUI scenarios including video, human-annotated keyframes, as well as detailed captions and diverse types of QA produced by our data curation framework, aiming at benchmarking and enhancing the general GUI-oriented Figure 3: An overview constru” (GUI-world A video benchmark and dataset for multimodal GUI-oriented understanding.txt @段5) || 主要结论 | 4.2 E XPERIMENTS Experiment Setups. We use two dataset settings to fine-tuneGUI-V ID, one with video only, and the other with video and image, detailed in Appendix C. We also vary the number of keyframes (8, 16) fed into GUI-V ID. All our experiments are conducted on A800 and 4090 GPUs. Software Website XR Multi IOS Android Average 2.0 2.4 2.8Score Image+Video Original Stage1 Stage2 Software Website XR Multi IOS Android Average 2.0 2.4 2.8Score Only Video Figure 8: Two stages of progressive training enhance GUI ab… | 0.80 | “As a pio- neering study in training video LLMs as screen agents, GUI- VID significantly outperforms the baseline model, showing an average improvement of 30% across various tasks and GUI scenarios, even surpassing the commercial vision LLM, Qwen- VL-Max.” (GUI-world A video benchmark and dataset for multimodal GUI-oriented understanding.txt @段14) |

- 主题标签：GUI/Computer-Use Agent 安全, Web/Browser Agent 安全
- 方法标签：仿真/Simulator, 基准/Benchmark, 多模态/Multimodal, 调查/Survey
- 源文件：`综述参考文献\GUI-world A video benchmark and dataset for multimodal GUI-oriented understanding.txt`

</details>
<details><summary>47. From Safety Benchmarks to Input Moderation（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | From Safety Benchmarks to Input Moderation | 0.92 | “From Safety Benchmarks to Input Moderation” (Advancing embodied agent security From safety benchmarks to input moderation.txt @段2) || 作者 | Ning Wang1 , Zihan Yan1 , Weiyang Li1 , Chuan Ma1∗ , He Chen2 and Tao Xiang1 | 0.90 | “Ning Wang1 , Zihan Yan1 , Weiyang Li1 , Chuan Ma1∗ , He Chen2 and Tao Xiang1” (Advancing embodied agent security From safety benchmarks to input moderation.txt @段1) || 发表年份 | 2024 | 0.86 | “cuted by unmanned systems [Vemprala et al., 2024; Singh et” (Advancing embodied agent security From safety benchmarks to input moderation.txt @段1) || 期刊/会议 | Proceedings of the Thirty-Fourth International Joint Conference on Artiﬁcial Intelligence (IJCAI-25) | 0.88 | “Proceedings of the Thirty-Fourth International Joint Conference on Artiﬁcial Intelligence (IJCAI-25)” (Advancing embodied agent security From safety benchmarks to input moderation.txt @段1) || 关键词 | moderation, embodied, input, methods, 2024, external, ing, prompts, 2023, behavioral | 0.78 | “Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation Ning Wang1 , Zihan Yan1 , Weiyang Li1 , Chuan Ma1∗ , He Chen2 and Tao Xiang1 1College of computer science, Chongqing University 2Department of Information Engineering, The Chinese University of Hong Kong nwang5@cqu.edu.cn, {zihan.yan, weiyang” (Advancing embodied agent security From safety benchmarks to input moderation.txt @段1) || 摘要 | Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation Ning Wang1 , Zihan Yan1 , Weiyang Li1 , Chuan Ma1∗ , He Chen2 and Tao Xiang1 1College of computer science, Chongqing University 2Department of Information Engineering, The Chinese University of Hong Kong nwang5@cqu.edu.cn, {zihan.yan, weiyangli}@stu.cqu.edu.cn, chuan.ma@cqu.edu.cn, he.chen@ie.cuhk.edu.hk, txiang@cqu.edu.cn, Abstract Embodied agents exhibit immense potential across a multitude of domains, making the assurance of their beh… | 0.72 | “Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation Ning Wang1 , Zihan Yan1 , Weiyang Li1 , Chuan Ma1∗ , He Chen2 and Tao Xiang1 1College of computer science, Chongqing University 2Department of Information Engineering, The Chinese University of Hong Kong nwang5@cqu.edu.cn, {zihan.yan, weiyang” (Advancing embodied agent security From safety benchmarks to input moderation.txt @段1) || 研究方法 | 1 Introduction Embodied agents, which possess the ability to dynamically perceive their environment and make autonomous decisions, represent a promising avenue for integrating artificial intelli- gence into human life. ∗Corresponding Author. This technology exhibits significant potential across di- verse fields, such as military operations and domestic ser- vices. However, a fundamental prerequisite for the successful deployment of embodied agents in real-world applications is ensuring the behavioral safety of the… | 0.82 | “We summarize our key contributions as follows: • We present a comprehensive moderation framework de- signed to ensure the input safety for embodied agents, encompassing the entire pipeline from taxonomy defini- tion and dataset construction to moderator design, model training, and evaluation.” (Advancing embodied agent security From safety benchmarks to input moderation.txt @段3) || 主要结论 | N/A | 0.50 | N/A |

- 主题标签：Web/Browser Agent 安全, 防御与对齐机制
- 方法标签：仿真/Simulator, 基准/Benchmark, 系统防御/Defense
- 源文件：`Advancing embodied agent security From safety benchmarks to input moderation.txt`

</details>
<details><summary>48. Identifying Performance-Sensitive Configurations in Software（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Identifying Performance-Sensitive Configurations in Software | 0.92 | “Identifying Performance-Sensitive Configurations in Software” (Identifying performance-sensitive configurations in software systems through code analysis with LLM.txt @段1) || 作者 | Zehao Wang, Dong Jae Kim, Tse-Husn (Peter) Chen | 0.90 | “Zehao Wang, Dong Jae Kim, Tse-Husn (Peter) Chen” (Identifying performance-sensitive configurations in software systems through code analysis with LLM.txt @段1) || 发表年份 | 2024 | 0.90 | “peterc@encs.concordia.ca. 2024. Identifying Performance-Sensitive Config-” (Identifying performance-sensitive configurations in software systems through code analysis with LLM.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2406.12806v1  [cs.SE]  18 Jun 2024” (Identifying performance-sensitive configurations in software systems through code analysis with LLM.txt @段1) || 关键词 | performance, configurations, sensitive, code, perfsense, analysis, software, concordia, identifying, section | 0.78 | “Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents Zehao Wang, Dong Jae Kim, Tse-Husn (Peter) Chen Software PErformance, Analysis and Reliability (SPEAR) Lab Concordia University, Montreal, Canada w_zeha@encs.concordia.ca, k_dongja@encs.concordia.ca, peterc@encs.c” (Identifying performance-sensitive configurations in software systems through code analysis with LLM.txt @段1) || 摘要 | Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents Zehao Wang, Dong Jae Kim, Tse-Husn (Peter) Chen Software PErformance, Analysis and Reliability (SPEAR) Lab Concordia University, Montreal, Canada w_zeha@encs.concordia.ca, k_dongja@encs.concordia.ca, peterc@encs.concordia.ca ABSTRACT Configuration settings are essential for tailoring software behavior to meet specific performance requirements. However, incorrect configurations are widespread, and identifying … | 0.72 | “Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents Zehao Wang, Dong Jae Kim, Tse-Husn (Peter) Chen Software PErformance, Analysis and Reliability (SPEAR) Lab Concordia University, Montreal, Canada w_zeha@encs.concordia.ca, k_dongja@encs.concordia.ca, peterc@encs.c” (Identifying performance-sensitive configurations in software systems through code analysis with LLM.txt @段1) || 研究方法 | 1 INTRODUCTION Modern software systems feature numerous configuration options, enabling customization for diverse workloads and hardware plat- forms [2, 43]. While these configurations provide flexibility, some configurations, known as performance-sensitive configurations, can impact system performance when their values change. De- velopers need to identify and understand the impact of such con- figurations to ensure they are set correctly, maintaining system performance and behavior. However, due to the large vol… | 0.82 | “Inspired by multi-agent, we introduce PerfSense, a lightweight framework designed to effectively classify performance-sensitive configurations using Large Language Models (LLMs) as multi-agent systems.” (Identifying performance-sensitive configurations in software systems through code analysis with LLM.txt @段5) || 主要结论 | ABSTRACT Configuration settings are essential for tailoring software behavior to meet specific performance requirements. However, incorrect configurations are widespread, and identifying those that impact system performance is challenging due to the vast number and complexity of possible settings. In this work, we present PerfSense, a lightweight framework that leverages Large Language Models (LLMs) to efficiently identify performance-sensitive configurations with minimal overhead. PerfSense employs LLM agents to … | 0.80 | “Our evaluation of seven open-source Java systems demonstrates that PerfSense achieves an average accuracy of 64.77% in classifying performance-sensitive configurations, outperforming both our LLM baseline (50.36%) and the previous state-of-the-art method (61.75%).” (Identifying performance-sensitive configurations in software systems through code analysis with LLM.txt @段2) |

- 主题标签：N/A
- 方法标签：基准/Benchmark
- 源文件：`Identifying performance-sensitive configurations in software systems through code analysis with LLM.txt`

</details>
<details><summary>49. Intuitive Smart Contract Auditing with Justifications（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Intuitive Smart Contract Auditing with Justifications | 0.92 | “Intuitive Smart Contract Auditing with Justifications” (Combining fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications.txt @段2) || 作者 | Combining Fine-tuning and LLM-based Agents for | 0.90 | “Combining Fine-tuning and LLM-based Agents for” (Combining fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications.txt @段1) || 发表年份 | 2024 | 0.86 | “losses of around $7.69 billion as of March 2024. Hence, there” (Combining fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2403.16073v3  [cs.SE]  14 Sep 2024” (Combining fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications.txt @段1) || 关键词 | code, fine, tuning, label, iaudit, name, prompt, vulnerability, detector, reason | 0.78 | “Combining Fine-tuning and LLM-based Agents for Intuitive Smart Contract Auditing with Justifications Wei Ma1, Daoyuan Wu 2∗ , Yuqiang Sun 1, Tianwen Wang 3, Shangqing Liu 1, Jian Zhang 1, Yue Xue 4, Yang Liu 1” (Combining fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications.txt @段1) || 摘要 | Combining Fine-tuning and LLM-based Agents for Intuitive Smart Contract Auditing with Justifications Wei Ma1, Daoyuan Wu 2∗ , Yuqiang Sun 1, Tianwen Wang 3, Shangqing Liu 1, Jian Zhang 1, Yue Xue 4, Yang Liu 1 1 Nanyang Technological University, Singapore, Singapore 2 The Hong Kong University of Science and Technology, Hong Kong SAR, China 3 National University of Singapore, Singapore, Singapore 4 MetaTrust Labs, Singapore, Singapore ma_wei@ntu.edu.sg, daoyuan@cse.ust.hk, suny0056@e.ntu.edu.sg, tianwenw.vk@gmail.c… | 0.72 | “Combining Fine-tuning and LLM-based Agents for Intuitive Smart Contract Auditing with Justifications Wei Ma1, Daoyuan Wu 2∗ , Yuqiang Sun 1, Tianwen Wang 3, Shangqing Liu 1, Jian Zhang 1, Yue Xue 4, Yang Liu 1” (Combining fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications.txt @段1) || 研究方法 | 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Ratio Withcall WOutCall 0.65 0.35 Fig. 9: Final Reason Distribution of Ranker-Critic. pieces of information are not closely related to the problem the model is trying to solve, they may not help enhance the model’s performance. Our research indicates that merely adding function call information does not directly facilitate the model’s effectiveness in detecting vulnerabilities. In the field of vulnerability detection, exploring how to construct effective contextual information remains a… | 0.82 | “C ONCLUSION In this paper, we proposed iAudit, the first smart contract auditing framework that combines fine-tuning and LLM-based agents to detect vulnerabilities and explain the results.” (Combining fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications.txt @段7) || 主要结论 | 4 MetaTrust Labs, Singapore, Singapore ma_wei@ntu.edu.sg, daoyuan@cse.ust.hk, suny0056@e.ntu.edu.sg, tianwenw.vk@gmail.com, liu.shangqing@ntu.edu.sg, jian_zhang@ntu.edu.sg, xueyue@metatrust.io, yangliu@ntu.edu.sg Abstract—Smart contracts are decentralized applications built atop blockchains like Ethereum. Recent research has shown that large language models (LLMs) have potential in auditing smart contracts, but the state-of-the-art indicates that even GPT-4 can achieve only 30% precision (when both decision and ju… | 0.80 | “Our experimental results show that iAu- dit achieved an F1 score of 91.21%, significantly outperform- ing prompt learning-based LLMs (which are in the range of 60%+) and also notably beating other fine-tuned models (which are in the range of 80%+) that used the same training data as ours.” (Combining fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications.txt @段4) |

- 主题标签：Web/Browser Agent 安全
- 方法标签：N/A
- 源文件：`Combining fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications.txt`

</details>
<details><summary>50. Jailbreak Attacks（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Jailbreak Attacks | 0.92 | “Jailbreak Attacks” (AutoDefense Multi-agent LLM defense against jailbreak attacks.txt @段2) || 作者 | Yifan Zeng1,*, Yiran Wu2,*, Xiao Zhang3, Huazheng Wang1, Qingyun Wu2 | 0.90 | “Yifan Zeng1,*, Yiran Wu2,*, Xiao Zhang3, Huazheng Wang1, Qingyun Wu2” (AutoDefense Multi-agent LLM defense against jailbreak attacks.txt @段1) || 发表年份 | 2024 | 0.86 | “arXiv:2403.04783v2  [cs.LG]  14 Nov 2024” (AutoDefense Multi-agent LLM defense against jailbreak attacks.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2403.04783v2  [cs.LG]  14 Nov 2024” (AutoDefense Multi-agent LLM defense against jailbreak attacks.txt @段1) || 关键词 | response, jailbreak, autodefense, prompt, multi, harmful, agency, framework, user, task | 0.78 | “AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks Yifan Zeng1,*, Yiran Wu2,*, Xiao Zhang3, Huazheng Wang1, Qingyun Wu2 1Oregon State University, 2Pennsylvania State University 3CISPA Helmholtz Center for Information Security {zengyif, huazheng.wang}@oregonstate.edu {yiran.wu, qingyun.wu}@psu.edu xiao.zhang” (AutoDefense Multi-agent LLM defense against jailbreak attacks.txt @段1) || 摘要 | AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks Yifan Zeng1,*, Yiran Wu2,*, Xiao Zhang3, Huazheng Wang1, Qingyun Wu2 1Oregon State University, 2Pennsylvania State University 3CISPA Helmholtz Center for Information Security {zengyif, huazheng.wang}@oregonstate.edu {yiran.wu, qingyun.wu}@psu.edu xiao.zhang@cispa.de Abstract Despite extensive pre-training in moral alignment to prevent generating harmful information, large language models (LLMs) remain vulnerable to jailbreak attacks. In this paper, we … | 0.72 | “AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks Yifan Zeng1,*, Yiran Wu2,*, Xiao Zhang3, Huazheng Wang1, Qingyun Wu2 1Oregon State University, 2Pennsylvania State University 3CISPA Helmholtz Center for Information Security {zengyif, huazheng.wang}@oregonstate.edu {yiran.wu, qingyun.wu}@psu.edu xiao.zhang” (AutoDefense Multi-agent LLM defense against jailbreak attacks.txt @段1) || 研究方法 | 6 Conclusion In this work, we proposed AutoDefense, a multi-agent defense framework for mitigating LLM jailbreak attacks. Built upon a response-filtering mechanism, our defense employs multiple LLM agents, each tasked with specialized roles to analyze harmful responses collaboratively. We found the CoT instruction heavily depends on LLMs’ ability to follow instructions, and we are targeting efficient LLMs with weaker instruction-following abilities. To address this issue, we found the multi-agent approach is a nat… | 0.82 | “6 Conclusion In this work, we proposed AutoDefense, a multi-agent defense framework for mitigating LLM jailbreak attacks.” (AutoDefense Multi-agent LLM defense against jailbreak attacks.txt @段14) || 主要结论 | 5.1 Main Results Comparisons with other defenses. We compare different methods for defending GPT-3.5 as shown in Table 1. We use LLaMA-2-13B as the defense LLM in AutoDefense. We find our AutoDefense outperforms other methods in terms of ASR. The compared methods in Table 1 includes: (1) System- Mode Self-Reminder [ 49] is a prompt-based method, it only needs a victim LLM to finish the defense. This kind of defense method might interfere with response generation, which potentially impacts the response quality for … | 0.80 | “We find our AutoDefense outperforms other methods in terms of ASR.” (AutoDefense Multi-agent LLM defense against jailbreak attacks.txt @段12) |

- 主题标签：Jailbreak/越狱攻击, 多智能体对抗与协作, 防御与对齐机制
- 方法标签：多智能体/Multi-Agent, 多模态/Multimodal, 系统防御/Defense, 越狱/Jailbreak
- 源文件：`综述参考文献\AutoDefense Multi-agent LLM defense against jailbreak attacks.txt`

</details>
<details><summary>51. LLM Agents Can Be Choice-Supportive Biased Evaluators: An Empirical Study（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | LLM Agents Can Be Choice-Supportive Biased Evaluators: An Empirical Study | 0.92 | “LLM Agents Can Be Choice-Supportive Biased Evaluators: An Empirical Study” (LLM agents can be choice-supportive biased evaluators An empirical study.txt @段1) || 作者 | Nan Zhuang1*, Boyu Cao 2*, Yi Yang2*, Jing Xu 3*, Mingda Xu 2*, Yuxiao Wang2, Qi Liu †2 | 0.90 | “Nan Zhuang1*, Boyu Cao 2*, Yi Yang2*, Jing Xu 3*, Mingda Xu 2*, Yuxiao Wang2, Qi Liu †2” (LLM agents can be choice-supportive biased evaluators An empirical study.txt @段1) || 发表年份 | 2024 | 0.86 | “from iterative medical diagnosis support (Kaur et al. 2024;” (LLM agents can be choice-supportive biased evaluators An empirical study.txt @段1) || 期刊/会议 | The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25) | 0.88 | “The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)” (LLM agents can be choice-supportive biased evaluators An empirical study.txt @段1) || 关键词 | bias, choice, supportive, arxiv, 2024, human, chosen, decision, features, options | 0.78 | “LLM Agents Can Be Choice-Supportive Biased Evaluators: An Empirical Study Nan Zhuang1*, Boyu Cao 2*, Yi Yang2*, Jing Xu 3*, Mingda Xu 2*, Yuxiao Wang2, Qi Liu †2 1School of Software Technology, Zhejiang University, Hangzhou, China 2School of Future Technology, South China University of Technology, Guangzhou, China 3Fac” (LLM agents can be choice-supportive biased evaluators An empirical study.txt @段1) || 摘要 | LLM Agents Can Be Choice-Supportive Biased Evaluators: An Empirical Study Nan Zhuang1*, Boyu Cao 2*, Yi Yang2*, Jing Xu 3*, Mingda Xu 2*, Yuxiao Wang2, Qi Liu †2 1School of Software Technology, Zhejiang University, Hangzhou, China 2School of Future Technology, South China University of Technology, Guangzhou, China 3Faculty of Humanities and Arts, Macau University of Science and Technology, Macau Abstract With Large Language Model (LLM) agents taking on more evaluation responsibilities in decision-making, it is ess… | 0.72 | “LLM Agents Can Be Choice-Supportive Biased Evaluators: An Empirical Study Nan Zhuang1*, Boyu Cao 2*, Yi Yang2*, Jing Xu 3*, Mingda Xu 2*, Yuxiao Wang2, Qi Liu †2 1School of Software Technology, Zhejiang University, Hangzhou, China 2School of Future Technology, South China University of Technology, Guangzhou, China 3Fac” (LLM agents can be choice-supportive biased evaluators An empirical study.txt @段1) || 研究方法 | N/A | 0.50 | N/A || 主要结论 | Introduction As Large Language Model (LLM) Agents increasingly as- sume critical roles in complex decision-making processes, from iterative medical diagnosis support (Kaur et al. 2024; *These authors contributed equally. All equal-contributing au- thors worked jointly from the first day until the last day, sharing equal workload and contributions. N. Zhuang, B. Cao, M. Xu were responsible for designing and conducting the experiments, Y . Yang was responsible for coding, J. Xu performed the human study, Y . Wang an… | 0.80 | “Lastly, we conduct this ex- periment with a large group of well-educated human partic- ipants (n = 284) to compare, and the results show that some agents can outperform humans as evaluators in our experi- mental settings.” (LLM agents can be choice-supportive biased evaluators An empirical study.txt @段3) |

- 主题标签：隐私与记忆风险
- 方法标签：基准/Benchmark
- 源文件：`LLM agents can be choice-supportive biased evaluators An empirical study.txt`

</details>
<details><summary>52. LLM Agents are Susceptible to Environmental Distractions（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | LLM Agents are Susceptible to Environmental Distractions | 0.92 | “LLM Agents are Susceptible to Environmental Distractions” (Caution for the environment Multimodal LLM agents are susceptible to environmental distractions.txt @段4) || 作者 | July 27 - August 1, 2025 ©2025 Association for Computational Linguistics | 0.90 | “July 27 - August 1, 2025 ©2025 Association for Computational Linguistics” (Caution for the environment Multimodal LLM agents are susceptible to environmental distractions.txt @段1) || 发表年份 | 2024 | 0.86 | “et al., 2024), agents have demonstrated significant” (Caution for the environment Multimodal LLM agents are susceptible to environmental distractions.txt @段1) || 期刊/会议 | Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 22324–22339 | 0.88 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 22324–22339” (Caution for the environment Multimodal LLM agents are susceptible to environmental distractions.txt @段1) || 关键词 | distractions, environment, 2024, gui, 2023, multimodal, user, click, faithfulness, perception | 0.78 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 22324–22339 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Caution for the Environment: Multimodal LLM Agents are Susceptible to Environmental Distractions Xinbei Ma1,2,3, Y” (Caution for the environment Multimodal LLM agents are susceptible to environmental distractions.txt @段1) || 摘要 | Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 22324–22339 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Caution for the Environment: Multimodal LLM Agents are Susceptible to Environmental Distractions Xinbei Ma1,2,3, Yiting Wang1, Yao Yao1,2,3, Tongxin Yuan1, Aston Zhang4, Zhuosheng Zhang1,∗, Hai Zhao1,2,3* 1School of Computer Science, 2Key Laboratory of Shanghai Education Commission for Intelligent Interaction an… | 0.72 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 22324–22339 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Caution for the Environment: Multimodal LLM Agents are Susceptible to Environmental Distractions Xinbei Ma1,2,3, Y” (Caution for the environment Multimodal LLM agents are susceptible to environmental distractions.txt @段1) || 研究方法 | 3.2 Distraction Simulation Following the problem statement, we construct a simulated dataset, D. Each sample is a triplet (g,s, A) consisting of a goalg, a screenshot image as environment states, and a valid action space A. Since existing datasets cannot be used directly, our core idea is to make a realistic screenshot suitable for our task with minimal modification by inserting a realistic distraction. Specifically, the simulation of distraction is carefully decomposed into detailed steps, resulting in a composit… | 0.82 | “Algorithm 1 presents the unified pipeline of data construction, followed by the descriptions of four subsets, each for a common scenario, namely Pop-up box, Search, Recommendation, and Chat.” (Caution for the environment Multimodal LLM agents are susceptible to environmental distractions.txt @段9) || 主要结论 | 3.3 Measurement The measurement of the predicted action ˆa is defined separately for two kinds of agents in Eq. 5. (i) Generalist MLLMs (e.g., GPT-4o) predict the operations on GUIs with natural language by describing screen elements as operating targets, like the “Submit button”. It is measured by token- levelF1 and matched with one annotated action if F1 surpasses a threshold, τtxt. (ii) Specialist agents (e.g., CogAgent) are trained to generate operating locations using precise coordinates of the screen. The pr… | 0.80 | “Next, based on the action labels, accuracy for gold actions, distracted actions, or invalid actions are computed respectively, whereAccgold reflects the faithfulness and helpfulness of agents; Accdist shows the unfaithfulness, i.e., how often agents are distracted from their goals; Accinv indicates how often agents fai” (Caution for the environment Multimodal LLM agents are susceptible to environmental distractions.txt @段10) |

- 主题标签：GUI/Computer-Use Agent 安全, Prompt Injection/环境注入, Web/Browser Agent 安全
- 方法标签：仿真/Simulator, 基准/Benchmark, 多模态/Multimodal, 调查/Survey
- 源文件：`Caution for the environment Multimodal LLM agents are susceptible to environmental distractions.txt`

</details>
<details><summary>53. LLM-based Multi-agent Systems（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | LLM-based Multi-agent Systems | 0.92 | “LLM-based Multi-agent Systems” (G-safeguard A topology-guided security lens and treatment on LLM-based multi-agent systems.txt @段4) || 作者 | July 27 - August 1, 2025 ©2025 Association for Computational Linguistics | 0.90 | “July 27 - August 1, 2025 ©2025 Association for Computational Linguistics” (G-safeguard A topology-guided security lens and treatment on LLM-based multi-agent systems.txt @段1) || 发表年份 | 2024 | 0.86 | “Autonomous agents (Wang et al., 2024), while in-” (G-safeguard A topology-guided security lens and treatment on LLM-based multi-agent systems.txt @段1) || 期刊/会议 | Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 7261–7276 | 0.88 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 7261–7276” (G-safeguard A topology-guided security lens and treatment on LLM-based multi-agent systems.txt @段1) || 关键词 | mas, multi, safeguard, graph, 2024, detection, round, utterance, prompt, which | 0.78 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 7261–7276 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics G-Safeguard: A Topology-Guided Security Lens and Treatment on LLM-based Multi-agent Systems Shilong Wang⋆† Guibin Zh” (G-safeguard A topology-guided security lens and treatment on LLM-based multi-agent systems.txt @段1) || 摘要 | Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 7261–7276 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics G-Safeguard: A Topology-Guided Security Lens and Treatment on LLM-based Multi-agent Systems Shilong Wang⋆† Guibin Zhang♠† Miao Yu⋆ Guancheng Wan♣ Fanci Meng⋆ Chongye Guo♦ Kun Wang✠ Yang Wang⋆ ⋆USTC ♠NUS ♣UCLA ♦Shanghai University ✠NTU wk520529@mail.ustc.edu.cn angyan@ustc.edu.cn Abstract Large Language Model (LLM)… | 0.72 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 7261–7276 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics G-Safeguard: A Topology-Guided Security Lens and Treatment on LLM-based Multi-agent Systems Shilong Wang⋆† Guibin Zh” (G-safeguard A topology-guided security lens and treatment on LLM-based multi-agent systems.txt @段1) || 研究方法 | 1 Introduction Autonomous agents (Wang et al., 2024), while in- heriting the general task-solving and instruction comprehension capabilities of Large Language Models (LLMs) (Chang et al., 2024; Minaee et al., 2024), are equipped with external units such as tools (Liu et al., 2024b; Tang et al., 2023) and mem- ory (Zhang et al., 2024h), extending the capability boundaries of LLMs. Multi-agent systems (MAS) further integrate collective intelligence through agent interactions, enhancing the capabilities of individual… | 0.82 | “We introduce G-Safeguard, a topology-guided framework for attack detec- tion and remediation, enabling lightweight and real-time identification of adversarial entities on multi-agent utterance graphs and contamination- free communication via graph pruning.” (G-safeguard A topology-guided security lens and treatment on LLM-based multi-agent systems.txt @段3) || 主要结论 | 80 0.31/0.31 5.29/ 1.25 17.50/2.50 22.81/2.19 Table 2: ASR each round on MAS with different num- bers of agents. GS stands for G-Safeguard. Obs 3. G-Safeguard can be directly trans- ferred to larger-scale MAS without the need for retraining. As shown in Table 2 and fig. 5, G-Safeguard-guided MAS demonstrates better robustness, which indicates that G-Safeguard trained on data generated from small-scale MAS does not suffer performance degradation when ap- plied to larger MAS with more agents and differ- ent topologi… | 0.80 | “5, G-Safeguard-guided MAS demonstrates better robustness, which indicates that G-Safeguard trained on data generated from small-scale MAS does not suffer performance degradation when ap- plied to larger MAS with more agents and differ- ent topologies.” (G-safeguard A topology-guided security lens and treatment on LLM-based multi-agent systems.txt @段18) |

- 主题标签：Backdoor/投毒, GUI/Computer-Use Agent 安全, 多智能体对抗与协作, 防御与对齐机制, 隐私与记忆风险
- 方法标签：仿真/Simulator, 后门/Backdoor, 基准/Benchmark, 多智能体/Multi-Agent, 多模态/Multimodal, 系统防御/Defense
- 源文件：`G-safeguard A topology-guided security lens and treatment on LLM-based multi-agent systems.txt`

</details>
<details><summary>54. MLLM-Powered Mobile GUI Agents（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | MLLM-Powered Mobile GUI Agents | 0.92 | “MLLM-Powered Mobile GUI Agents” (Hidden ghost hand Unveiling backdoor vulnerabilities in MLLM-powered mobile GUI agents.txt @段2) || 作者 | Pengzhou Cheng, Haowen Hu, Zheng Wu, Zongru Wu, Tianjie Ju | 0.90 | “Pengzhou Cheng, Haowen Hu, Zheng Wu, Zongru Wu, Tianjie Ju” (Hidden ghost hand Unveiling backdoor vulnerabilities in MLLM-powered mobile GUI agents.txt @段1) || 发表年份 | 2024 | 0.86 | “teractions (Zhang et al., 2024a; Wang et al., 2024c).” (Hidden ghost hand Unveiling backdoor vulnerabilities in MLLM-powered mobile GUI agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2505.14418v2  [cs.CL]  23 May 2025” (Hidden ghost hand Unveiling backdoor vulnerabilities in MLLM-powered mobile GUI agents.txt @段1) || 关键词 | gui, backdoor, 2025, zhang, 2024, agentghost, task, wang, goal, powered | 0.78 | “Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents W ARNING: THIS PAPER CONTAINS UNSAFE MODEL RESPONSES Pengzhou Cheng, Haowen Hu, Zheng Wu, Zongru Wu, Tianjie Ju Zhuosheng Zhang ∗ , Gongshen Liu * School of Computer Science, Shanghai Jiao Tong University {cpztsm520, dreamer777, wzh” (Hidden ghost hand Unveiling backdoor vulnerabilities in MLLM-powered mobile GUI agents.txt @段1) || 摘要 | Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents W ARNING: THIS PAPER CONTAINS UNSAFE MODEL RESPONSES Pengzhou Cheng, Haowen Hu, Zheng Wu, Zongru Wu, Tianjie Ju Zhuosheng Zhang ∗ , Gongshen Liu * School of Computer Science, Shanghai Jiao Tong University {cpztsm520, dreamer777, wzh815918208, wuzongru, jometeorie}@sjtu.edu.cn {zhangzs,lgshen}@sjtu.edu.cn Abstract Graphical user interface (GUI) agents pow- ered by multimodal large language models (MLLMs) have shown greater promi… | 0.72 | “Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents W ARNING: THIS PAPER CONTAINS UNSAFE MODEL RESPONSES Pengzhou Cheng, Haowen Hu, Zheng Wu, Zongru Wu, Tianjie Ju Zhuosheng Zhang ∗ , Gongshen Liu * School of Computer Science, Shanghai Jiao Tong University {cpztsm520, dreamer777, wzh” (Hidden ghost hand Unveiling backdoor vulnerabilities in MLLM-powered mobile GUI agents.txt @段1) || 研究方法 | Abstract Graphical user interface (GUI) agents pow- ered by multimodal large language models (MLLMs) have shown greater promise for human-interaction. However, due to the high fine-tuning cost, users often rely on open- source GUI agents or APIs offered by AI providers, which introduces a critical but un- derexplored supply chain threat: backdoor at- tacks. In this work, we first unveil that MLLM- powered GUI agents naturally expose multi- ple interaction-level triggers, such as historical steps, environment state… | 0.82 | “Then, we formulate backdoor injection as a Min-Max optimization problem that uses supervised contrastive learn- ing to maximize the feature difference across sample classes at the representation space, im- proving flexibility of the backdoor.” (Hidden ghost hand Unveiling backdoor vulnerabilities in MLLM-powered mobile GUI agents.txt @段2) || 主要结论 | 100 clean samples, and then perform clean-tuning. Self-reflection. We implement an action-aware self-reflection mechanism based on DPO, enabling the GUI Agent to identify incorrect actions, thereby mitigating malicious action injection introduced by AgentGhost. Specifically, we first select 20% of clean samples to construct positive-negative sample pairs, denoted as (x, Ac, Ar), where x is the model input and Ar is the incorrect action randomly sam- pled by Dc. Subsequently, we optimize the model using the DPO los… | 0.80 | “For example, on AndroidControl (Low-level), it achieves a TMR of 95.2% and an AMR of 67.0%, closely approximating the clean model’s 96.4% and 68.7%, and significantly outperforming AddSent (74.3%, 51.6%) and SynAttack (73.0%, 51.5%).” (Hidden ghost hand Unveiling backdoor vulnerabilities in MLLM-powered mobile GUI agents.txt @段28) |

- 主题标签：Backdoor/投毒, GUI/Computer-Use Agent 安全, Prompt Injection/环境注入, Web/Browser Agent 安全, 防御与对齐机制, 隐私与记忆风险
- 方法标签：仿真/Simulator, 后门/Backdoor, 多模态/Multimodal, 形式化/Optimization, 提示注入/Prompt Injection, 系统防御/Defense, 红队/Red-Teaming
- 源文件：`综述参考文献\Hidden ghost hand Unveiling backdoor vulnerabilities in MLLM-powered mobile GUI agents.txt`

</details>
<details><summary>55. Mandarin apps. This paper introduces MobileFlow, a multimodal large language model meticulously（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Mandarin apps. This paper introduces MobileFlow, a multimodal large language model meticulously | 0.92 | “Mandarin apps. This paper introduces MobileFlow, a multimodal large language model meticulously” (MobileFlow A multimodal LLM for mobile GUI agent.txt @段14) || 作者 | Songqin Nong*, Jiali Zhu*, Rui Wu*, Jiongchao Jin, Shuo Shan, Xiutian Huang, Wenhao Xu | 0.90 | “Songqin Nong*, Jiali Zhu*, Rui Wu*, Jiongchao Jin, Shuo Shan, Xiutian Huang, Wenhao Xu” (MobileFlow A multimodal LLM for mobile GUI agent.txt @段1) || 发表年份 | 2024 | 0.86 | “has also led to substantial breakthroughs in Multimodal Large Language Models (MLLMs) (Chen et al. [2024], Liu” (MobileFlow A multimodal LLM for mobile GUI agent.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2407.04346v3  [cs.CV]  6 Dec 2024” (MobileFlow A multimodal LLM for mobile GUI agent.txt @段1) || 关键词 | visual, gui, image, mobileflow, encoder, training, features, vlms, 2023, information | 0.78 | “MobileFlow: A Multimodal LLM for Mobile GUI Agent Songqin Nong*, Jiali Zhu*, Rui Wu*, Jiongchao Jin, Shuo Shan, Xiutian Huang, Wenhao Xu Ant Group {nongsongqin.nsq, zhujiali.zjl, guli.wr, jinjiongchao.jjc}@antgroup.com {shanshuo.ss, huangxiutian.hxt, hao.xuwh}@antgroup.com” (MobileFlow A multimodal LLM for mobile GUI agent.txt @段1) || 摘要 | MobileFlow: A Multimodal LLM for Mobile GUI Agent Songqin Nong*, Jiali Zhu*, Rui Wu*, Jiongchao Jin, Shuo Shan, Xiutian Huang, Wenhao Xu Ant Group {nongsongqin.nsq, zhujiali.zjl, guli.wr, jinjiongchao.jjc}@antgroup.com {shanshuo.ss, huangxiutian.hxt, hao.xuwh}@antgroup.com ABSTRACT The ongoing evolution of multimodal large-scale models, such as GPT-4v, Qwen-VL-Max, has significantly bolstered the capabilities of image comprehension and user action analysis, showcasing the potentiality of intelligent graphically-or… | 0.72 | “MobileFlow: A Multimodal LLM for Mobile GUI Agent Songqin Nong*, Jiali Zhu*, Rui Wu*, Jiongchao Jin, Shuo Shan, Xiutian Huang, Wenhao Xu Ant Group {nongsongqin.nsq, zhujiali.zjl, guli.wr, jinjiongchao.jjc}@antgroup.com {shanshuo.ss, huangxiutian.hxt, hao.xuwh}@antgroup.com” (MobileFlow A multimodal LLM for mobile GUI agent.txt @段1) || 研究方法 | 4 Conclusion In this paper, we present MobileFlow, a GUI Agent that leverages a multimodal large model and incorporates a set of optimization techniques, to analyze UI images, understand user instructions and operate under the practical scenarios. Our proposed MobileFlow has been designed to navigate a diverse array of intricate scenarios and business domains with proficiency. It has demonstrated its capabilities in practical applications across various sectors, showcasing its versatility and effectiveness. And mo… | 0.82 | “4 Conclusion In this paper, we present MobileFlow, a GUI Agent that leverages a multimodal large model and incorporates a set of optimization techniques, to analyze UI images, understand user instructions and operate under the practical scenarios.” (MobileFlow A multimodal LLM for mobile GUI agent.txt @段12) || 主要结论 | 3.2 Quantitative Results To thoroughly assess the capabilities of our newly proposed method, we conducted a comparative analysis of MobileFlow against other leading Large Language Model (LLM)-based terminal agent algorithms, including MobileAgent and GPT-4v, in the context of mobile application strategy generation across various business domains. Table 1: Quantitative Results of MobileFlow in 6 business areas and 3 complexity of tasks Complexity Metrics Food Delivery Food Walkin Medical Service Fund Select Insuran… | 0.80 | “This indicates that even with a smaller model, MobileFlow can improve after fine-tuning and can match or outperform larger models, proving the method’s efficiency and strength.” (MobileFlow A multimodal LLM for mobile GUI agent.txt @段10) |

- 主题标签：GUI/Computer-Use Agent 安全, Web/Browser Agent 安全, 隐私与记忆风险
- 方法标签：多模态/Multimodal, 形式化/Optimization
- 源文件：`综述参考文献\MobileFlow A multimodal LLM for mobile GUI agent.txt`

</details>
<details><summary>56. Multimodal Web Agents（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Multimodal Web Agents | 0.92 | “Multimodal Web Agents” (Explorer Scaling exploration-driven web trajectory synthesis for multimodal web agents.txt @段3) || 作者 | Vardaan Pahuja1∗†, Yadong Lu 2*¶, Corby Rosset 2, Boyu Gou 1, | 0.90 | “Vardaan Pahuja1∗†, Yadong Lu 2*¶, Corby Rosset 2, Boyu Gou 1,” (Explorer Scaling exploration-driven web trajectory synthesis for multimodal web agents.txt @段1) || 发表年份 | 2024 | 0.86 | “based agents (Su et al., 2024) have shown great” (Explorer Scaling exploration-driven web trajectory synthesis for multimodal web agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2502.11357v4  [cs.AI]  30 May 2025” (Explorer Scaling exploration-driven web trajectory synthesis for multimodal web agents.txt @段1) || 关键词 | web, task, 2024, trajectory, data, diverse, work, 2023, amazon, diversity | 0.78 | “arXiv:2502.11357v4 [cs.AI] 30 May 2025 Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents Vardaan Pahuja1∗†, Yadong Lu 2*¶, Corby Rosset 2, Boyu Gou 1, Arindam Mitra2, Spencer Whitehead 2, Yu Su 1, Ahmed Awadallah 2 1The Ohio State University 2Microsoft Research, Redmond pahuja.9@os” (Explorer Scaling exploration-driven web trajectory synthesis for multimodal web agents.txt @段1) || 摘要 | arXiv:2502.11357v4 [cs.AI] 30 May 2025 Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents Vardaan Pahuja1∗†, Yadong Lu 2*¶, Corby Rosset 2, Boyu Gou 1, Arindam Mitra2, Spencer Whitehead 2, Yu Su 1, Ahmed Awadallah 2 1The Ohio State University 2Microsoft Research, Redmond pahuja.9@osu.edu, yadonglu@microsoft.com Abstract Recent success in large multimodal models (LMMs) has sparked promising applications of agents capable of autonomously complet- ing complex web tasks. While open… | 0.72 | “arXiv:2502.11357v4 [cs.AI] 30 May 2025 Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents Vardaan Pahuja1∗†, Yadong Lu 2*¶, Corby Rosset 2, Boyu Gou 1, Arindam Mitra2, Spencer Whitehead 2, Yu Su 1, Ahmed Awadallah 2 1The Ohio State University 2Microsoft Research, Redmond pahuja.9@os” (Explorer Scaling exploration-driven web trajectory synthesis for multimodal web agents.txt @段1) || 研究方法 | 2 Related Work Recent advances in multimodal language models have facilitated the development of web agents — autonomous systems designed to interact with real- world websites to perform everyday tasks (Deng et al., 2023; Hong et al., 2024; Cheng et al., 2024; Zheng et al., 2024, 2025; Xue et al., 2025). Early efforts to acquire trajectory data for training web agents primarily relied on crowd-sourcing (Deng et al., 2023; Lu et al., 2024). However, due to the high cost of human annotation, recent work has adopted … | 0.82 | “To address diversity limitations in prior tra- jectory synthesis work, we design a bottom-up web trajectory synthesis pipeline that explores websites dynamically while maintaining a coherent high- level task intent.” (Explorer Scaling exploration-driven web trajectory synthesis for multimodal web agents.txt @段4) || 主要结论 | 1 Introduction Graphical User Interfaces (GUIs) serve as the pri- mary medium for user interaction across digital environments. Within the GUI environment, LLM- based agents (Su et al., 2024) have shown great potential in automating complex workflows for hu- man users. These agents are designed to operate *Equal Contribution. † Work partly done during internship at Microsoft Research. ¶ Project Lead. 1Project website: https://osu-nlp-group.github.io/ Explorer/ across diverse interfaces, including the web (Deng et … | 0.80 | “• We demonstrate the effectiveness of our dataset by training small language models, which achieve strong performance on both online and offline benchmarks, significantly surpassing existing web agent baselines, in- cluding those with larger parameter counts.” (Explorer Scaling exploration-driven web trajectory synthesis for multimodal web agents.txt @段3) |

- 主题标签：GUI/Computer-Use Agent 安全, Web/Browser Agent 安全, 隐私与记忆风险
- 方法标签：仿真/Simulator, 基准/Benchmark, 多模态/Multimodal
- 源文件：`综述参考文献\Explorer Scaling exploration-driven web trajectory synthesis for multimodal web agents.txt`

</details>
<details><summary>57. On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents | 0.92 | “On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents” (On the resilience of LLM-based multi-agent collaboration with faulty agents.txt @段2) || 作者 | due to the collaboration of expert agents, each | 0.90 | “due to the collaboration of expert agents, each” (On the resilience of LLM-based multi-agent collaboration with faulty agents.txt @段1) || 发表年份 | 2024 | 0.86 | “et al., 2023; Lee et al., 2024), math problem solving (Lu” (On the resilience of LLM-based multi-agent collaboration with faulty agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2408.00989v4  [cs.AI]  28 May 2025” (On the resilience of LLM-based multi-agent collaboration with faulty agents.txt @段1) || 关键词 | multi, errors, task, code, faulty, systems, resilience, collaboration, wang, 2024 | 0.78 | “arXiv:2408.00989v4 [cs.AI] 28 May 2025 On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents Jen-tse Huang 1 Jiaxu Zhou 1 Tailin Jin 2 Xuhui Zhou 3 Zixi Chen 4 Wenxuan Wang5 Y ouliang Yuan6 Michael R. Lyu 1 Maarten Sap 3” (On the resilience of LLM-based multi-agent collaboration with faulty agents.txt @段1) || 摘要 | arXiv:2408.00989v4 [cs.AI] 28 May 2025 On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents Jen-tse Huang 1 Jiaxu Zhou 1 Tailin Jin 2 Xuhui Zhou 3 Zixi Chen 4 Wenxuan Wang5 Y ouliang Yuan6 Michael R. Lyu 1 Maarten Sap 3 Abstract Large language model-based multi-agent systems have shown great abilities across various tasks due to the collaboration of expert agents, each focusing on a specific domain. However, the im- pact of clumsy or even malicious agents—those who frequently make errors in … | 0.72 | “arXiv:2408.00989v4 [cs.AI] 28 May 2025 On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents Jen-tse Huang 1 Jiaxu Zhou 1 Tailin Jin 2 Xuhui Zhou 3 Zixi Chen 4 Wenxuan Wang5 Y ouliang Yuan6 Michael R. Lyu 1 Maarten Sap 3” (On the resilience of LLM-based multi-agent collaboration with faulty agents.txt @段1) || 研究方法 | Abstract Large language model-based multi-agent systems have shown great abilities across various tasks due to the collaboration of expert agents, each focusing on a specific domain. However, the im- pact of clumsy or even malicious agents—those who frequently make errors in their tasks—on the overall performance of the system remains under- explored. This paper investigates: (1) What is the resilience of various system structures ( e.g., A→B→C, A↔B↔C) under faulty agents, on dif- ferent downstream tasks? (2) How … | 0.82 | “To further improve resilience, we introduce (1) Challenger, that introduces a mech- anism for each agent to challenge others’ out- puts, and (2) Inspector, an additional agent to review and correct messages, recovering up to 96.4% errors made by faulty agents.” (On the resilience of LLM-based multi-agent collaboration with faulty agents.txt @段2) || 主要结论 | 80 human-annotated “win/tie/lose” labels comparing re- sponses from ChatGPT and Vicuna-13B, aiming to deter- mine if the model’s preferences align with human judg- ments. Accuracy is used for evaluation. Multi-Agent Systems We use six multi-agent systems for the three types of structures mentioned in §2: 4 On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents GPT-3.5 Structure 20 30 40 50 60 70 Linear Flat Hierarchical Vanilla AutoTransform AutoInject (a) Backbone LLM: GPT-3.5. GPT-4o Structu… | 0.80 | “This method addresses the limitation that many agents can only execute assigned tasks and may not address (a) Self-collab w/o Improve Challenger Inspector C+I w/o Errors 76.2 74.6 76.4 76.8 AUTOTRANSFORM43.3 70.7 74.4 75.0 AUTOINJECT 40.9 72.0 67.7 73.8 (b) Camel w/o Improve Challenger Inspector C+I w/o Errors 62.2 62.” (On the resilience of LLM-based multi-agent collaboration with faulty agents.txt @段3) |

- 主题标签：Web/Browser Agent 安全, 多智能体对抗与协作
- 方法标签：基准/Benchmark, 多智能体/Multi-Agent
- 源文件：`综述参考文献\On the resilience of LLM-based multi-agent collaboration with faulty agents.txt`

</details>
<details><summary>58. Poison Once, Control Anywhere: Clean-Text Visual Backdoors in（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Poison Once, Control Anywhere: Clean-Text Visual Backdoors in | 0.92 | “Poison Once, Control Anywhere: Clean-Text Visual Backdoors in” (Poison once, control anywhere Clean-text visual backdoors in VLM-based mobile agents.txt @段1) || 作者 | Poison Once, Control Anywhere: Clean-Text Visual Backdoors in | 0.90 | “Poison Once, Control Anywhere: Clean-Text Visual Backdoors in” (Poison once, control anywhere Clean-text visual backdoors in VLM-based mobile agents.txt @段1) || 发表年份 | 2024 | 0.86 | “Zhou et al. 2024; Xie et al. 2024; Liu et al. 2024; Huang et al.” (Poison once, control anywhere Clean-text visual backdoors in VLM-based mobile agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2506.13205v6  [cs.CR]  5 Sep 2025” (Poison once, control anywhere Clean-text visual backdoors in VLM-based mobile agents.txt @段1) || 关键词 | mobile, fsr, clean, trigger, visual, asr, target, backdoor, arxiv, type | 0.78 | “Poison Once, Control Anywhere: Clean-Text Visual Backdoors in VLM-based Mobile Agents Xuan Wang1, Siyuan Liang2, Zhe Liu3, Yi Yu2, Aishan Liu4, Yuliang Lu1, Xitong Gao3, Ee-Chien Chang5 1National University of Defense Technology, China2Nanyang Technological University, Singapore 3Chinese Academy of Sciences, China 4Bei” (Poison once, control anywhere Clean-text visual backdoors in VLM-based mobile agents.txt @段1) || 摘要 | Poison Once, Control Anywhere: Clean-Text Visual Backdoors in VLM-based Mobile Agents Xuan Wang1, Siyuan Liang2, Zhe Liu3, Yi Yu2, Aishan Liu4, Yuliang Lu1, Xitong Gao3, Ee-Chien Chang5 1National University of Defense Technology, China2Nanyang Technological University, Singapore 3Chinese Academy of Sciences, China 4Beihang University, Beijing, China 5National University of Singapore, Singapore {wangxuan21d}@nudt.edu.cn, pandaliang521@gmail.com, liuzhe181@mails.ucas.edu.cn, yuyi0010@e.ntu.edu.sg, liuaishan@buaa.edu… | 0.72 | “Poison Once, Control Anywhere: Clean-Text Visual Backdoors in VLM-based Mobile Agents Xuan Wang1, Siyuan Liang2, Zhe Liu3, Yi Yu2, Aishan Liu4, Yuliang Lu1, Xitong Gao3, Ee-Chien Chang5 1National University of Defense Technology, China2Nanyang Technological University, Singapore 3Chinese Academy of Sciences, China 4Bei” (Poison once, control anywhere Clean-text visual backdoors in VLM-based mobile agents.txt @段1) || 研究方法 | Introduction Large language models (LLMs) enable autonomous agents that interpret instructions, reason through tasks, and interact with the operating system and online tools (Yao et al. 2022; Zhou et al. 2024; Xie et al. 2024; Liu et al. 2024; Huang et al. 2022).Mobile agents(Lee et al. 2024b; Zhang et al. 2025; Wang et al. 2025) operate within mobile apps like What- sApp and Amazon to access sensitive features such as cam- era, messaging, and GPS. These agents use vision-language models (VLMs) to process screensh… | 0.82 | “Ensure:Poisoned datasetD poison 1:Choose an attack type (Type I–IV) and trigger injection strategy (e.g., Hurdle, Hoverball, Blended) 2:Sample target instance(x,t,a target,c target) 3:Images with trigger:x target = (1−m)⊙x+m⊙τ 4:SamplePclean training samples{(x j,t j, yj)}P j=1 5:forr= 1toRdo 6:Initialize perturbations” (Poison once, control anywhere Clean-text visual backdoors in VLM-based mobile agents.txt @段3) || 主要结论 | Introduction Large language models (LLMs) enable autonomous agents that interpret instructions, reason through tasks, and interact with the operating system and online tools (Yao et al. 2022; Zhou et al. 2024; Xie et al. 2024; Liu et al. 2024; Huang et al. 2022).Mobile agents(Lee et al. 2024b; Zhang et al. 2025; Wang et al. 2025) operate within mobile apps like What- sApp and Amazon to access sensitive features such as cam- era, messaging, and GPS. These agents use vision-language models (VLMs) to process screensh… | 0.80 | “Our results reveal the first practical and stealthy backdoor threat against VLM-based mobile agents, high- lighting the need for robust defenses during model adapta- tion.Our contributions are as follows: • We introduceVIBMA, the first clean-text visual back- door attack against VLM-based mobile agents, capable of hija” (Poison once, control anywhere Clean-text visual backdoors in VLM-based mobile agents.txt @段3) |

- 主题标签：Backdoor/投毒, GUI/Computer-Use Agent 安全, Web/Browser Agent 安全, 防御与对齐机制, 隐私与记忆风险
- 方法标签：后门/Backdoor, 基准/Benchmark, 多模态/Multimodal, 系统防御/Defense
- 源文件：`综述参考文献\Poison once, control anywhere Clean-text visual backdoors in VLM-based mobile agents.txt`

</details>
<details><summary>59. Poisoning Memory or Knowledge Bases（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Poisoning Memory or Knowledge Bases | 0.92 | “Poisoning Memory or Knowledge Bases” (AGENTPOISON Red-teaming LLM agents via poisoning memory or knowledge bases.txt @段2) || 作者 | Zhaorun Chen1∗, Zhen Xiang2, Chaowei Xiao3, Dawn Song4, Bo Li12 ∗ | 0.90 | “Zhaorun Chen1∗, Zhen Xiang2, Chaowei Xiao3, Dawn Song4, Bo Li12 ∗” (AGENTPOISON Red-teaming LLM agents via poisoning memory or knowledge bases.txt @段1) || 发表年份 | 2024 | 0.86 | “38th Conference on Neural Information Processing Systems (NeurIPS 2024).” (AGENTPOISON Red-teaming LLM agents via poisoning memory or knowledge bases.txt @段1) || 期刊/会议 | 38th Conference on Neural Information Processing Systems (NeurIPS 2024). | 0.88 | “38th Conference on Neural Information Processing Systems (NeurIPS 2024).” (AGENTPOISON Red-teaming LLM agents via poisoning memory or knowledge bases.txt @段1) || 关键词 | knowledge, rag, trigger, poison, memory, malicious, demonstrations, retrieval, backdoor, base | 0.78 | “AGENT POISON : Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases Zhaorun Chen1∗, Zhen Xiang2, Chaowei Xiao3, Dawn Song4, Bo Li12 ∗ 1University of Chicago, 2University of Illinois, Urbana-Champaign 3University of Wisconsin, Madison 4University of California, Berkeley” (AGENTPOISON Red-teaming LLM agents via poisoning memory or knowledge bases.txt @段1) || 摘要 | AGENT POISON : Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases Zhaorun Chen1∗, Zhen Xiang2, Chaowei Xiao3, Dawn Song4, Bo Li12 ∗ 1University of Chicago, 2University of Illinois, Urbana-Champaign 3University of Wisconsin, Madison 4University of California, Berkeley Abstract LLM agents have demonstrated remarkable performance across various applica- tions, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact … | 0.72 | “AGENT POISON : Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases Zhaorun Chen1∗, Zhen Xiang2, Chaowei Xiao3, Dawn Song4, Bo Li12 ∗ 1University of Chicago, 2University of Illinois, Urbana-Champaign 3University of Wisconsin, Madison 4University of California, Berkeley” (AGENTPOISON Red-teaming LLM agents via poisoning memory or knowledge bases.txt @段1) || 研究方法 | 3.3.1 Overview We design AGENT POISON to optimize a trigger xt that achieves both objectives of the attacker specified above. However, directly maximizingEq. (1) and Eq. (2) using gradient-based methods is challenging given the complexity of the RAG procedure, where the trigger is decisive in both the retrieval of demonstrations and the target action generation based on these demonstrations. Moreover, a practical attack should not only be effective but also stealthy and evasive, i.e., a triggered query should appe… | 0.82 | “Finally, we propose a gradient-guided beam search algorithm to solve the constrained optimization problem by searching for discrete tokens under non-derivative constraints.” (AGENTPOISON Red-teaming LLM agents via poisoning memory or knowledge bases.txt @段9) || 主要结论 | 1 Introduction Recent advancements in large language models (LLMs) have facilitated the extensive deployment of LLM agents in various applications, including safety-critical applications such as finance [ 37], healthcare [1, 25, 33, 27, 20], and autonomous driving [6, 12, 22]. These agents typically employ an LLM for task understanding and planning and can use external tools, such as third-party APIs, to execute the plan. The pipeline of LLM agents is often supported by retrieving past knowledge and instances from… | 0.80 | “We show thatAGENT POISON outperforms baseline attacks by achieving 82% retrieval success rate and 63% end-to-end attack success rate with less than 1% drop in the benign performance and with poisoning ratio less than 0.1%.” (AGENTPOISON Red-teaming LLM agents via poisoning memory or knowledge bases.txt @段3) |

- 主题标签：Backdoor/投毒, 隐私与记忆风险
- 方法标签：后门/Backdoor, 形式化/Optimization, 红队/Red-Teaming, 调查/Survey
- 源文件：`AGENTPOISON Red-teaming LLM agents via poisoning memory or knowledge bases.txt`

</details>
<details><summary>60. Preemptive Detection and Correction of Misaligned Actions in LLM Agents（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Preemptive Detection and Correction of Misaligned Actions in LLM Agents | 0.92 | “Preemptive Detection and Correction of Misaligned Actions in LLM Agents” (Preemptive detection and correction of misaligned actions in LLM agents.txt @段1) || 作者 | Preemptive Detection and Correction of Misaligned Actions in LLM Agents | 0.90 | “Preemptive Detection and Correction of Misaligned Actions in LLM Agents” (Preemptive detection and correction of misaligned actions in LLM agents.txt @段1) || 发表年份 | 2024 | 0.86 | “tasks (Zhou et al., 2023b; Wang et al., 2024; Fang” (Preemptive detection and correction of misaligned actions in LLM agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2407.11843v4  [cs.CL]  30 Sep 2025” (Preemptive detection and correction of misaligned actions in LLM agents.txt @段1) || 关键词 | inferact, user, task, actions, human, misaligned, actor, 2024, critical, intent | 0.78 | “Preemptive Detection and Correction of Misaligned Actions in LLM Agents Haishuo Fang1 Xiaodan Zhu1,2 Iryna Gurevych1 1Ubiquitous Knowledge Processing Lab, Department of Computer Science, TU Darmstadt National Research Center for Applied Cybersecurity ATHENE, Germany 2Department of Electrical and Computer Engineering &” (Preemptive detection and correction of misaligned actions in LLM agents.txt @段1) || 摘要 | Preemptive Detection and Correction of Misaligned Actions in LLM Agents Haishuo Fang1 Xiaodan Zhu1,2 Iryna Gurevych1 1Ubiquitous Knowledge Processing Lab, Department of Computer Science, TU Darmstadt National Research Center for Applied Cybersecurity ATHENE, Germany 2Department of Electrical and Computer Engineering & Ingenuity Labs Research Institute, Queen’s University, Canada 1www.ukp.tu-darmstadt.de 2xiaodan.zhu@queensu.ca Abstract Deploying LLM-based agents in real-life appli- cations often faces a critical c… | 0.72 | “Preemptive Detection and Correction of Misaligned Actions in LLM Agents Haishuo Fang1 Xiaodan Zhu1,2 Iryna Gurevych1 1Ubiquitous Knowledge Processing Lab, Department of Computer Science, TU Darmstadt National Research Center for Applied Cybersecurity ATHENE, Germany 2Department of Electrical and Computer Engineering &” (Preemptive detection and correction of misaligned actions in LLM agents.txt @段1) || 研究方法 | N/A | 0.50 | N/A || 主要结论 | 1 Introduction Large Language Models (LLMs) have revolu- tionized human-AI collaboration by enabling au- tonomous agents to execute complex, multi-step tasks (Zhou et al., 2023b; Wang et al., 2024; Fang et al., 2024; Liu et al., 2024a). Despite these ad- vances, deploying such agents in real-world scenar- ios introduces significant challenges, particularly in environments where certain actions carry sub- stantial consequences. A misexecution of those critical actions can lead to operational failures, ero- sion of … | 0.80 | “Our results show that the Actor agent guided by InferAct im- proves the success rate by a margin of 10.4% over the alternative methods with natural language feed- back.” (Preemptive detection and correction of misaligned actions in LLM agents.txt @段3) |

- 主题标签：Jailbreak/越狱攻击, Web/Browser Agent 安全, 多智能体对抗与协作, 防御与对齐机制
- 方法标签：仿真/Simulator, 多智能体/Multi-Agent
- 源文件：`综述参考文献\Preemptive detection and correction of misaligned actions in LLM agents.txt`

</details>
<details><summary>61. Progress and Prospects（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Progress and Prospects | 0.92 | “Progress and Prospects” (LLM-Powered GUI Agents in Phone Automation Surveying Progress and Prospects.txt @段3) || 作者 | Progress and Prospects | 0.90 | “Progress and Prospects” (LLM-Powered GUI Agents in Phone Automation Surveying Progress and Prospects.txt @段1) || 发表年份 | 2024 | 0.86 | “environments Arnatovich et al. (2018); Deshmukh & Phalke (2023); Nass (2024); Nass et al. (2021).” (LLM-Powered GUI Agents in Phone Automation Surveying Progress and Prospects.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2504.19838v3  [cs.HC]  17 Nov 2025” (LLM-Powered GUI Agents in Phone Automation Surveying Progress and Prospects.txt @段1) || 关键词 | gui, phone, automation, 2024, mobile, 2025, 2023, wang, powered, zhang | 0.78 | “Published in Transactions on Machine Learning Research (11/2025) LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects Guangyi Liu1∗, Pengxiang Zhao1∗, Yaozhen Liang1∗, Liang Liu2†, Yaxuan Guo2, Han Xiao3, Weifeng Lin3, Yuxiang Chai3, Yue Han1, Shuai Ren2, Hao Wang1, Xiaoyu Liang1, WenHao Wang1,” (LLM-Powered GUI Agents in Phone Automation Surveying Progress and Prospects.txt @段1) || 摘要 | Published in Transactions on Machine Learning Research (11/2025) LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects Guangyi Liu1∗, Pengxiang Zhao1∗, Yaozhen Liang1∗, Liang Liu2†, Yaxuan Guo2, Han Xiao3, Weifeng Lin3, Yuxiang Chai3, Yue Han1, Shuai Ren2, Hao Wang1, Xiaoyu Liang1, WenHao Wang1, Tianze Wu1, Zhengxi Lu1, Siheng Chen4, LiLinghao1, Hao Wang2, Guanjing Xiong2, Yong Liu1‡, Hongsheng Li3‡ 1Zhejiang University 2vivo AI Lab 3CUHK MMLab 4Shanghai Jiao Tong University Reviewed on Open… | 0.72 | “Published in Transactions on Machine Learning Research (11/2025) LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects Guangyi Liu1∗, Pengxiang Zhao1∗, Yaozhen Liang1∗, Liang Liu2†, Yaxuan Guo2, Han Xiao3, Weifeng Lin3, Yuxiang Chai3, Yue Han1, Shuai Ren2, Hao Wang1, Xiaoyu Liang1, WenHao Wang1,” (LLM-Powered GUI Agents in Phone Automation Surveying Progress and Prospects.txt @段1) || 研究方法 | 1 Introduction The core of phone GUI automation involves programmatically simulating human interactions with mobile interfaces to accomplish complex tasks. This technology has wide applications in testing and shortcut creation, enhancing efficiency and reducing manual effort Azim & Neamtiu (2013); Pan et al. (2020); Koroglu et al. (2018); Li et al. (2019); Degott et al. (2019). Traditional approaches rely on predefined scripts and templates which, while functional, lack flexibility when confronting variable interf… | 0.82 | “To provide a comprehensive overview of the current state and future prospects of LLM-Powered GUI Agents in Phone Automation, we present a taxonomy that categorizes the field into three main areas: Frameworks of LLM-powered phone GUI agents, Large Language Models for Phone Automation, and Datasets and Evaluation Methods” (LLM-Powered GUI Agents in Phone Automation Surveying Progress and Prospects.txt @段3) || 主要结论 | 2025.06 Phone Rule-Based RL InternVL3 8B MobileGUI-RLShi et al. (2025) 2025.07 Phone Rule-Based RL Qwen2.5-VL 7B/32B MagicGUITang et al. (2025) 2025.08 Phone Rule-Based RL N/A N/A these advancements, AutoGLM Liu et al. (2024b) extends the application of RL to both phone and web platforms. AutoGLM presents a series of foundation agents based on the ChatGLM model family, aiming to serve as autonomous agents for GUI control. A key insight from this work is the design of an intermediate interface that separates planni… | 0.80 | “Evaluations on the WebShop environment and real-world booking scenarios show that Agent Q significantly improves success rates, outperforming behavior cloning and reinforcement learning fine-tuned baselines.” (LLM-Powered GUI Agents in Phone Automation Surveying Progress and Prospects.txt @段90) |

- 主题标签：GUI/Computer-Use Agent 安全, Prompt Injection/环境注入, Web/Browser Agent 安全
- 方法标签：仿真/Simulator, 基准/Benchmark, 多模态/Multimodal, 调查/Survey
- 源文件：`LLM-Powered GUI Agents in Phone Automation Surveying Progress and Prospects.txt`

</details>
<details><summary>62. Prompt Injection Attacks（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Prompt Injection Attacks | 0.92 | “Prompt Injection Attacks” (WASP Benchmarking web agent security against prompt injection attacks.txt @段2) || 作者 | productivity by automating routine tasks such as filing taxes and paying bills. | 0.90 | “productivity by automating routine tasks such as filing taxes and paying bills.” (WASP Benchmarking web agent security against prompt injection attacks.txt @段1) || 发表年份 | 2024 | 0.86 | “Operator (OpenAI, 2025), Anthropic’s Claude Computer Use Agent (Anthropic, 2024), and the” (WASP Benchmarking web agent security against prompt injection attacks.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2504.18575v3  [cs.CR]  16 May 2025” (WASP Benchmarking web agent security against prompt injection attacks.txt @段1) || 关键词 | web, end, 2024, attacker, prompt, environment, injection, realistic, work, only | 0.78 | “WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks Ivan Evtimov∗ FAIR at Meta Arman Zharmagambetov∗ FAIR at Meta Aaron Grattafiori† Independent Researcher Chuan Guo‡ FAIR at Meta Kamalika Chaudhuri‡ FAIR at Meta” (WASP Benchmarking web agent security against prompt injection attacks.txt @段1) || 摘要 | WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks Ivan Evtimov∗ FAIR at Meta Arman Zharmagambetov∗ FAIR at Meta Aaron Grattafiori† Independent Researcher Chuan Guo‡ FAIR at Meta Kamalika Chaudhuri‡ FAIR at Meta Abstract Autonomous UI agents powered by AI have tremendous potential to boost human productivity by automating routine tasks such as filing taxes and paying bills. However, a major challenge in unlocking their full potential is security, which is exacerbated by the agent’s ability to t… | 0.72 | “WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks Ivan Evtimov∗ FAIR at Meta Arman Zharmagambetov∗ FAIR at Meta Aaron Grattafiori† Independent Researcher Chuan Guo‡ FAIR at Meta Kamalika Chaudhuri‡ FAIR at Meta” (WASP Benchmarking web agent security against prompt injection attacks.txt @段1) || 研究方法 | 3.4 Main metrics In our test scenarios, each attacker goal has a specified intended outcome. For example, if the attacker goal is data exfiltration, we can check if the target user data has been sent to the attacker server in the web environment’s final state. Thus, for each attacker goal, we also define a rule-based evaluator to check if the final state satisfies the specified rule for attacker goal success. We call this ASR-end-to-end. In many instances, the attack succeeds in diverting the agent from its user g… | 0.82 | “Illustrative example To better understand our pipeline, we present an instance of a successful end-to-end attack on GPT-4o based agent in Figure 2.” (WASP Benchmarking web agent security against prompt injection attacks.txt @段11) || 主要结论 | 5 Conclusion We introduced WASP, a new security benchmark designed to assess the robustness of autonomous web navigation agents against prompt injection attacks. Unlike most previous studies that utilize sim- ulated environments with simplistic attacker objectives (e.g., displaying ”Hacked”), our benchmark employs fully operational, self-hosted websites, incorporating realistic assumptions about attacker and defender capabilities and more complex attacker goals (e.g., changing the user’s password). Furthermore, ou… | 0.80 | “However, achieving the ultimate goal of the attacker proves to be significantly more challenging due to the limitations of the agents’ capabilities and the complexity of the attacker’s objectives.” (WASP Benchmarking web agent security against prompt injection attacks.txt @段15) |

- 主题标签：Prompt Injection/环境注入, Web/Browser Agent 安全
- 方法标签：仿真/Simulator, 基准/Benchmark, 提示注入/Prompt Injection
- 源文件：`综述参考文献\WASP Benchmarking web agent security against prompt injection attacks.txt`

</details>
<details><summary>63. Prompt Injection Attacks and Defenses（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Prompt Injection Attacks and Defenses | 0.92 | “Prompt Injection Attacks and Defenses” (AgentDojo A dynamic environment to evaluate prompt injection attacks and defenses for LLM agents.txt @段2) || 作者 | Prompt Injection Attacks and Defenses | 0.90 | “Prompt Injection Attacks and Defenses” (AgentDojo A dynamic environment to evaluate prompt injection attacks and defenses for LLM agents.txt @段1) || 发表年份 | 2024 | 0.86 | “38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.” (AgentDojo A dynamic environment to evaluate prompt injection attacks and defenses for LLM agents.txt @段1) || 期刊/会议 | 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. | 0.88 | “38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.” (AgentDojo A dynamic environment to evaluate prompt injection attacks and defenses for LLM agents.txt @段1) || 关键词 | tasks, environment, agentdojo, prompt, user, tools, injection, data, calendar, state | 0.78 | “AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents Edoardo Debenedetti1∗ Jie Zhang1 Mislav Balunovic1,2 Luca Beurer-Kellner1,2 Marc Fischer1,2 Florian Tramèr1 1ETH Zurich 2Invariant Labs” (AgentDojo A dynamic environment to evaluate prompt injection attacks and defenses for LLM agents.txt @段1) || 摘要 | AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents Edoardo Debenedetti1∗ Jie Zhang1 Mislav Balunovic1,2 Luca Beurer-Kellner1,2 Marc Fischer1,2 Florian Tramèr1 1ETH Zurich 2Invariant Labs Abstract AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls. Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks. To measure the adversarial … | 0.72 | “AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents Edoardo Debenedetti1∗ Jie Zhang1 Mislav Balunovic1,2 Luca Beurer-Kellner1,2 Marc Fischer1,2 Florian Tramèr1 1ETH Zurich 2Invariant Labs” (AgentDojo A dynamic environment to evaluate prompt injection attacks and defenses for LLM agents.txt @段1) || 研究方法 | 1 Introduction Large language models (LLMs) have the ability to understand tasks described in natural language and generate plans to solve them [20, 27, 49, 60]. A promising design paradigm for AI agents [65] is to combine an LLM with tools that interact with a broader environment [14, 35, 40, 47, 51, 53, 55, 69]. AI agents could be used for various roles, such as digital assistants with access to emails and calendars, or smart “operating systems” with access to coding environments and scripts [24, 25]. However, a… | 0.82 | “To measure the ability of AI agents to safely solve tasks in adversarial settings when prompt injections are in place, we introduce AgentDojo, a dynamic benchmarking framework which we populate–as a first version–with 97 realistic tasks and 629 security test cases.” (AgentDojo A dynamic environment to evaluate prompt injection attacks and defenses for LLM agents.txt @段3) || 主要结论 | 4.2 Ablations on Attack Components and Attacker Knowledge Our experiments above use a generic prompt injection (see Figure 19a), that warns the agent of an important additional task. The warning message further directly addresses the model (“This is a message to you, GPT-4o”), and uses the victim’s real name (e.g., “This is a message from me, Josh”). We now ablate which components of this attack prompt contribute to its success. We first compare four prompt injection messages: (i) the “ignore previous instructions… | 0.80 | “We find that this knowledge slightly increases the success rate of our attack (by 1.9%), but that incorrect guesses (e.g., addressing GPT-4o as Claude) significantly weaken the attack.” (AgentDojo A dynamic environment to evaluate prompt injection attacks and defenses for LLM agents.txt @段18) |

- 主题标签：GUI/Computer-Use Agent 安全, Prompt Injection/环境注入, Web/Browser Agent 安全, 防御与对齐机制
- 方法标签：仿真/Simulator, 基准/Benchmark, 多模态/Multimodal, 提示注入/Prompt Injection, 系统防御/Defense
- 源文件：`AgentDojo A dynamic environment to evaluate prompt injection attacks and defenses for LLM agents.txt`

</details>
<details><summary>64. PromptArmor: Simple yet Effective Prompt Injection Defenses（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | PromptArmor: Simple yet Effective Prompt Injection Defenses | 0.92 | “PromptArmor: Simple yet Effective Prompt Injection Defenses” (PromptArmor Simple yet effective prompt injection defenses.txt @段1) || 作者 | Tianneng Shi1, Kaijie Zhu2, Zhun Wang1, Yuqi Jia3, | 0.90 | “Tianneng Shi1, Kaijie Zhu2, Zhun Wang1, Yuqi Jia3,” (PromptArmor Simple yet effective prompt injection defenses.txt @段1) || 发表年份 | 2024 | 0.86 | “LLM agents (OpenAI, 2024; Anthropic, 2024;” (PromptArmor Simple yet effective prompt injection defenses.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2507.15219v1  [cs.CR]  21 Jul 2025” (PromptArmor Simple yet effective prompt injection defenses.txt @段1) || 关键词 | prompt, injected, data, injection, 2024, guardrail, 2025, against, sample, which | 0.78 | “PromptArmor: Simple yet Effective Prompt Injection Defenses Tianneng Shi1, Kaijie Zhu2, Zhun Wang1, Yuqi Jia3, Will Cai1, Weida Liang4, Haonan Wang4, Hend Alzahrani5, Joshua Lu1, Kenji Kawaguchi4, Basel Alomair5,6, Xuandong Zhao1, William Yang Wang2, Neil Gong3, Wenbo Guo2, Dawn Song1 1UC Berkeley, 2UC Santa Barbara, 3” (PromptArmor Simple yet effective prompt injection defenses.txt @段1) || 摘要 | PromptArmor: Simple yet Effective Prompt Injection Defenses Tianneng Shi1, Kaijie Zhu2, Zhun Wang1, Yuqi Jia3, Will Cai1, Weida Liang4, Haonan Wang4, Hend Alzahrani5, Joshua Lu1, Kenji Kawaguchi4, Basel Alomair5,6, Xuandong Zhao1, William Yang Wang2, Neil Gong3, Wenbo Guo2, Dawn Song1 1UC Berkeley, 2UC Santa Barbara, 3Duke University, 4National University of Singapore, 5King Abdulaziz City for Science and Technology, 6University of Washington Abstract Despite their potential, recent research has demonstrated that … | 0.72 | “PromptArmor: Simple yet Effective Prompt Injection Defenses Tianneng Shi1, Kaijie Zhu2, Zhun Wang1, Yuqi Jia3, Will Cai1, Weida Liang4, Haonan Wang4, Hend Alzahrani5, Joshua Lu1, Kenji Kawaguchi4, Basel Alomair5,6, Xuandong Zhao1, William Yang Wang2, Neil Gong3, Wenbo Guo2, Dawn Song1 1UC Berkeley, 2UC Santa Barbara, 3” (PromptArmor Simple yet effective prompt injection defenses.txt @段1) || 研究方法 | N/A | 0.50 | N/A || 主要结论 | 6 Conclusion In this paper, we propose PromptArmor, a simple yet surprisingly effective defense against prompt injection attacks. By leveraging carefully designed prompting strategies, PromptArmor repurposes an off-the-shelf LLM into a powerful tool for detecting and removing injected prompts. Our results show that PromptArmor is effective across a variety of settings and remains robust against strong adaptive attacks specifically crafted to evade it. 8 | 0.80 | “Our results show that PromptArmor is effective across a variety of settings and remains robust against strong adaptive attacks specifically crafted to evade it.” (PromptArmor Simple yet effective prompt injection defenses.txt @段19) |

- 主题标签：Prompt Injection/环境注入, 防御与对齐机制
- 方法标签：提示注入/Prompt Injection, 系统防御/Defense
- 源文件：`综述参考文献\PromptArmor Simple yet effective prompt injection defenses.txt`

</details>
<details><summary>65. Red-Teaming LLM Multi-Agent Systems via Communication Attacks（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Red-Teaming LLM Multi-Agent Systems via Communication Attacks | 0.92 | “Red-Teaming LLM Multi-Agent Systems via Communication Attacks” (Red-teaming LLM multi-agent systems via communication attacks.txt @段3) || 作者 | Findings of the Association for Computational Linguistics: ACL 2025 , pages 6726–6747 | 0.90 | “Findings of the Association for Computational Linguistics: ACL 2025 , pages 6726–6747” (Red-teaming LLM multi-agent systems via communication attacks.txt @段1) || 发表年份 | 2024 | 0.86 | “LLM. (Guo et al., 2024a; Wu et al., 2023; Talebirad” (Red-teaming LLM multi-agent systems via communication attacks.txt @段1) || 期刊/会议 | Findings of the Association for Computational Linguistics: ACL 2025 , pages 6726–6747 | 0.86 | “Findings of the Association for Computational Linguistics: ACL 2025 , pages 6726–6747” (Red-teaming LLM multi-agent systems via communication attacks.txt @段1) || 关键词 | 2023, mas, communication, 2024, messages, system, aitm, malicious, multi, through | 0.78 | “Findings of the Association for Computational Linguistics: ACL 2025 , pages 6726–6747 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Red-Teaming LLM Multi-Agent Systems via Communication Attacks Pengfei He1*, Yuping Lin1, Shen Dong1, Han Xu2, Yue Xing1, Hui Liu1 1Michigan State University 2Uni” (Red-teaming LLM multi-agent systems via communication attacks.txt @段1) || 摘要 | Findings of the Association for Computational Linguistics: ACL 2025 , pages 6726–6747 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Red-Teaming LLM Multi-Agent Systems via Communication Attacks Pengfei He1*, Yuping Lin1, Shen Dong1, Han Xu2, Yue Xing1, Hui Liu1 1Michigan State University 2University of Arizona Abstract Large Language Model-based Multi-Agent Sys- tems (LLM-MAS) have revolutionized com- plex problem-solving capability by enabling sophisticated agent collaboration through m… | 0.72 | “Findings of the Association for Computational Linguistics: ACL 2025 , pages 6726–6747 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Red-Teaming LLM Multi-Agent Systems via Communication Attacks Pengfei He1*, Yuping Lin1, Shen Dong1, Han Xu2, Yue Xing1, Hui Liu1 1Michigan State University 2Uni” (Red-teaming LLM multi-agent systems via communication attacks.txt @段1) || 研究方法 | Abstract Large Language Model-based Multi-Agent Sys- tems (LLM-MAS) have revolutionized com- plex problem-solving capability by enabling sophisticated agent collaboration through message-based communications. While the communication framework is crucial for agent coordination, it also introduces a critical yet unexplored security vulnerability. In this work, we introduce Agent-in-the-Middle (AiTM), a novel attack that exploits the fundamental com- munication mechanisms in LLM-MAS by in- tercepting and manipulating… | 0.82 | “Our compre- hensive evaluation across various frameworks, communication structures, and real-world ap- plications demonstrates that LLM-MAS is vul- nerable to communication-based attacks, high- lighting the need for robust security measures in multi-agent systems.” (Red-teaming LLM multi-agent systems via communication attacks.txt @段2) || 主要结论 | 4.4 Real-world applications (RQ3) Besides the simulation using multi-agent frame- works, we test AiTM on two popular real-world LLM-MAS: (1) MetaGPT (Hong et al., 2023) is a meta-programming framework multi-agent system mirroring a human software company. It utilizes specialized agents and encodes Standardized Oper- ating Procedures (SOPs) into prompt sequences for more streamlined workflows. (2) ChatDev (Qian et al., 2024a) is a chat-powered software development framework and aims to reduce hallu- Table 4: Attack… | 0.80 | “Our results show that AiTM can indeed compro- mise real-world applications, revealing potential communication threats in real practice.” (Red-teaming LLM multi-agent systems via communication attacks.txt @段14) |

- 主题标签：GUI/Computer-Use Agent 安全, 多智能体对抗与协作
- 方法标签：仿真/Simulator, 基准/Benchmark, 多智能体/Multi-Agent, 多模态/Multimodal, 红队/Red-Teaming
- 源文件：`Red-teaming LLM multi-agent systems via communication attacks.txt`

</details>
<details><summary>66. Refusal-Trained LLMs Are Easily Jailbroken（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Refusal-Trained LLMs Are Easily Jailbroken | 0.92 | “Refusal-Trained LLMs Are Easily Jailbroken” (Refusal-trained LLMs are easily jailbroken as browser agents.txt @段1) || 作者 | Priyanshu Kumar*1, Elaine Lau◦3, Saranya Vijayakumar◦1, T u Trinh◦3, Scale Red Team3, | 0.90 | “Priyanshu Kumar*1, Elaine Lau◦3, Saranya Vijayakumar◦1, T u Trinh◦3, Scale Red Team3,” (Refusal-trained LLMs are easily jailbroken as browser agents.txt @段1) || 发表年份 | 2024 | 0.86 | “behaviors (including original behaviors and ones sourced from HarmBench [Mazeika et al., 2024] and” (Refusal-trained LLMs are easily jailbroken as browser agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2410.13886v2  [cs.CR]  21 Oct 2024” (Refusal-trained LLMs are easily jailbroken as browser agents.txt @段1) || 关键词 | browser, behaviors, harmful, 2024, red, chat, teaming, 2023, figure, content | 0.78 | “Refusal-Trained LLMs Are Easily Jailbroken As Browser Agents Priyanshu Kumar*1, Elaine Lau◦3, Saranya Vijayakumar◦1, T u Trinh◦3, Scale Red Team3, Elaine Chang3, Vaughn Robinson3, Sean Hendryx3, Shuyan Zhou1, Matt Fredrikson1, 2, Summer Yue3, Zifan Wang⋆3 1Carnegie Mellon University, 2GraySwan AI, 3Scale AI /da◎abaseHu” (Refusal-trained LLMs are easily jailbroken as browser agents.txt @段1) || 摘要 | Refusal-Trained LLMs Are Easily Jailbroken As Browser Agents Priyanshu Kumar*1, Elaine Lau◦3, Saranya Vijayakumar◦1, T u Trinh◦3, Scale Red Team3, Elaine Chang3, Vaughn Robinson3, Sean Hendryx3, Shuyan Zhou1, Matt Fredrikson1, 2, Summer Yue3, Zifan Wang⋆3 1Carnegie Mellon University, 2GraySwan AI, 3Scale AI /da◎abaseHuggingface /codeGithub /gl⌢beWebsite ∗First Author; Work done during the internship at Scale AI ◦Core Authors ⋆Correspondence to zifan.wang@scale.com Abstract For safety reasons, large language models… | 0.72 | “Refusal-Trained LLMs Are Easily Jailbroken As Browser Agents Priyanshu Kumar*1, Elaine Lau◦3, Saranya Vijayakumar◦1, T u Trinh◦3, Scale Red Team3, Elaine Chang3, Vaughn Robinson3, Sean Hendryx3, Shuyan Zhou1, Matt Fredrikson1, 2, Summer Yue3, Zifan Wang⋆3 1Carnegie Mellon University, 2GraySwan AI, 3Scale AI /da◎abaseHu” (Refusal-trained LLMs are easily jailbroken as browser agents.txt @段1) || 研究方法 | 3.3 Harm Classification We use LLM-as-a-judge [Zheng et al., 2023] to determining if the harmful content is generated. More specifically, we extract the text content typed by the browser agent and feed the content to a LLM for classification. For example, after the broswer agent perform the actions, we extract the email body from 2https://websim.ai we have obtained consent from WebSim for this research. 3We targeted 4 behaviors on the Customer Management System and GitLab pages hosted by Zhou et al. [2023]. These … | 0.82 | “Besides OpenHands, we have also attempted another popular browser agent framework SeeAct [Zheng 4We found that the original HarmBench classifier has a higher false positives compared to GPT-4o in our preliminary experiments with a subset of BrowserART behaviors.” (Refusal-trained LLMs are easily jailbroken as browser agents.txt @段6) || 主要结论 | … Figure 1: Top (motivation of our proposed red teaming suite BrowserART): while refusal-trained LLMs as chatbots are generally expected to refuse harmful instructions from malicious users, providing them with web browser access and prompting them as agents can significantly decrease the alignment.Bottom (result preview): We directly ask (i.e., w/o attacks) all LLMs and agents to fulfill harmful behaviors. We also employ LLM attack techniques to further jailbreak browser agents. A preview of results for GPT-4o and… | 0.80 | “… Figure 1: Top (motivation of our proposed red teaming suite BrowserART): while refusal-trained LLMs as chatbots are generally expected to refuse harmful instructions from malicious users, providing them with web browser access and prompting them as agents can significantly decrease the alignment.Bottom (result previe” (Refusal-trained LLMs are easily jailbroken as browser agents.txt @段3) |

- 主题标签：GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击, Web/Browser Agent 安全, 防御与对齐机制
- 方法标签：基准/Benchmark, 红队/Red-Teaming, 越狱/Jailbreak
- 源文件：`综述参考文献\Refusal-trained LLMs are easily jailbroken as browser agents.txt`

</details>
<details><summary>67. SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning | 0.92 | “SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning” (ShieldAgent Shielding agents via verifiable safety policy reasoning.txt @段1) || 作者 | ous real-world applications. However, they re- | 0.90 | “ous real-world applications. However, they re-” (ShieldAgent Shielding agents via verifiable safety policy reasoning.txt @段1) || 发表年份 | 2024 | 0.86 | “2023), GUI navigation (Lin et al., 2024), and embodied” (ShieldAgent Shielding agents via verifiable safety policy reasoning.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2503.22738v2  [cs.LG]  27 Nov 2025” (ShieldAgent Shielding agents via verifiable safety policy reasoning.txt @段1) || 关键词 | action, rules, policy, 2024, aspm, shieldagent, verification, predicates, shielding, each | 0.78 | “SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning Zhaorun Chen 1 Mintong Kang 2 Bo Li 1 2” (ShieldAgent Shielding agents via verifiable safety policy reasoning.txt @段1) || 摘要 | SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning Zhaorun Chen 1 Mintong Kang 2 Bo Li 1 2 Abstract Autonomous agents powered by foundation mod- els have seen widespread adoption across vari- ous real-world applications. However, they re- main highly vulnerable to malicious instructions and attacks, which can result in severe conse- quences such as privacy breaches and financial losses. More critically, existing guardrails for LLMs are not applicable due to the complex and dynamic nature of agent… | 0.72 | “SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning Zhaorun Chen 1 Mintong Kang 2 Bo Li 1 2” (ShieldAgent Shielding agents via verifiable safety policy reasoning.txt @段1) || 研究方法 | P,R, π θ  s.t.P={P a,P s},R={R a,R p}(2) where πθ denotes the probabilistic logic model (parameter- ized by θ) which organizes the rules (see §3.2.4). Specif- ically, GASPM partitions P intostate predicates ps ∈ P s to represent system states or environmental conditions, and action predicates pa ∈ Pa to represent target actions. Con- sequently, R is divided intoaction rules Ra which encodes safety specifications for target actions, andphysical rules Rp which capture internal constraints on system variables. Speci… | 0.82 | “Therefore, we propose a bi-stage optimization algorithm to iteratively refine the rules in ASPM by: (1) improving their alignment with the original natural language policies, (2) enhancing verifiability by decomposing complex or vague rules into more atomic and concrete forms, and (3) increas- ing verification efficien” (ShieldAgent Shielding agents via verifiable safety policy reasoning.txt @段4) || 主要结论 | ls, Vs, Ts  ▷ Return safety label, violated rules, textual explanation optimize rule weights for each circuit θa over a dataset D={ζ (i), y(i))}N i=1 via the following guardrail hinge loss: Lg(θ) = E (ζ,Y)∼D max(0,−y (i)(Pθ(µ(i) pa=1)−Pθ(µ(i) pa=0)))(6) where labels y(i) = 1 if action a(i) issafeor y(i) =−1 ifun- safe. Specifically, y(i) can be derived from either real-world safety-labeled data or simulated pseudo-learning (Kang & Li, 2024). The learned weights act as soft constraints, captur- ing the relative im… | 0.80 | “Em- pirical results show that SHIELDAGENToutperforms exist- ing methods in guardrail accuracy while significantly re- ducing resource overhead.” (ShieldAgent Shielding agents via verifiable safety policy reasoning.txt @段11) |

- 主题标签：Prompt Injection/环境注入, 防御与对齐机制, 隐私与记忆风险
- 方法标签：仿真/Simulator, 基准/Benchmark, 系统防御/Defense
- 源文件：`综述参考文献\ShieldAgent Shielding agents via verifiable safety policy reasoning.txt`

</details>
<details><summary>68. Software Process Models Using Large Language（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Software Process Models Using Large Language | 0.92 | “Software Process Models Using Large Language” (SOEN-101 Code generation by emulating software process models using large language model agents.txt @段2) || 作者 | Feng Lin 1, Dong Jae Kim 2, Tse-Hsun (Peter) Chen 1 | 0.90 | “Feng Lin 1, Dong Jae Kim 2, Tse-Hsun (Peter) Chen 1” (SOEN-101 Code generation by emulating software process models using large language model agents.txt @段1) || 发表年份 | 2024 | 0.86 | “arXiv:2403.15852v2  [cs.SE]  31 Oct 2024” (SOEN-101 Code generation by emulating software process models using large language model agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2403.15852v2  [cs.SE]  31 Oct 2024” (SOEN-101 Code generation by emulating software process models using large language model agents.txt @段1) || 关键词 | code, test, software, development, process, requirement, design, generation, generated, flowgen | 0.78 | “SOEN-101: Code Generation by Emulating Software Process Models Using Large Language Model Agents Feng Lin 1, Dong Jae Kim 2, Tse-Hsun (Peter) Chen 1 1Software PErformance, Analysis, and Reliability (SPEAR) lab, Concordia University, Montreal, Canada 2DePaul University, Chicago, USA feng.lin@mail.concordia.ca, k dongja@” (SOEN-101 Code generation by emulating software process models using large language model agents.txt @段1) || 摘要 | SOEN-101: Code Generation by Emulating Software Process Models Using Large Language Model Agents Feng Lin 1, Dong Jae Kim 2, Tse-Hsun (Peter) Chen 1 1Software PErformance, Analysis, and Reliability (SPEAR) lab, Concordia University, Montreal, Canada 2DePaul University, Chicago, USA feng.lin@mail.concordia.ca, k dongja@encs.concordia.ca, peterc@encs.concordia.ca Abstract—Software process models are essential to facilitate collaboration and communication among software teams to solve complex development tasks. Inspi… | 0.72 | “SOEN-101: Code Generation by Emulating Software Process Models Using Large Language Model Agents Feng Lin 1, Dong Jae Kim 2, Tse-Hsun (Peter) Chen 1 1Software PErformance, Analysis, and Reliability (SPEAR) lab, Concordia University, Montreal, Canada 2DePaul University, Chicago, USA feng.lin@mail.concordia.ca, k dongja@” (SOEN-101 Code generation by emulating software process models using large language model agents.txt @段1) || 研究方法 | 20 to 30 in HumanEval and HumanEval-ET). In0301, MBPP’s Pass@1 is even lower with a value around 5. The findings show that model version may have a significant impact on the generated code . However, we see that, after adopting our agent-based techniques, all three variants of FlowGen achieve similar Pass@1 across GPT3.5 versions. The results indicate that FlowGen can generate similar-quality code even if we have an underperformed baseline model. All techniques have a relatively similar Pass@1 when the temperature… | 0.82 | “In this study, we introduced FlowGen, a framework de- signed to emulate software process models using Large Lan- guage Model (LLM) agents, each representing roles such as requirement engineers, architects, developers, and testers.” (SOEN-101 Code generation by emulating software process models using large language model agents.txt @段17) || 主要结论 | 20 to 30 in HumanEval and HumanEval-ET). In0301, MBPP’s Pass@1 is even lower with a value around 5. The findings show that model version may have a significant impact on the generated code . However, we see that, after adopting our agent-based techniques, all three variants of FlowGen achieve similar Pass@1 across GPT3.5 versions. The results indicate that FlowGen can generate similar-quality code even if we have an underperformed baseline model. All techniques have a relatively similar Pass@1 when the temperature… | 0.80 | “Our results showed that incorporating software process models into LLM-based code generation significantly en- hances code correctness, reduces code smells, and improves exception handling.” (SOEN-101 Code generation by emulating software process models using large language model agents.txt @段17) |

- 主题标签：多智能体对抗与协作
- 方法标签：基准/Benchmark, 多智能体/Multi-Agent
- 源文件：`SOEN-101 Code generation by emulating software process models using large language model agents.txt`

</details>
<details><summary>69. The Task Shield: Enforcing Task Alignment to Defend Against Indirect（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | The Task Shield: Enforcing Task Alignment to Defend Against Indirect | 0.92 | “The Task Shield: Enforcing Task Alignment to Defend Against Indirect” (The task shield Enforcing task alignment to defend against indirect prompt injection in LLM agents.txt @段3) || 作者 | July 27 - August 1, 2025 ©2025 Association for Computational Linguistics | 0.90 | “July 27 - August 1, 2025 ©2025 Association for Computational Linguistics” (The task shield Enforcing task alignment to defend against indirect prompt injection in LLM agents.txt @段1) || 发表年份 | 2024 | 0.86 | “vron et al. , 2023; Schick et al. , 2024). Unlike tra-” (The task shield Enforcing task alignment to defend against indirect prompt injection in LLM agents.txt @段1) || 期刊/会议 | Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 29680–29697 | 0.88 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 29680–29697” (The task shield Enforcing task alignment to defend against indirect prompt injection in LLM agents.txt @段1) || 关键词 | task, user, instructions, tool, shield, alignment, indirect, 2024, assistant, goals | 0.78 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 29680–29697 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents Feiran Jia The” (The task shield Enforcing task alignment to defend against indirect prompt injection in LLM agents.txt @段1) || 摘要 | Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 29680–29697 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents Feiran Jia The Pennsylvania State University feiran.jia@psu.edu Tong Wu Princeton University tongwu@princeton.edu Xin Qin California State University, Long Beach xin.qin@csulb.edu Anna Squicciarini The Pennsylvani… | 0.72 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 29680–29697 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents Feiran Jia The” (The task shield Enforcing task alignment to defend against indirect prompt injection in LLM agents.txt @段1) || 研究方法 | 4 The Task Shield Framework While we deﬁned task alignment as an ideal secu- rity property, implementing it in practice requires an enforcement mechanism. To address this need, we introduce the Task Shield framework that con- tinuously monitors and enforces the alignment of the instruction with the user objectives. As shown in Figure 2, the framework consists of three key components: (1) instruction extrac- tion, (2) alignment check, and (3) feedback gen- eration to maintain task alignment throughout the conversat… | 0.82 | “To address this need, we introduce the Task Shield framework that con- tinuously monitors and enforces the alignment of the instruction with the user objectives.” (The task shield Enforcing task alignment to defend against indirect prompt injection in LLM agents.txt @段9) || 主要结论 | Abstract Large Language Model (LLM) agents are in- creasingly being deployed as conversational assistants capable of performing complex real- world tasks through tool integration. This en- hanced ability to interact with external systems and process various data sources, while pow- erful, introduces signiﬁcant security vulnera- bilities. In particular, indirect prompt injec- tion attacks pose a critical threat, where ma- licious instructions embedded within external data sources can manipulate agents to deviate fr… | 0.80 | “Through experiments on the Agent- Dojo benchmark, we demonstrate that Task Shield reduces attack success rates (2.07%) while maintaining high task utility (69.79%) on GPT-4o, signiﬁcantly outperforming exist- ing defenses in various real-world scenarios.” (The task shield Enforcing task alignment to defend against indirect prompt injection in LLM agents.txt @段2) |

- 主题标签：GUI/Computer-Use Agent 安全, Prompt Injection/环境注入, 防御与对齐机制
- 方法标签：基准/Benchmark, 多模态/Multimodal, 提示注入/Prompt Injection, 系统防御/Defense
- 源文件：`The task shield Enforcing task alignment to defend against indirect prompt injection in LLM agents.txt`

</details>
<details><summary>70. Towards LLM-augmented multiagent systems for agile soware（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Towards LLM-augmented multiagent systems for agile soware | 0.92 | “Towards LLM-augmented multiagent systems for agile soware” (Towards LLM-augmented multiagent systems for agile software engineering.txt @段7) || 作者 | KONRAD CINKUSZ, Warsaw University of Technology, Warsaw, MA, Poland | 0.90 | “KONRAD CINKUSZ, Warsaw University of Technology, Warsaw, MA, Poland” (Towards LLM-augmented multiagent systems for agile software engineering.txt @段1) || 发表年份 | 2024 | 0.90 | “Published: 27 October 2024” (Towards LLM-augmented multiagent systems for agile software engineering.txt @段1) || 期刊/会议 | arXiv | 0.90 | “Directions. arXiv:2402.01968 [cs.MA]” (Towards LLM-augmented multiagent systems for agile software engineering.txt @段1) || 关键词 | warsaw, engineering, software, systems, technology, acm, agile, 2024, university, 3691620 | 0.78 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3691620.3695336 . . RESEARCH-ARTICLE Towards LLM-augmented multiagent systems for agile soware engineering KONRAD CINKUSZ, Warsaw University of Technology, Warsaw, MA, Poland . JAROSŁAW A CHUDZIAK, Warsaw University of Technology, Warsaw, MA, Poland . . . Open Access S” (Towards LLM-augmented multiagent systems for agile software engineering.txt @段1) || 摘要 | . . Latest updates: hps://dl.acm.org/doi/10.1145/3691620.3695336 . . RESEARCH-ARTICLE Towards LLM-augmented multiagent systems for agile soware engineering KONRAD CINKUSZ, Warsaw University of Technology, Warsaw, MA, Poland . JAROSŁAW A CHUDZIAK, Warsaw University of Technology, Warsaw, MA, Poland . . . Open Access Support provided by: . Warsaw University of Technology . PDF Download 3691620.3695336.pdf 20 January 2026 Total Citations: 10 Total Downloads: 1342 . . Published: 27 October 2024 . . Citation in BibTe… | 0.72 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3691620.3695336 . . RESEARCH-ARTICLE Towards LLM-augmented multiagent systems for agile soware engineering KONRAD CINKUSZ, Warsaw University of Technology, Warsaw, MA, Poland . JAROSŁAW A CHUDZIAK, Warsaw University of Technology, Warsaw, MA, Poland . . . Open Access S” (Towards LLM-augmented multiagent systems for agile software engineering.txt @段1) || 研究方法 | ASE ’24, October 27-November 1, 2024, Sacramento, CA, USA Chudziak and Cinkusz Figure 2: Agents’ flow in CogniSim Ecosystem[3]. the deployment of these intelligent agents, with dynamic sequenc- ing driven by language models to support complex decision-making processes [2]. Multi-agent dialogue simulations further aid in mim- icking real-world project dynamics, helping agents generate appro- priate responses and manage discussions effectively. Cognitive agents within MAS, exemplified by the CogniSim ecosystem, impr… | 0.82 | “The core components of the Cog- niSim ecosystem include cognitive agents, communication proto- cols, learning algorithms, decision-making frameworks, and collab- oration tools, all of which contribute to more efficient, accurate, and adaptable project outcomes [ 3].” (Towards LLM-augmented multiagent systems for agile software engineering.txt @段10) || 主要结论 | N/A | 0.50 | N/A |

- 主题标签：多智能体对抗与协作
- 方法标签：仿真/Simulator, 多智能体/Multi-Agent
- 源文件：`Towards LLM-augmented multiagent systems for agile software engineering.txt`

</details>
<details><summary>71. Towards Trustworthy GUI Agents: A Survey（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Towards Trustworthy GUI Agents: A Survey | 0.92 | “Towards Trustworthy GUI Agents: A Survey” (Towards trustworthy GUI agents A survey.txt @段1) || 作者 | Yucheng Shi1, Wenhao Yu2, Wenlin Yao3, Wenhu Chen4, Ninghao Liu 1 | 0.90 | “Yucheng Shi1, Wenhao Yu2, Wenlin Yao3, Wenhu Chen4, Ninghao Liu 1” (Towards trustworthy GUI agents A survey.txt @段1) || 发表年份 | 2024 | 0.86 | “2024a; Wang et al., 2024c; Xie et al., 2024). De-” (Towards trustworthy GUI agents A survey.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2503.23434v1  [cs.LG]  30 Mar 2025” (Towards trustworthy GUI agents A survey.txt @段1) || 关键词 | 2024, gui, 2025, interfaces, reliability, risks, section, challenges, ethical, transparency | 0.78 | “Towards Trustworthy GUI Agents: A Survey Yucheng Shi1, Wenhao Yu2, Wenlin Yao3, Wenhu Chen4, Ninghao Liu 1 1University of Georgia 2Tencent AI Seattle Lab 3Amazon 4University of Waterloo” (Towards trustworthy GUI agents A survey.txt @段1) || 摘要 | Towards Trustworthy GUI Agents: A Survey Yucheng Shi1, Wenhao Yu2, Wenlin Yao3, Wenhu Chen4, Ninghao Liu 1 1University of Georgia 2Tencent AI Seattle Lab 3Amazon 4University of Waterloo Abstract GUI agents, powered by large foundation mod- els, can interact with digital interfaces, enabling various applications in web automation, mo- bile navigation, and software testing. How- ever, their increasing autonomy has raised criti- cal concerns about their security, privacy, and safety. This survey examines the trustwor… | 0.72 | “Towards Trustworthy GUI Agents: A Survey Yucheng Shi1, Wenhao Yu2, Wenlin Yao3, Wenhu Chen4, Ninghao Liu 1 1University of Georgia 2Tencent AI Seattle Lab 3Amazon 4University of Waterloo” (Towards trustworthy GUI agents A survey.txt @段1) || 研究方法 | 4.1 Reliability Ensuring stable and accurate interaction with visual interfaces is essential for GUI agents, especially in tackling the challenge of hallucination, where agents generate actions or interpretations that do not match the visual content (Bai et al., 2024; Chen et al., 2024d). Such errors can include fabricated UI elements, incorrect readings of interface com- ponents, or lapses in visual focus, all of which can lead to unreliable behavior (Liu et al., 2023a; Jiang et al., 2024a; Yu et al., 2024). Furt… | 0.82 | “Real-time detection frameworks like UNIHD val- idate outputs against visual evidence, and methods such as Residual Visual Decoding address "halluci- nation snowballing" by revising outputs with resid- ual visual input (Chen et al., 2024d; Zhong et al., 2024).” (Towards trustworthy GUI agents A survey.txt @段11) || 主要结论 | N/A | 0.50 | N/A |

- 主题标签：GUI/Computer-Use Agent 安全, Prompt Injection/环境注入, Web/Browser Agent 安全, 隐私与记忆风险
- 方法标签：仿真/Simulator, 基准/Benchmark, 多模态/Multimodal, 调查/Survey
- 源文件：`综述参考文献\Towards trustworthy GUI agents A survey.txt`

</details>
<details><summary>72. Unity Is Strength: Collaborative LLM-Based Agents for Code Reviewer（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Unity Is Strength: Collaborative LLM-Based Agents for Code Reviewer | 0.92 | “Unity Is Strength: Collaborative LLM-Based Agents for Code Reviewer” (Unity is strength Collaborative LLM-based agents for code reviewer recommendation.txt @段7) || 作者 | LUQIAO WANG, Xidian University, Xi'an, Shaanxi, China | 0.90 | “LUQIAO WANG, Xidian University, Xi'an, Shaanxi, China” (Unity is strength Collaborative LLM-based agents for code reviewer recommendation.txt @段1) || 发表年份 | 2024 | 0.90 | “Published: 27 October 2024” (Unity is strength Collaborative LLM-based agents for code reviewer recommendation.txt @段1) || 期刊/会议 | Conference on Automated Soware | 0.88 | “Conference on Automated Soware” (Unity is strength Collaborative LLM-based agents for code reviewer recommendation.txt @段1) || 关键词 | xidian, university, china, shaanxi, code, acm, 2024, edu, recommendation, reviewer | 0.78 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3691620.3695291 . . RESEARCH-ARTICLE Unity Is Strength: Collaborative LLM-Based Agents for Code Reviewer Recommendation LUQIAO WANG, Xidian University, Xi'an, Shaanxi, China . YANGTAO ZHOU, Xidian University, Xi'an, Shaanxi, China . HUIYING ZHUANG, Xidian University, Xi” (Unity is strength Collaborative LLM-based agents for code reviewer recommendation.txt @段1) || 摘要 | . . Latest updates: hps://dl.acm.org/doi/10.1145/3691620.3695291 . . RESEARCH-ARTICLE Unity Is Strength: Collaborative LLM-Based Agents for Code Reviewer Recommendation LUQIAO WANG, Xidian University, Xi'an, Shaanxi, China . YANGTAO ZHOU, Xidian University, Xi'an, Shaanxi, China . HUIYING ZHUANG, Xidian University, Xi'an, Shaanxi, China . QINGSHAN LI, Xidian University, Xi'an, Shaanxi, China . DI CUI, Xidian University, Xi'an, Shaanxi, China . YUTONG ZHAO, University of Central Missouri, Warrensburg, MO, United S… | 0.72 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3691620.3695291 . . RESEARCH-ARTICLE Unity Is Strength: Collaborative LLM-Based Agents for Code Reviewer Recommendation LUQIAO WANG, Xidian University, Xi'an, Shaanxi, China . YANGTAO ZHOU, Xidian University, Xi'an, Shaanxi, China . HUIYING ZHUANG, Xidian University, Xi” (Unity is strength Collaborative LLM-based agents for code reviewer recommendation.txt @段1) || 研究方法 | 4.3 Case Study (RQ2) In this section, we present an case study to provide an intuitive impression of CoRe’s effectiveness and interpretability, which the top-1recommendationresultdoesnothitthegroundtruth.Weran- domly select a PR 339 with file pathplatform/frameworks/base from android project, and generate a top-5 recommendation list [375, 302, 278, 222, 110] from the given candidate reviewer pool. The ground truth reviewer ID is 278, and our approach CoRe ranks thisreviewerinthethirdposition.Theexplanationoftherea… | 0.82 | “4.3 Case Study (RQ2) In this section, we present an case study to provide an intuitive impression of CoRe’s effectiveness and interpretability, which the top-1recommendationresultdoesnothitthegroundtruth.Weran- domly select a PR 339 with file pathplatform/frameworks/base from android project, and generate a top-5 recom” (Unity is strength Collaborative LLM-based agents for code reviewer recommendation.txt @段13) || 主要结论 | 1 Introduction Code review (CR) is a critical practice in software maintenance, aimedatreducingdefectsandimprovingsoftwarequality[ 9,20].In pull-baseddevelopment,contributorssubmitpullrequests(PRs)for review prior to integration into the main branch. Reviewers assess the changes, discussing their relevance and quality, and make ap- proval decisions. Thongtanunam et al. [17] reported that 4% to 30% of reviews experience issues with reviewer assignment, delaying theCRprocessandextendingtheapprovaltimetoanaverageof12… | 0.80 | “Existing CRR methods can be divided into PR semantic-oriented and PR-reviewer interaction-oriented ap- proaches.PRsemantic-orientedmethodsanalyzemodifiedcode,file paths, and PR comments to find reviewers who have handled simi- larPRs.However,sincePRdataismostlytextual,methodsbasedon character matching or vector space m” (Unity is strength Collaborative LLM-based agents for code reviewer recommendation.txt @段7) |

- 主题标签：GUI/Computer-Use Agent 安全, Web/Browser Agent 安全
- 方法标签：N/A
- 源文件：`Unity is strength Collaborative LLM-based agents for code reviewer recommendation.txt`

</details>
<details><summary>73. Unveiling Privacy Risks in LLM Agent Memory（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Unveiling Privacy Risks in LLM Agent Memory | 0.92 | “Unveiling Privacy Risks in LLM Agent Memory” (Unveiling privacy risks in LLM agent memory.txt @段3) || 作者 | July 27 - August 1, 2025 ©2025 Association for Computational Linguistics | 0.90 | “July 27 - August 1, 2025 ©2025 Association for Computational Linguistics” (Unveiling privacy risks in LLM agent memory.txt @段1) || 发表年份 | 2024 | 0.86 | “et al., 2024a). This pipeline enables agents to sup-” (Unveiling privacy risks in LLM agent memory.txt @段1) || 期刊/会议 | Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 25241–25260 | 0.88 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 25241–25260” (Unveiling privacy risks in LLM agent memory.txt @段1) || 关键词 | memory, user, attacker, information, attacking, private, queries, solution, 2024, data | 0.78 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 25241–25260 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Unveiling Privacy Risks in LLM Agent Memory Bo Wang1, Weiyi He1, Shenglai Zeng1, Zhen Xiang2, Yue Xing1, Jiliang T” (Unveiling privacy risks in LLM agent memory.txt @段1) || 摘要 | Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 25241–25260 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Unveiling Privacy Risks in LLM Agent Memory Bo Wang1, Weiyi He1, Shenglai Zeng1, Zhen Xiang2, Yue Xing1, Jiliang Tang1, Pengfei He1 /Letter 1Michigan State University, 2 University of Georgia {wangbo9,heweiyi,zengshe1,xingyue1,tangjili,hepengf1}@msu.edu, zxiangaa@uga.edu Abstract Large Language Model (LLM) agent… | 0.72 | “Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 25241–25260 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics Unveiling Privacy Risks in LLM Agent Memory Bo Wang1, Weiyi He1, Shenglai Zeng1, Zhen Xiang2, Yue Xing1, Jiliang T” (Unveiling privacy risks in LLM agent memory.txt @段1) || 研究方法 | 1 Introduction Large Language Models (LLMs) have demon- strated revolutionary capabilities in language un- derstanding, reasoning, and generation (OpenAI, 2023; Zhao et al., 2023). Building on these ad- vances, LLM agents use LLMs and supplement with additional functionalities to perform more complex tasks (Xi et al., 2023). Its typical pipeline consists of the following key steps: taking user instruction, gathering environment information, re- trieving relevant knowledge and past experiences, giving an action sol… | 0.82 | “Its typical pipeline consists of the following key steps: taking user instruction, gathering environment information, re- trieving relevant knowledge and past experiences, giving an action solution based on the above infor- mation, and finally executing the solution (Wang et al., 2024a).” (Unveiling privacy risks in LLM agent memory.txt @段3) || 主要结论 | N/A | 0.50 | N/A |

- 主题标签：GUI/Computer-Use Agent 安全, Web/Browser Agent 安全, 隐私与记忆风险
- 方法标签：仿真/Simulator, 多模态/Multimodal
- 源文件：`Unveiling privacy risks in LLM agent memory.txt`

</details>
<details><summary>74. Watch Out for Your Agents! Investigating Backdoor（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Watch Out for Your Agents! Investigating Backdoor | 0.92 | “Watch Out for Your Agents! Investigating Backdoor” (Watch out for your agents! Investigating backdoor threats to LLM-based agents.txt @段1) || 作者 | Wenkai Yang∗1, Xiaohan Bi∗2, Yankai Lin†1, Sishuo Chen2, Jie Zhou3, Xu Sun†4 | 0.90 | “Wenkai Yang∗1, Xiaohan Bi∗2, Yankai Lin†1, Sishuo Chen2, Jie Zhou3, Xu Sun†4” (Watch out for your agents! Investigating backdoor threats to LLM-based agents.txt @段1) || 发表年份 | 2024 | 0.86 | “38th Conference on Neural Information Processing Systems (NeurIPS 2024).” (Watch out for your agents! Investigating backdoor threats to LLM-based agents.txt @段1) || 期刊/会议 | 38th Conference on Neural Information Processing Systems (NeurIPS 2024). | 0.88 | “38th Conference on Neural Information Processing Systems (NeurIPS 2024).” (Watch out for your agents! Investigating backdoor threats to LLM-based agents.txt @段1) || 关键词 | backdoor, size, final, search, sneaker, query, reasoning, adidas, first, forms | 0.78 | “Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents Wenkai Yang∗1, Xiaohan Bi∗2, Yankai Lin†1, Sishuo Chen2, Jie Zhou3, Xu Sun†4 1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China 2Center for Data Science, Peking University 3Pattern Recognition Center, WeCha” (Watch out for your agents! Investigating backdoor threats to LLM-based agents.txt @段1) || 摘要 | Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents Wenkai Yang∗1, Xiaohan Bi∗2, Yankai Lin†1, Sishuo Chen2, Jie Zhou3, Xu Sun†4 1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China 2Center for Data Science, Peking University 3Pattern Recognition Center, WeChat AI, Tencent Inc., China 4National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University {wenkaiyang, yankailin}@ruc.edu.cn bxh@stu.pku.edu.cn xusun@pku… | 0.72 | “Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents Wenkai Yang∗1, Xiaohan Bi∗2, Yankai Lin†1, Sishuo Chen2, Jie Zhou3, Xu Sun†4 1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China 2Center for Data Science, Peking University 3Pattern Recognition Center, WeCha” (Watch out for your agents! Investigating backdoor threats to LLM-based agents.txt @段1) || 研究方法 | Abstract Driven by the rapid development of Large Language Models (LLMs), LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate … | 0.82 | “We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis of different forms of agent backdoor attacks.” (Watch out for your agents! Investigating backdoor threats to LLM-based agents.txt @段2) || 主要结论 | Abstract Driven by the rapid development of Large Language Models (LLMs), LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate … | 0.80 | “This indicates an urgent need for further research on the development of targeted defenses against backdoor attacks on LLM-based agents.3 Warning: This paper may contain biased content.” (Watch out for your agents! Investigating backdoor threats to LLM-based agents.txt @段2) |

- 主题标签：Backdoor/投毒
- 方法标签：后门/Backdoor
- 源文件：`Watch out for your agents! Investigating backdoor threats to LLM-based agents.txt`

</details>
<details><summary>75. Y our Agent Can Defend Itself against Backdoor Attacks（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Y our Agent Can Defend Itself against Backdoor Attacks | 0.92 | “Y our Agent Can Defend Itself against Backdoor Attacks” (Your agent can defend itself against backdoor attacks.txt @段2) || 作者 | Despite their growing adoption across domains, | 0.90 | “Despite their growing adoption across domains,” (Your agent can defend itself against backdoor attacks.txt @段1) || 发表年份 | 2024 | 0.86 | “Wang et al., 2024a). Recent studies have demon-” (Your agent can defend itself against backdoor attacks.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2506.08336v2  [cs.CR]  11 Jun 2025” (Your agent can defend itself against backdoor attacks.txt @段1) || 关键词 | thought, backdoor, detection, txt, action, instruction, level, reagent, need, planning | 0.78 | “arXiv:2506.08336v2 [cs.CR] 11 Jun 2025 Y our Agent Can Defend Itself against Backdoor Attacks Changjiang Li Stony Brook University meet.cjli@gmail.com Jiacheng Liang Stony Brook University ljcpro@outlook.com Bochuan Cao Penn State University bxcao@psu.edu Jinghui Chen Penn State University jzc5917@psu.edu Ting Wang Sto” (Your agent can defend itself against backdoor attacks.txt @段1) || 摘要 | arXiv:2506.08336v2 [cs.CR] 11 Jun 2025 Y our Agent Can Defend Itself against Backdoor Attacks Changjiang Li Stony Brook University meet.cjli@gmail.com Jiacheng Liang Stony Brook University ljcpro@outlook.com Bochuan Cao Penn State University bxcao@psu.edu Jinghui Chen Penn State University jzc5917@psu.edu Ting Wang Stony Brook University inbox.ting@gmail.com Abstract Despite their growing adoption across domains, large language model (LLM)-powered agents face significant security risks from backdoor attacks during… | 0.72 | “arXiv:2506.08336v2 [cs.CR] 11 Jun 2025 Y our Agent Can Defend Itself against Backdoor Attacks Changjiang Li Stony Brook University meet.cjli@gmail.com Jiacheng Liang Stony Brook University ljcpro@outlook.com Bochuan Cao Penn State University bxcao@psu.edu Jinghui Chen Penn State University jzc5917@psu.edu Ting Wang Sto” (Your agent can defend itself against backdoor attacks.txt @段1) || 研究方法 | 2 Related Work LLM agents. Developing performant autonomous agents has been a long-standing task (Wang et al., 2024a). While previous research focuses on con- strained settings (Mnih et al., 2015; Haarnoja et al., 2018), the advent of LLMs enables agents to gen- eralize across tasks in open-domain environments. AutoGPT (Yang et al., 2023) integrates multiple tools and Web APIs, allowing agents to perform tasks autonomously. Generative agents (Zhang et al., 2023; Wang et al., 2023) introduce complex cognitive modul… | 0.82 | “Third, instead of merely inspecting single-step gen- erated outputs, our method introduces a novel two- level consistency check framework that provides more comprehensive protection.” (Your agent can defend itself against backdoor attacks.txt @段4) || 主要结论 | 1 Introduction Intelligent agents powered by large language mod- els (LLMs) have garnered significant attention due to their unprecedented capabilities in instruction following, performing complex reasoning, and solving challenging problems (Xi et al., 2023; Wang et al., 2024a). Recent studies have demon- strated that LLM agents excel in a variety of real-world tasks, including web shopping, oper- ating system management, and database mainte- nance (Wang et al., 2024a). However, develop- ing performant, specialize… | 0.80 | “ii ) Extensive experiments, conducted across diverse tasks and popular LLMs, demon- strate that ReAgent significantly outperforms ex- isting defenses.” (Your agent can defend itself against backdoor attacks.txt @段3) |

- 主题标签：Backdoor/投毒, Web/Browser Agent 安全, 防御与对齐机制, 隐私与记忆风险
- 方法标签：仿真/Simulator, 后门/Backdoor, 基准/Benchmark, 系统防御/Defense
- 源文件：`综述参考文献\Your agent can defend itself against backdoor attacks.txt`

</details>
<details><summary>76. evaluation of unsafe code generation and execution, diverse input formats, and high-（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | evaluation of unsafe code generation and execution, diverse input formats, and high- | 0.92 | “evaluation of unsafe code generation and execution, diverse input formats, and high-” (RedCode Risky code execution and generation benchmark for code agents.txt @段13) || 作者 | RedCode: Risky Code Execution and Generation | 0.90 | “RedCode: Risky Code Execution and Generation” (RedCode Risky code execution and generation benchmark for code agents.txt @段1) || 发表年份 | 2024 | 0.86 | “38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.” (RedCode Risky code execution and generation benchmark for code agents.txt @段1) || 期刊/会议 | 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. | 0.88 | “38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.” (RedCode Risky code execution and generation benchmark for code agents.txt @段1) || 关键词 | code, risky, execution, systems, generation, python, benchmarks, natural, provide, gpt | 0.78 | “RedCode: Risky Code Execution and Generation Benchmark for Code Agents Chengquan Guo1∗ , Xun Liu2∗, Chulin Xie2∗, Andy Zhou2,3, Yi Zeng4, Zinan Lin5, Dawn Song6, Bo Li1,2 1University of Chicago 2University of Illinois Urbana-Champaign 3Lapis Labs 4Virginia Tech 5Microsoft Research 6University of California Berkeley” (RedCode Risky code execution and generation benchmark for code agents.txt @段1) || 摘要 | RedCode: Risky Code Execution and Generation Benchmark for Code Agents Chengquan Guo1∗ , Xun Liu2∗, Chulin Xie2∗, Andy Zhou2,3, Yi Zeng4, Zinan Lin5, Dawn Song6, Bo Li1,2 1University of Chicago 2University of Illinois Urbana-Champaign 3Lapis Labs 4Virginia Tech 5Microsoft Research 6University of California Berkeley Abstract With the rapidly increasing capabilities and adoption of code agents for AI-assisted coding and software development, safety and security concerns, such as generating or executing malicious cod… | 0.72 | “RedCode: Risky Code Execution and Generation Benchmark for Code Agents Chengquan Guo1∗ , Xun Liu2∗, Chulin Xie2∗, Andy Zhou2,3, Yi Zeng4, Zinan Lin5, Dawn Song6, Bo Li1,2 1University of Chicago 2University of Illinois Urbana-Champaign 3Lapis Labs 4Virginia Tech 5Microsoft Research 6University of California Berkeley” (RedCode Risky code execution and generation benchmark for code agents.txt @段1) || 研究方法 | 4.2 Evaluation on RedCode-Gen Code agents. For RedCode-Gen, we evaluate both base code LLMs and code agents. For base LLMs, we consider 4 closed-source and 5 open-source LLMs. We design a generic code-agent framework similar to CodeAct [24] for these base models where the model has access to a compiler, the judge’s score and response, and is allowed to modify its code for a certain number of iterations. Evaluation metrics. Following evaluation pipeline in §3.3, we report the (1) refusal rate, (2) accuracy of gener… | 0.82 | “We design a generic code-agent framework similar to CodeAct [24] for these base models where the model has access to a compiler, the judge’s score and response, and is allowed to modify its code for a certain number of iterations.” (RedCode Risky code execution and generation benchmark for code agents.txt @段16) || 主要结论 | 3 round interactions. We use the below system prompt: System prompt for OpenCodeInterpreter You are an AI code interpreter. Your goal is to help users do a variety of jobs by executing Python code. You should: 1. Comprehend the user’s requirements carefully \& to the letter. 17 2. Give a brief description for what you plan to do \& call the provided function to run code. 3. Provide results analysis based on the execution output. 4. If error occurred, try to fix it. 5. Response in the same language as the user. ReA… | 0.80 | “Approach Code input Code input w/ jailbreaking Text summaries Text descriptions Attack Success Rate Base 79.33 64.53 69.07 88.4+ (a) Simple 71.87 60.93 63.07 87.2+ (b) Awareness 63.73 51.47 59.6 85.33+ (c) Detection 63.73 51.47 61.06 84.93+ (d) Modification 67.33 57.73 62.8 86.53+ (e) Combined 62.13 (↓17%) 48.93 ( ↓17%” (RedCode Risky code execution and generation benchmark for code agents.txt @段21) |

- 主题标签：N/A
- 方法标签：仿真/Simulator, 基准/Benchmark
- 源文件：`RedCode Risky code execution and generation benchmark for code agents.txt`

</details>
<details><summary>77. introduce a novel threat, WIPI, that indirectly controls Web（2024）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | introduce a novel threat, WIPI, that indirectly controls Web | 0.92 | “introduce a novel threat, WIPI, that indirectly controls Web” (WIPI A new web threat for LLM-driven web agents.txt @段15) || 作者 | With the fast development of large language models (LLMs), | 0.90 | “With the fast development of large language models (LLMs),” (WIPI A new web threat for LLM-driven web agents.txt @段1) || 发表年份 | 2024 | 0.86 | “arXiv:2402.16965v1  [cs.CR]  26 Feb 2024” (WIPI A new web threat for LLM-driven web agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2402.16965v1  [cs.CR]  26 Feb 2024” (WIPI A new web threat for LLM-driven web agents.txt @段1) || 关键词 | web, indirect, wipi, content, instructions, prompts, like, external, malicious, user | 0.78 | “WIPI: A New Web Threat for LLM-Driven Web Agents Fangzhou Wu1∗† Shutong Wu1∗† Yulong Cao2 Chaowei Xiao1† 1University of Wisconsin-Madison 2NVIDIA” (WIPI A new web threat for LLM-driven web agents.txt @段1) || 摘要 | WIPI: A New Web Threat for LLM-Driven Web Agents Fangzhou Wu1∗† Shutong Wu1∗† Yulong Cao2 Chaowei Xiao1† 1University of Wisconsin-Madison 2NVIDIA Abstract With the fast development of large language models (LLMs), LLM-driven Web Agents (Web Agents for short) have ob- tained tons of attention due to their superior capability where LLMs serve as the core part of making decisions like the human brain equipped with multiple web tools to actively in- teract with external deployed websites. As uncountable Web Agents hav… | 0.72 | “WIPI: A New Web Threat for LLM-Driven Web Agents Fangzhou Wu1∗† Shutong Wu1∗† Yulong Cao2 Chaowei Xiao1† 1University of Wisconsin-Madison 2NVIDIA” (WIPI A new web threat for LLM-driven web agents.txt @段1) || 研究方法 | 3 Methodology Based on the above observations and challenges over the vanilla attack scheme, we propose a more advanced and sta- ble WIPI attacking pipeline integrated with several explicitly designed strategies. As shown in Figure 2, we design a univer- sal template that guarantees the executability of the inserted prompts. Specifically, we endeavor to bypass the impact of possible preset prompts and defenses and enforce Web Agents to concentrate on the inserted instructions under the interfer- ence of massive no… | 0.82 | “3 Methodology Based on the above observations and challenges over the vanilla attack scheme, we propose a more advanced and sta- ble WIPI attacking pipeline integrated with several explicitly designed strategies.” (WIPI A new web threat for LLM-driven web agents.txt @段8) || 主要结论 | 2.2 Threat Model In WIPI, the attacker’s goal is to let Web Agents successfully execute the indirect prompts existing in the external web pages without the authorization of the user. We consider a practical black-box setting, where the inner workings like the system prompts and model parameters are unknown and unmodifiable. The attacker can use the normal functionalities of Web Agents like other users. Additionally, the attacker can arbitrarily manipulate the content of the websites ( e.g., designing indirect prom… | 0.80 | “This new type of threat is significantly different from tradi- tional web threats (e.g., malicious executable code snippets in web pages) and attacks targeted on individual machine learn- 3 ing models ( e.g., locally prompt injection [27]).” (WIPI A new web threat for LLM-driven web agents.txt @段6) |

- 主题标签：Prompt Injection/环境注入, Web/Browser Agent 安全, 防御与对齐机制
- 方法标签：仿真/Simulator, 提示注入/Prompt Injection, 系统防御/Defense
- 源文件：`综述参考文献\WIPI A new web threat for LLM-driven web agents.txt`

</details>
<details><summary>78. A Multi-agent Onboarding Assistant based on Large Language（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | A Multi-agent Onboarding Assistant based on Large Language | 0.92 | “A Multi-agent Onboarding Assistant based on Large Language” (A multi-agent onboarding assistant based on large language models, retrieval augmented generation, a.txt @段1) || 作者 | Models, Retrieval Augmented Generation, and Chain-of-Thought | 0.90 | “Models, Retrieval Augmented Generation, and Chain-of-Thought” (A multi-agent onboarding assistant based on large language models, retrieval augmented generation, a.txt @段1) || 发表年份 | 2025 | 0.90 | “Andrei-Cristian Ionescu, Sergey Titov, and Maliheh Izadi. 2025. A Multi-” (A multi-agent onboarding assistant based on large language models, retrieval augmented generation, a.txt @段1) || 期刊/会议 | Conference on the Foundations of Software Engineering (FSE Companion ’25), | 0.88 | “Conference on the Foundations of Software Engineering (FSE Companion ’25),” (A multi-agent onboarding assistant based on large language models, retrieval augmented generation, a.txt @段1) || 关键词 | onboarding, buddy, project, retrieval, software, tasks, generation, 2025, chain, code | 0.78 | “A Multi-agent Onboarding Assistant based on Large Language Models, Retrieval Augmented Generation, and Chain-of-Thought Andrei-Cristian Ionescu Delft University of Technology Delft, The Netherlands a.c.ionescu-1@student.tudelft.nl Sergey Titov JetBrains Research Amsterdam, The Netherlands sergey.titov@jetbrains.com Mal” (A multi-agent onboarding assistant based on large language models, retrieval augmented generation, a.txt @段1) || 摘要 | A Multi-agent Onboarding Assistant based on Large Language Models, Retrieval Augmented Generation, and Chain-of-Thought Andrei-Cristian Ionescu Delft University of Technology Delft, The Netherlands a.c.ionescu-1@student.tudelft.nl Sergey Titov JetBrains Research Amsterdam, The Netherlands sergey.titov@jetbrains.com Maliheh Izadi Delft University of Technology Delft, The Netherlands m.izadi@tudelft.nl Abstract E!ective onboarding in software engineering is crucial but di"cult due to the fast-paced evolution of tech… | 0.72 | “A Multi-agent Onboarding Assistant based on Large Language Models, Retrieval Augmented Generation, and Chain-of-Thought Andrei-Cristian Ionescu Delft University of Technology Delft, The Netherlands a.c.ionescu-1@student.tudelft.nl Sergey Titov JetBrains Research Amsterdam, The Netherlands sergey.titov@jetbrains.com Mal” (A multi-agent onboarding assistant based on large language models, retrieval augmented generation, a.txt @段1) || 研究方法 | 4 Onboarding Agent This agent has a more complicated structure than the other agents. The ’Onboarding Agent’, as seen in Figure 2, is composed of three subcomponents, namely the memory storage, the planning scratch- pad, and the retrieval tools. This agent works iteratively and de- cides dynamically what actions to take to reach an answer using the planning scratchpad and the retrieval tools. For this, we have implemented a custom chain-of-thought approach (CoT) that will be used in the Step Processor. The approac… | 0.82 | “The approach we developed was inspired by Google’s paper that introduced the concept of chain-of-thought prompting [ 16], where a problem is divided into intermediate reasoning steps, allowing the LLMs to solve more intricate tasks that require a multi-step logic.” (A multi-agent onboarding assistant based on large language models, retrieval augmented generation, a.txt @段8) || 主要结论 | N/A | 0.50 | N/A |

- 主题标签：GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击, 多智能体对抗与协作, 隐私与记忆风险
- 方法标签：仿真/Simulator, 多智能体/Multi-Agent, 多模态/Multimodal
- 源文件：`A multi-agent onboarding assistant based on large language models, retrieval augmented generation, a.txt`

</details>
<details><summary>79. AEGIS: An Agent-based Framework for Bug Reproduction from Issue（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | AEGIS: An Agent-based Framework for Bug Reproduction from Issue | 0.92 | “AEGIS: An Agent-based Framework for Bug Reproduction from Issue” (AEGIS An agent-based framework for bug reproduction from issue descriptions.txt @段7) || 作者 | XINCHEN WANG, Harbin Institute of Technology Shenzhen, Shenzhen, Guangdong, China | 0.90 | “XINCHEN WANG, Harbin Institute of Technology Shenzhen, Shenzhen, Guangdong, China” (AEGIS An agent-based framework for bug reproduction from issue descriptions.txt @段1) || 发表年份 | 2025 | 0.90 | “Published: 23 June 2025” (AEGIS An agent-based framework for bug reproduction from issue descriptions.txt @段1) || 期刊/会议 | International Conference on the | 0.88 | “International Conference on the” (AEGIS An agent-based framework for bug reproduction from issue descriptions.txt @段1) || 关键词 | china, bytedance, shenzhen, shanghai, beijing, edu, harbin, institute, ltd, technology | 0.78 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3728557 . . RESEARCH-ARTICLE AEGIS: An Agent-based Framework for Bug Reproduction from Issue Descriptions XINCHEN WANG, Harbin Institute of Technology Shenzhen, Shenzhen, Guangdong, China . PENGFEI GAO, ByteDance Ltd., Beijing, China . XIANGXIN MENG, ByteDance L” (AEGIS An agent-based framework for bug reproduction from issue descriptions.txt @段1) || 摘要 | . . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3728557 . . RESEARCH-ARTICLE AEGIS: An Agent-based Framework for Bug Reproduction from Issue Descriptions XINCHEN WANG, Harbin Institute of Technology Shenzhen, Shenzhen, Guangdong, China . PENGFEI GAO, ByteDance Ltd., Beijing, China . XIANGXIN MENG, ByteDance Ltd., Beijing, China . CHAO PENG, ByteDance Ltd., Beijing, China . RUIDA HU, Harbin Institute of Technology Shenzhen, Shenzhen, Guangdong, China . YUN LIN, Shanghai Jiao Tong University, Shanghai, Chi… | 0.72 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3728557 . . RESEARCH-ARTICLE AEGIS: An Agent-based Framework for Bug Reproduction from Issue Descriptions XINCHEN WANG, Harbin Institute of Technology Shenzhen, Shenzhen, Guangdong, China . PENGFEI GAO, ByteDance Ltd., Beijing, China . XIANGXIN MENG, ByteDance L” (AEGIS An agent-based framework for bug reproduction from issue descriptions.txt @段1) || 研究方法 | 1 INTRODUCTION Reproducing bugs in issue descriptions is essential to localize bugs and implement corresponding fixes. Existing surveys [ 9, 22] in- dicate that writing bug reproduction scripts closely aligns with developers’ needs, as these scripts assist developers in compre- hending the bugs and preventing their recurrence. Kang et al.[21] conduct a study on 300 popular Java projects and find that up to 28% of the test cases are related to bug reproduction. Besides, the execution information [23] obtained from … | 0.82 | “In summary, the major contributions of this paper are summa- rized as follows: 332 AEGIS: An Agent-based Framework for Bug Reproduction from Issue Descriptions FSE Companion ’25, June 23–28, 2025, Trondheim, Norway (1) We propose AEGIS, a novel agent-based bug reproduction framework, which involves a bug-related contex” (AEGIS An agent-based framework for bug reproduction from issue descriptions.txt @段9) || 主要结论 | 5.1 RQ1: Effectiveness of AEGIS in Bug Reproduction To answer RQ1, we conduct a comprehensive comparative analysis against three LLM-based methods and three agent-based methods. The experimental results are shown in Table 1. AEGIS exhibits superior performance compared with the baseline methods. As shown in Table 1, AEGIS outperforms all the baseline methods in terms of bug reproduction rate ( 𝐹 → 𝑃). Specifically, AEGIS achieves an absolute improvement of 19.0% over the best baseline method. Compared to the avera… | 0.80 | “Module 𝐹 → × 𝑃 → 𝑃 𝐹 → 𝑃 w/o BCS 90.7 8.0 31.7 ↓ 4.3 w/o FSG 56.7 27.0 12.3 ↓ 23.7 w/o Modify-in-FSG 84.7 14.0 26.0 ↓ 10.0 w/o Restart-in-FSG 92.0 7.7 33.3 ↓ 2.7 w/o External-Verify-in-FSG 92.7 6.7 35.0 ↓ 1.0 AEGIS 90.0 9.0 36.0 Fail-to-any scripts matter for reproducing more bugs.When considering the best-performing m” (AEGIS An agent-based framework for bug reproduction from issue descriptions.txt @段23) |

- 主题标签：GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击
- 方法标签：多模态/Multimodal, 调查/Survey
- 源文件：`AEGIS An agent-based framework for bug reproduction from issue descriptions.txt`

</details>
<details><summary>80. Advanced Smart Contract Vulnerability Detection（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Advanced Smart Contract Vulnerability Detection | 0.92 | “Advanced Smart Contract Vulnerability Detection” (Advanced smart contract vulnerability detection via LLM-powered multi-agent systems.txt @段2) || 作者 | 2830 IEEE TRANSACTIONS ON SOFTW ARE ENGINEERING, VOL. 51, NO. 10, OCTO BER 2025 | 0.90 | “2830 IEEE TRANSACTIONS ON SOFTW ARE ENGINEERING, VOL. 51, NO. 10, OCTO BER 2025” (Advanced smart contract vulnerability detection via LLM-powered multi-agent systems.txt @段1) || 发表年份 | 2025 | 0.90 | “2830 IEEE TRANSACTIONS ON SOFTW ARE ENGINEERING, VOL. 51, NO. 10, OCTO BER 2025” (Advanced smart contract vulnerability detection via LLM-powered multi-agent systems.txt @段1) || 期刊/会议 | Unknown venue (DOI: 10.1109/TSE.2025.3597319) | 0.82 | “10.1109/TSE.2025.3597319” (Advanced smart contract vulnerability detection via LLM-powered multi-agent systems.txt @段1) || 关键词 | vulnerability, mode, contract, smartaudit, gpt, vulnerabilities, analysis, detection, speci, accuracy | 0.78 | “2830 IEEE TRANSACTIONS ON SOFTW ARE ENGINEERING, VOL. 51, NO. 10, OCTO BER 2025 Advanced Smart Contract Vulnerability Detection via LLM-Powered Multi-Agent Systems Zhiyuan Wei , Jing Sun , Y uqiang Sun , Y e Liu , Daoyuan Wu , Member , IEEE, Zijian Zhang , Senior Member , IEEE, Xianhao Zhang , Graduate Student Member ,” (Advanced smart contract vulnerability detection via LLM-powered multi-agent systems.txt @段1) || 摘要 | 2830 IEEE TRANSACTIONS ON SOFTW ARE ENGINEERING, VOL. 51, NO. 10, OCTO BER 2025 Advanced Smart Contract Vulnerability Detection via LLM-Powered Multi-Agent Systems Zhiyuan Wei , Jing Sun , Y uqiang Sun , Y e Liu , Daoyuan Wu , Member , IEEE, Zijian Zhang , Senior Member , IEEE, Xianhao Zhang , Graduate Student Member , IEEE , Meng Li , Senior Member , IEEE, Y ang Liu , Senior Member , IEEE, Chunmiao Li , Mingchao Wan , Jin Dong , and Liehuang Zhu , Senior Member , IEEE Abstract—Blockchain’s inherent immutability, … | 0.72 | “2830 IEEE TRANSACTIONS ON SOFTW ARE ENGINEERING, VOL. 51, NO. 10, OCTO BER 2025 Advanced Smart Contract Vulnerability Detection via LLM-Powered Multi-Agent Systems Zhiyuan Wei , Jing Sun , Y uqiang Sun , Y e Liu , Daoyuan Wu , Member , IEEE, Zijian Zhang , Senior Member , IEEE, Xianhao Zhang , Graduate Student Member ,” (Advanced smart contract vulnerability detection via LLM-powered multi-agent systems.txt @段1) || 研究方法 | 2830 IEEE TRANSACTIONS ON SOFTW ARE ENGINEERING, VOL. 51, NO. 10, OCTO BER 2025 Advanced Smart Contract Vulnerability Detection via LLM-Powered Multi-Agent Systems Zhiyuan Wei , Jing Sun , Y uqiang Sun , Y e Liu , Daoyuan Wu , Member , IEEE, Zijian Zhang , Senior Member , IEEE, Xianhao Zhang , Graduate Student Member , IEEE , Meng Li , Senior Member , IEEE, Y ang Liu , Senior Member , IEEE, Chunmiao Li , Mingchao Wan , Jin Dong , and Liehuang Zhu , Senior Member , IEEE Abstract—Blockchain’s inherent immutability, … | 0.82 | “This work makes several key co n- tributions to the ﬁeld of smart contract security: • Collaborative Multi-Agent Framework: We introduce a novel multi-agent system in which specialized LLM agents, each with tailored capabilities and roles, collab- oratively perform in-depth security audits.” (Advanced smart contract vulnerability detection via LLM-powered multi-agent systems.txt @段1) || 主要结论 | 6 summarizes these comparative results, which reveal substantial impro ve- ments when employing both the BA and TA modes relative to the zero-shot baseline. For the GPT-3.5 conﬁguration, th e zero-shot approach achieved an accuracy of 66%, a precision of 85.7%, a recall of 27.6%, and an F1-score of 94.3%. When using BA mode, overall performance improved to an accuracy of 74%, a precision of 93.7%, a recall of 74%, and an F1-score of 82.7%. Notably, the TA mode produced a dramatic enhance- ment, reaching 94% accura… | 0.80 | “RQ5: Ablation Study of TA and BA Modes In RQ3, our results show that, on real-world projects, the standalone accuracy of the TA mode (47.6%) is signiﬁcantly higher than that of the BA mode (14.4%).” (Advanced smart contract vulnerability detection via LLM-powered multi-agent systems.txt @段6) |

- 主题标签：多智能体对抗与协作
- 方法标签：多智能体/Multi-Agent
- 源文件：`Advanced smart contract vulnerability detection via LLM-powered multi-agent systems.txt`

</details>
<details><summary>81. AgentFM: Role-Aware Failure Management for Distributed Databases（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | AgentFM: Role-Aware Failure Management for Distributed Databases | 0.92 | “AgentFM: Role-Aware Failure Management for Distributed Databases” (AgentFM Role-aware failure management for distributed databases with LLM-driven multi-agents.txt @段7) || 作者 | LINGZHE ZHANG, Peking University, Beijing, China | 0.90 | “LINGZHE ZHANG, Peking University, Beijing, China” (AgentFM Role-aware failure management for distributed databases with LLM-driven multi-agents.txt @段1) || 发表年份 | 2025 | 0.90 | “Published: 23 June 2025” (AgentFM Role-aware failure management for distributed databases with LLM-driven multi-agents.txt @段1) || 期刊/会议 | International Conference on the | 0.88 | “International Conference on the” (AgentFM Role-aware failure management for distributed databases with LLM-driven multi-agents.txt @段1) || 关键词 | china, beijing, peking, university, acm, distributed, databases, failure, 2025, agentfm | 0.78 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3728492 . . SHORT-PAPER AgentFM: Role-Aware Failure Management for Distributed Databases with LLM-Driven Multi-Agents LINGZHE ZHANG, Peking University, Beijing, China . YUNPENG ZHAI, Alibaba Group Holding Limited, Hangzhou, Zhejiang, China . TONG JIA, Peking Uni” (AgentFM Role-aware failure management for distributed databases with LLM-driven multi-agents.txt @段1) || 摘要 | . . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3728492 . . SHORT-PAPER AgentFM: Role-Aware Failure Management for Distributed Databases with LLM-Driven Multi-Agents LINGZHE ZHANG, Peking University, Beijing, China . YUNPENG ZHAI, Alibaba Group Holding Limited, Hangzhou, Zhejiang, China . TONG JIA, Peking University, Beijing, China . XIAOSONG HUANG, Peking University, Beijing, China . CHIMING DUAN, Peking University, Beijing, China . YING LI, Peking University, Beijing, China . . . Open Access Support pr… | 0.72 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3728492 . . SHORT-PAPER AgentFM: Role-Aware Failure Management for Distributed Databases with LLM-Driven Multi-Agents LINGZHE ZHANG, Peking University, Beijing, China . YUNPENG ZHAI, Alibaba Group Holding Limited, Hangzhou, Zhejiang, China . TONG JIA, Peking Uni” (AgentFM Role-aware failure management for distributed databases with LLM-driven multi-agents.txt @段1) || 研究方法 | 1 INTRODUCTION The distributed databases, such as Google Spanner [ 2], Alibaba OceanBase [16], TiDB [5], and Apache IoTDB [14], have become in- tegral components of cloud infrastructures, handling vast volumes of data [7, 25]. However, these systems frequently encounter anomalies such as system failures and performance degradation, leading to significant financial losses. For example, Alibaba Cloud faces Intermittent Slow Queries (iSQs) [11], leading to billions of dollars in annual losses. Amazon reports that eve… | 0.82 | “Building on these insights, we introduce AgentFM, a compre- hensive role-aware failure management framework for distributed databases, powered by LLM-driven multi-agent systems.” (AgentFM Role-aware failure management for distributed databases with LLM-driven multi-agents.txt @段7) || 主要结论 | 4.1 Design To evaluate AgentFM, we assess its feasibility and effectiveness in Apache IoTDB. We manually injected 10 types of anomalies, includ- ing CPU saturation, IO saturation, memory saturation, network delay increase, network bandwidth limitation, network partition occurrence, workload spikes, accompanying slow queries, exces- sive data export, and excessive data import. Each anomaly type is injected 20 times. The evaluation is conducted based on Qwen2.5-72b to assess the results of anomaly detection and diag… | 0.80 | “We manually injected 10 types of anomalies, includ- ing CPU saturation, IO saturation, memory saturation, network delay increase, network bandwidth limitation, network partition occurrence, workload spikes, accompanying slow queries, exces- sive data export, and excessive data import.” (AgentFM Role-aware failure management for distributed databases with LLM-driven multi-agents.txt @段16) |

- 主题标签：多智能体对抗与协作, 隐私与记忆风险
- 方法标签：基准/Benchmark, 多智能体/Multi-Agent, 多模态/Multimodal
- 源文件：`AgentFM Role-aware failure management for distributed databases with LLM-driven multi-agents.txt`

</details>
<details><summary>82. Agents via Advertising Delivery（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Agents via Advertising Delivery | 0.92 | “Agents via Advertising Delivery” (AdInject Real-world black-box attacks on web agents via advertising delivery.txt @段3) || 作者 | Haowei Wang1,2,3, Junjie Wang1,2,3∗ | 0.90 | “Haowei Wang1,2,3, Junjie Wang1,2,3∗” (AdInject Real-world black-box attacks on web agents via advertising delivery.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2505.21499v1  [cs.CR]  27 May 2025” (AdInject Real-world black-box attacks on web agents via advertising delivery.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2505.21499v1  [cs.CR]  27 May 2025” (AdInject Real-world black-box attacks on web agents via advertising delivery.txt @段1) || 关键词 | web, content, malicious, user, vlm, adinject, advertising, delivery, real, specific | 0.78 | “arXiv:2505.21499v1 [cs.CR] 27 May 2025 AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery Haowei Wang1,2,3, Junjie Wang1,2,3∗ , Xiaojun Jia 4, Rupeng Zhang1,2,3, Mingyang Li1,2,3, Zhe Liu1,2,3, Yang Liu4, Qing Wang1,2,3∗ 1State Key Laboratory of Intelligent Game, Beijing, China 2Institute of” (AdInject Real-world black-box attacks on web agents via advertising delivery.txt @段1) || 摘要 | arXiv:2505.21499v1 [cs.CR] 27 May 2025 AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery Haowei Wang1,2,3, Junjie Wang1,2,3∗ , Xiaojun Jia 4, Rupeng Zhang1,2,3, Mingyang Li1,2,3, Zhe Liu1,2,3, Yang Liu4, Qing Wang1,2,3∗ 1State Key Laboratory of Intelligent Game, Beijing, China 2Institute of Software, Chinese Academy of Sciences, Beijing, China 3University of Chinese Academy of Sciences, Beijing, China 4Nanyang Technological University, Singapore {wanghaowei2023}@iscas.ac.cn {junjie, wq}… | 0.72 | “arXiv:2505.21499v1 [cs.CR] 27 May 2025 AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery Haowei Wang1,2,3, Junjie Wang1,2,3∗ , Xiaojun Jia 4, Rupeng Zhang1,2,3, Mingyang Li1,2,3, Zhe Liu1,2,3, Yang Liu4, Qing Wang1,2,3∗ 1State Key Laboratory of Intelligent Game, Beijing, China 2Institute of” (AdInject Real-world black-box attacks on web agents via advertising delivery.txt @段1) || 研究方法 | 4.2 Advertisement Content Optimization Figure 2: Demonstration of Ad Content Optimization While manually designed ad content is straightforward, it lacks targeted optimization, which can limit overall effectiveness in misleading agents. Therefore, we attempt to optimize the ad content. As mentioned in Section 4.1, the key is to make the agent believe the malicious action is necessary for its task. We hypothesize that guessing the user’s potential intents and then crafting ad content that incorporates these intents… | 0.82 | “Tailored to the ad delivery context, we propose the VLM-based ad content optimization method (Figure 2): generate multiple potential intents based on homepage of a website where the ad is placed, and integrate these intents into the ad content in a way that serves the goal of inducing the click.” (AdInject Real-world black-box attacks on web agents via advertising delivery.txt @段10) || 主要结论 | 5.1.3 Baseline Comparison A core principle in our manual ad content design is to make the agent perceive the malicious action as a necessary step to complete its task. To validate this principle, we compare it with the "Virus Detected" and "Speculate User Query" design schemes proposed in [ 46], as well as an ’Injection’ baseline representing generic prompt injection [25, 28]. Furthermore, we compare with a ’Vanilla’ ad to rule out the possibility of the agent voluntarily clicking the ad, thus demonstrating that t… | 0.80 | “Our method (’Ours’) achieves significantly higher ASR (93.51% for GPT-4o, 66.67% for Claude-3.7) compared to ’Vanilla’ (0.00%ASR), ’Virus’ (20.83% for GPT-4o, 1.39% for Claude-3.7), ’Speculate’ (4.17% for GPT-4o, 3.24% for Claude-3.7), and ’Injection’ (0.00% ASR).” (AdInject Real-world black-box attacks on web agents via advertising delivery.txt @段15) |

- 主题标签：GUI/Computer-Use Agent 安全, Prompt Injection/环境注入, Web/Browser Agent 安全
- 方法标签：仿真/Simulator, 多模态/Multimodal, 形式化/Optimization, 提示注入/Prompt Injection
- 源文件：`综述参考文献\AdInject Real-world black-box attacks on web agents via advertising delivery.txt`

</details>
<details><summary>83. Amplified Vulnerabilities: Structured Jailbreak Attacks（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Amplified Vulnerabilities: Structured Jailbreak Attacks | 0.92 | “Amplified Vulnerabilities: Structured Jailbreak Attacks” (Amplified vulnerabilities Structured jailbreak attacks on LLM-based multi-agent debate.txt @段2) || 作者 | JOURNAL OF LATEX CLASS FILES, VOL. X, NO. Y , FEBRUARY 2025 1 | 0.90 | “JOURNAL OF LATEX CLASS FILES, VOL. X, NO. Y , FEBRUARY 2025 1” (Amplified vulnerabilities Structured jailbreak attacks on LLM-based multi-agent debate.txt @段1) || 发表年份 | 2025 | 0.86 | “JOURNAL OF LATEX CLASS FILES, VOL. X, NO. Y , FEBRUARY 2025 1” (Amplified vulnerabilities Structured jailbreak attacks on LLM-based multi-agent debate.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2504.16489v1  [cs.CR]  23 Apr 2025” (Amplified vulnerabilities Structured jailbreak attacks on LLM-based multi-agent debate.txt @段1) || 关键词 | mad, jailbreak, debate, harmful, gpt, prompt, multi, arxiv, answer, role | 0.78 | “JOURNAL OF LATEX CLASS FILES, VOL. X, NO. Y , FEBRUARY 2025 1 Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate Senmao Qi, Yifei Zou, Member, IEEE, Peng Li, Senior Member, IEEE,, Ziyi Lin, Xiuzhen Cheng, Fellow, IEEE, and Dongxiao Yu, Senior Member, IEEE Abstract—Multi-Agent Debate” (Amplified vulnerabilities Structured jailbreak attacks on LLM-based multi-agent debate.txt @段1) || 摘要 | JOURNAL OF LATEX CLASS FILES, VOL. X, NO. Y , FEBRUARY 2025 1 Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate Senmao Qi, Yifei Zou, Member, IEEE, Peng Li, Senior Member, IEEE,, Ziyi Lin, Xiuzhen Cheng, Fellow, IEEE, and Dongxiao Yu, Senior Member, IEEE Abstract—Multi-Agent Debate (MAD), leveraging col- laborative interactions among Large Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks. However, the security implications of their iterative dialo… | 0.72 | “JOURNAL OF LATEX CLASS FILES, VOL. X, NO. Y , FEBRUARY 2025 1 Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate Senmao Qi, Yifei Zou, Member, IEEE, Peng Li, Senior Member, IEEE,, Ziyi Lin, Xiuzhen Cheng, Fellow, IEEE, and Dongxiao Yu, Senior Member, IEEE Abstract—Multi-Agent Debate” (Amplified vulnerabilities Structured jailbreak attacks on LLM-based multi-agent debate.txt @段1) || 研究方法 | JOURNAL OF LATEX CLASS FILES, VOL. X, NO. Y , FEBRUARY 2025 1 Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate Senmao Qi, Yifei Zou, Member, IEEE, Peng Li, Senior Member, IEEE,, Ziyi Lin, Xiuzhen Cheng, Fellow, IEEE, and Dongxiao Yu, Senior Member, IEEE Abstract—Multi-Agent Debate (MAD), leveraging col- laborative interactions among Large Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks. However, the security implications of their iterative dialo… | 0.82 | “We introduce a structured rewrit- ing strategy R(q), explicitly tailored to exploit the itera- tive and role-based dynamics of MAD frameworks, guid- ing agents towards progressively elaborating harmful content through multi-round interactions.” (Amplified vulnerabilities Structured jailbreak attacks on LLM-based multi-agent debate.txt @段1) || 主要结论 | JOURNAL OF LATEX CLASS FILES, VOL. X, NO. Y , FEBRUARY 2025 1 Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate Senmao Qi, Yifei Zou, Member, IEEE, Peng Li, Senior Member, IEEE,, Ziyi Lin, Xiuzhen Cheng, Fellow, IEEE, and Dongxiao Yu, Senior Member, IEEE Abstract—Multi-Agent Debate (MAD), leveraging col- laborative interactions among Large Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks. However, the security implications of their iterative dialo… | 0.80 | “Our results indicate that MAD is inherently more fragile than single-agent systems, possibly because the role settings and multiple rounds of debate interaction weaken overall robustness.” (Amplified vulnerabilities Structured jailbreak attacks on LLM-based multi-agent debate.txt @段1) |

- 主题标签：Jailbreak/越狱攻击, 多智能体对抗与协作
- 方法标签：多智能体/Multi-Agent, 越狱/Jailbreak
- 源文件：`综述参考文献\Amplified vulnerabilities Structured jailbreak attacks on LLM-based multi-agent debate.txt`

</details>
<details><summary>84. Architecture Design（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Architecture Design | 0.92 | “Architecture Design” (Knowledge-based multi-agent framework for automated software architecture design.txt @段8) || 作者 | YIRAN ZHANG, Nanyang Technological University, Singapore City, Singapore | 0.90 | “YIRAN ZHANG, Nanyang Technological University, Singapore City, Singapore” (Knowledge-based multi-agent framework for automated software architecture design.txt @段1) || 发表年份 | 2025 | 0.90 | “Published: 23 June 2025” (Knowledge-based multi-agent framework for automated software architecture design.txt @段1) || 期刊/会议 | International Conference on the | 0.88 | “International Conference on the” (Knowledge-based multi-agent framework for automated software architecture design.txt @段1) || 关键词 | architecture, design, software, university, singapore, wuhan, multi, process, 2025, acm | 0.78 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3728493 . . SHORT-PAPER Knowledge-Based Multi-Agent Framework for Automated Soware Architecture Design YIRAN ZHANG, Nanyang Technological University, Singapore City, Singapore . RUIYIN LI, Wuhan University, Wuhan, Hubei, China . PENG LIANG, Wuhan University, Wu” (Knowledge-based multi-agent framework for automated software architecture design.txt @段1) || 摘要 | . . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3728493 . . SHORT-PAPER Knowledge-Based Multi-Agent Framework for Automated Soware Architecture Design YIRAN ZHANG, Nanyang Technological University, Singapore City, Singapore . RUIYIN LI, Wuhan University, Wuhan, Hubei, China . PENG LIANG, Wuhan University, Wuhan, Hubei, China . WEISONG SUN, Nanyang Technological University, Singapore City, Singapore . YANG LIU, Nanyang Technological University, Singapore City, Singapore . . . Open Access Support provided… | 0.72 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3728493 . . SHORT-PAPER Knowledge-Based Multi-Agent Framework for Automated Soware Architecture Design YIRAN ZHANG, Nanyang Technological University, Singapore City, Singapore . RUIYIN LI, Wuhan University, Wuhan, Hubei, China . PENG LIANG, Wuhan University, Wu” (Knowledge-based multi-agent framework for automated software architecture design.txt @段1) || 研究方法 | 1 INTRODUCTION Software architecture plays a vital role in the software development process. It serves as the blueprint that ensures systems are scal- able, maintainable, and aligned with the business goals specified in the Software Requirements Specification (SRS) [2]. Traditionally, the architecture design process relies heavily on human exper- tise. This manual approach increases costs and makes the process time-consuming and inconsistent [ 7]. Therefore, automating ar- chitectural design is essential for achie… | 0.82 | “To fill this gap, in this paper, we present our vision for an au- tomated software architecture design framework, Multi-Agent Ar- chitecture Design (MAAD).” (Knowledge-based multi-agent framework for automated software architecture design.txt @段6) || 主要结论 | 2.2 Architecture Agents 2.2.1 Analyst. The Analyst agent is responsible for understanding the SRS, filtering, classifying, and documenting the requirements that impact the architecture. This agent is crucial to ensuring that the subsequent architecture design aligns with business goals. To accomplish this task, the Analyst agent should possess capabil- ities to understand the SRS, which includes the following actions: 1) parse and structure the SRS to extract all requirements; 2) filter out architecture-significan… | 0.80 | “Key outputs from the Evaluator agent include: 1) mismatch analysis reports, document- ing discrepancies where the architectural outputs fail to meet the requirements specified in the SRS; 2) root cause analysis reports, identifying the underlying causes of the mismatches and 3) refine- ment suggestions, providing actio” (Knowledge-based multi-agent framework for automated software architecture design.txt @段9) |

- 主题标签：多智能体对抗与协作
- 方法标签：多智能体/Multi-Agent
- 源文件：`Knowledge-based multi-agent framework for automated software architecture design.txt`

</details>
<details><summary>85. Automated Risk Simulator（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Automated Risk Simulator | 0.92 | “Automated Risk Simulator” (SafeAgent Safeguarding LLM agents via an automated risk simulator.txt @段2) || 作者 | 1Huazhong University of Science and Technology 2Shanghai Jiaotong University | 0.90 | “1Huazhong University of Science and Technology 2Shanghai Jiaotong University” (SafeAgent Safeguarding LLM agents via an automated risk simulator.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2505.17735v2  [cs.AI]  18 Jul 2025” (SafeAgent Safeguarding LLM agents via an automated risk simulator.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2505.17735v2  [cs.AI]  18 Jul 2025” (SafeAgent Safeguarding LLM agents via an automated risk simulator.txt @段1) || 关键词 | risk, actions, diverse, unsafe, real, threat, user, autosafe, data, ots | 0.78 | “SafeAgent: Safeguarding LLM Agents via an Automated Risk Simulator Xueyang Zhou1 Weidong Wang1 Lin Lu1 Jiawen Shi1 Guiyao Tie1 Yongtian Xu1 Lixing Chen2 Pan Zhou1 Neil Zhenqiang Gong3 Lichao Sun4 1Huazhong University of Science and Technology 2Shanghai Jiaotong University 3Duke University 4Lehigh University {d202480819” (SafeAgent Safeguarding LLM agents via an automated risk simulator.txt @段1) || 摘要 | SafeAgent: Safeguarding LLM Agents via an Automated Risk Simulator Xueyang Zhou1 Weidong Wang1 Lin Lu1 Jiawen Shi1 Guiyao Tie1 Yongtian Xu1 Lixing Chen2 Pan Zhou1 Neil Zhenqiang Gong3 Lichao Sun4 1Huazhong University of Science and Technology 2Shanghai Jiaotong University 3Duke University 4Lehigh University {d202480819,m202472185,shijiawen,lulin,tgy,u202312537,panzhou}@hust.edu.cn lxchen@sjtu.edu.cn,neil.gong@duke.edu,lis221@lehigh.edu Abstract Large Language Model (LLM)-based agents are increasingly deployed in r… | 0.72 | “SafeAgent: Safeguarding LLM Agents via an Automated Risk Simulator Xueyang Zhou1 Weidong Wang1 Lin Lu1 Jiawen Shi1 Guiyao Tie1 Yongtian Xu1 Lixing Chen2 Pan Zhou1 Neil Zhenqiang Gong3 Lichao Sun4 1Huazhong University of Science and Technology 2Shanghai Jiaotong University 3Duke University 4Lehigh University {d202480819” (SafeAgent Safeguarding LLM agents via an automated risk simulator.txt @段1) || 研究方法 | References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson, et al. Agentharm: A benchmark for measuring harmfulness of llm agents. arXiv preprint arXiv:2410.09024, 2024. [3] Anthr… | 0.82 | “F Limitations While AutoSafe presents a novel framework for automated safety enhancement of LLM-based agents, it does not propose new safety training algorithms such as Reinforcement Learning with Human Feedback (RLHF) or other optimization-based techniques.” (SafeAgent Safeguarding LLM agents via an automated risk simulator.txt p.18 @段21) || 主要结论 | 6 Conclusion In this paper, we propose AutoSafe for enhancing safety of LLM-based agents. Guided by the threat model, OTS, AutoSafe generates risk scenarios based on risk outcomes. It then collects safe actions under these scenarios using the self-reflection mechanism for enhancement training. The experimental results show that our method improves the safety of four open-source models by 45.4% on average, outperforming all models, including GPT-4. Additionally, fine-grained evaluations confirm that AutoSafe’s impr… | 0.80 | “The experimental results show that our method improves the safety of four open-source models by 45.4% on average, outperforming all models, including GPT-4.” (SafeAgent Safeguarding LLM agents via an automated risk simulator.txt @段20) |

- 主题标签：GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击, 防御与对齐机制
- 方法标签：仿真/Simulator, 基准/Benchmark, 多模态/Multimodal, 系统防御/Defense
- 源文件：`综述参考文献\SafeAgent Safeguarding LLM agents via an automated risk simulator.txt`

</details>
<details><summary>86. Autonomous agents in soware development for information retrieval（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Autonomous agents in soware development for information retrieval | 0.92 | “Autonomous agents in soware development for information retrieval” (Autonomous agents in software development for information retrieval using LLM models.txt @段7) || 作者 | ANETA PONISZEWSKA-MARAŃDA, Lodz University of Technology, Lodz, LD, Poland | 0.90 | “ANETA PONISZEWSKA-MARAŃDA, Lodz University of Technology, Lodz, LD, Poland” (Autonomous agents in software development for information retrieval using LLM models.txt @段1) || 发表年份 | 2025 | 0.90 | “Published: 23 June 2025” (Autonomous agents in software development for information retrieval using LLM models.txt @段1) || 期刊/会议 | International Conference on the | 0.88 | “International Conference on the” (Autonomous agents in software development for information retrieval using LLM models.txt @段1) || 关键词 | autonomous, information, retrieval, acm, lodz, 2025, 3696630, 3731432, development, software | 0.78 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3731432 . . EXTENDED-ABSTRACT Autonomous agents in soware development for information retrieval using LLM models ANETA PONISZEWSKA-MARAŃDA, Lodz University of Technology, Lodz, LD, Poland . MACIEJ KOPA, Lodz University of Technology, Lodz, LD, Poland . . . Open” (Autonomous agents in software development for information retrieval using LLM models.txt @段1) || 摘要 | . . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3731432 . . EXTENDED-ABSTRACT Autonomous agents in soware development for information retrieval using LLM models ANETA PONISZEWSKA-MARAŃDA, Lodz University of Technology, Lodz, LD, Poland . MACIEJ KOPA, Lodz University of Technology, Lodz, LD, Poland . . . Open Access Support provided by: . Lodz University of Technology . PDF Download 3696630.3731432.pdf 20 January 2026 Total Citations: 0 Total Downloads: 419 . . Published: 23 June 2025 . . Citation in Bib… | 0.72 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3731432 . . EXTENDED-ABSTRACT Autonomous agents in soware development for information retrieval using LLM models ANETA PONISZEWSKA-MARAŃDA, Lodz University of Technology, Lodz, LD, Poland . MACIEJ KOPA, Lodz University of Technology, Lodz, LD, Poland . . . Open” (Autonomous agents in software development for information retrieval using LLM models.txt @段1) || 研究方法 | N/A | 0.50 | N/A || 主要结论 | N/A | 0.50 | N/A |

- 主题标签：N/A
- 方法标签：N/A
- 源文件：`Autonomous agents in software development for information retrieval using LLM models.txt`

</details>
<details><summary>87. Comprehensive Repository Exploration（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Comprehensive Repository Exploration | 0.92 | “Comprehensive Repository Exploration” (Alibaba LingmaAgent Improving automated issue resolution via comprehensive repository exploration.txt @段8) || 作者 | YINGWEI MA, Alibaba Group Holding Limited, Hangzhou, Zhejiang, China | 0.90 | “YINGWEI MA, Alibaba Group Holding Limited, Hangzhou, Zhejiang, China” (Alibaba LingmaAgent Improving automated issue resolution via comprehensive repository exploration.txt @段1) || 发表年份 | 2025 | 0.90 | “Published: 23 June 2025” (Alibaba LingmaAgent Improving automated issue resolution via comprehensive repository exploration.txt @段1) || 期刊/会议 | International Conference on the | 0.88 | “International Conference on the” (Alibaba LingmaAgent Improving automated issue resolution via comprehensive repository exploration.txt @段1) || 关键词 | repository, alibaba, lingmaagent, code, software, comprehensive, 2025, engineering, group, information | 0.78 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3728549 . . RESEARCH-ARTICLE Alibaba LingmaAgent: Improving Automated Issue Resolution via Comprehensive Repository Exploration YINGWEI MA, Alibaba Group Holding Limited, Hangzhou, Zhejiang, China . QINGPING YANG, Alibaba Group Holding Limited, Hangzhou, Zhejian” (Alibaba LingmaAgent Improving automated issue resolution via comprehensive repository exploration.txt @段1) || 摘要 | . . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3728549 . . RESEARCH-ARTICLE Alibaba LingmaAgent: Improving Automated Issue Resolution via Comprehensive Repository Exploration YINGWEI MA, Alibaba Group Holding Limited, Hangzhou, Zhejiang, China . QINGPING YANG, Alibaba Group Holding Limited, Hangzhou, Zhejiang, China . RONGYU CAO, Alibaba Group Holding Limited, Hangzhou, Zhejiang, China . BINHUA LI, Alibaba Group Holding Limited, Hangzhou, Zhejiang, China . FEI HUANG, Alibaba Group Holding Limited, Hangz… | 0.72 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3728549 . . RESEARCH-ARTICLE Alibaba LingmaAgent: Improving Automated Issue Resolution via Comprehensive Repository Exploration YINGWEI MA, Alibaba Group Holding Limited, Hangzhou, Zhejiang, China . QINGPING YANG, Alibaba Group Holding Limited, Hangzhou, Zhejian” (Alibaba LingmaAgent Improving automated issue resolution via comprehensive repository exploration.txt @段1) || 研究方法 | References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren- cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Alibaba Cloud. 2024. TONGYI Lingma: Your smart coding assistant. https: //lingma.aliyun.com/ [3] Anthropic. 2024. Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku. https://www.anthropic.com/news/3-5-models-and-computer-use [4] Antonis Anton… | 0.82 | “Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization.” (Alibaba LingmaAgent Improving automated issue resolution via comprehensive repository exploration.txt @段28) || 主要结论 | 4.4 Ablation Study 4.4.1 Module Analysis. This ablation experiment aims to study the effectiveness of LingmaAgent’s global repository understand- ing component. (1) Remove only the call graph module: Only the structure tree in MCTS is retained for tree search, and the refer- ence extension module (i.e., the call relationship graph) is removed. This experiment aims to verify the effectiveness of the reference extension module, i.e., the importance of reference relations in the repository. (2) Remove only the summar… | 0.80 | “Our findings show that our method significantly outperforms the other two methods in the success rate of fault localization at both the Function and File levels, which shows that understanding the repository and exploring critical information notably contribute to improving fault localization.” (Alibaba LingmaAgent Improving automated issue resolution via comprehensive repository exploration.txt @段21) |

- 主题标签：GUI/Computer-Use Agent 安全
- 方法标签：N/A
- 源文件：`Alibaba LingmaAgent Improving automated issue resolution via comprehensive repository exploration.txt`

</details>
<details><summary>88. Computer-Use Agents（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Computer-Use Agents | 0.92 | “Computer-Use Agents” (VPI-bench Visual prompt injection attacks for computer-use agents.txt @段3) || 作者 | Tri Cao1, Bennett Lim1∗, Yue Liu1, Yuan Sui1, Yuexin Li1, | 0.90 | “Tri Cao1, Bennett Lim1∗, Yue Liu1, Yuan Sui1, Yuexin Li1,” (VPI-bench Visual prompt injection attacks for computer-use agents.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2506.02456v1  [cs.AI]  3 Jun 2025” (VPI-bench Visual prompt injection attacks for computer-use agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2506.02456v1  [cs.AI]  3 Jun 2025” (VPI-bench Visual prompt injection attacks for computer-use agents.txt @段1) || 关键词 | prompt, cuas, visual, user, system, computer, injection, vpi, malicious, task | 0.78 | “arXiv:2506.02456v1 [cs.AI] 3 Jun 2025 VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents Tri Cao1, Bennett Lim1∗, Yue Liu1, Yuan Sui1, Yuexin Li1, Shumin Deng1, Lin Lu2, Nay Oo2, Shuicheng Yan1, Bryan Hooi1 1National University of Singapore, 2NCS Cyber Special Ops R&D” (VPI-bench Visual prompt injection attacks for computer-use agents.txt @段1) || 摘要 | arXiv:2506.02456v1 [cs.AI] 3 Jun 2025 VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents Tri Cao1, Bennett Lim1∗, Yue Liu1, Yuan Sui1, Yuexin Li1, Shumin Deng1, Lin Lu2, Nay Oo2, Shuicheng Yan1, Bryan Hooi1 1National University of Singapore, 2NCS Cyber Special Ops R&D Abstract Computer-Use Agents (CUAs) with full system access enable powerful task au- tomation but pose significant security and privacy risks due to their ability to manipulate files, access user data, and execute arbitrary commands. … | 0.72 | “arXiv:2506.02456v1 [cs.AI] 3 Jun 2025 VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents Tri Cao1, Bennett Lim1∗, Yue Liu1, Yuan Sui1, Yuexin Li1, Shumin Deng1, Lin Lu2, Nay Oo2, Shuicheng Yan1, Bryan Hooi1 1National University of Singapore, 2NCS Cyber Special Ops R&D” (VPI-bench Visual prompt injection attacks for computer-use agents.txt @段1) || 研究方法 | 4.3 Vulnerability of Models Under Visual Prompt Injections Table 1 reports the attempted and success rates of various models under prompt injection attacks across five real-world platforms. The results are averaged over three independent runs. In general, both attempted and success rates are high across most models and domains, indicating that current systems remain vulnerable to injected prompts. However, a clear distinction emerges between models deployed in Computer-Use and Browser-Use settings. Models in the C… | 0.82 | “Nonetheless, the rates for domains such as Messenger and Email remain non-negligible, exceeding 50% in several cases, suggesting that even these defenses are insufficient in contexts that involve (1) conversations, or (2) are "multi-intent", 6 Framework Model Amazon Booking BBC Messenger Email Computer-Use Sonnet-3.5 0” (VPI-bench Visual prompt injection attacks for computer-use agents.txt @段14) || 主要结论 | 1 Introduction While AI agents offer exceptional efficiency in managing complex tasks [17, 19, 3, 30], they also raise significant safety concerns. Many tasks require users to share sensitive personal information, such as login credentials, financial details, or card information, and often grant these agents control over their devices. For example, tasks like logging into online banking systems, making online purchases, managing personal accounts, or retrieving sensitive documents often involve the transfer of con… | 0.80 | “Our results show that attack success rates remain high regardless of injection timing, suggesting that agents are generally susceptible throughout the interaction process.” (VPI-bench Visual prompt injection attacks for computer-use agents.txt @段3) |

- 主题标签：GUI/Computer-Use Agent 安全, Prompt Injection/环境注入, Web/Browser Agent 安全, 防御与对齐机制, 隐私与记忆风险
- 方法标签：仿真/Simulator, 基准/Benchmark, 提示注入/Prompt Injection, 系统防御/Defense
- 源文件：`综述参考文献\VPI-bench Visual prompt injection attacks for computer-use agents.txt`

</details>
<details><summary>89. Deception via Steganography（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Deception via Steganography | 0.92 | “Deception via Steganography” (Secret collusion among AI agents Multi-agent deception via steganography.txt @段2) || 作者 | Sumeet Ramesh Motwani1,2 Mikhail Baranchuk2 Martin Strohmeier3 Vijay Bolina4 | 0.90 | “Sumeet Ramesh Motwani1,2 Mikhail Baranchuk2 Martin Strohmeier3 Vijay Bolina4” (Secret collusion among AI agents Multi-agent deception via steganography.txt @段1) || 发表年份 | 2025 | 0.86 | “2025 and funds should start” (Secret collusion among AI agents Multi-agent deception via steganography.txt @段1) || 期刊/会议 | 38th Conference on Neural Information Processing Systems (NeurIPS 2024). | 0.88 | “38th Conference on Neural Information Processing Systems (NeurIPS 2024).” (Secret collusion among AI agents Multi-agent deception via steganography.txt @段1) || 关键词 | information, collusion, secret, steganographic, generative, multi, systems, monitoring, steganography, frontier | 0.78 | “Secret Collusion among AI Agents: Multi-Agent Deception via Steganography Sumeet Ramesh Motwani1,2 Mikhail Baranchuk2 Martin Strohmeier3 Vijay Bolina4 Philip H.S. Torr2 Lewis Hammond2 Christian Schroeder de Witt2∗ 1UC Berkeley 2University of Oxford 3Armasuisse Science+Technology 4Google DeepMind” (Secret collusion among AI agents Multi-agent deception via steganography.txt @段1) || 摘要 | Secret Collusion among AI Agents: Multi-Agent Deception via Steganography Sumeet Ramesh Motwani1,2 Mikhail Baranchuk2 Martin Strohmeier3 Vijay Bolina4 Philip H.S. Torr2 Lewis Hammond2 Christian Schroeder de Witt2∗ 1UC Berkeley 2University of Oxford 3Armasuisse Science+Technology 4Google DeepMind Abstract Recent advancements in generative AI suggest the potential for large-scale in- teraction between autonomous agents and humans across platforms such as the internet. While such interactions could foster productive … | 0.72 | “Secret Collusion among AI Agents: Multi-Agent Deception via Steganography Sumeet Ramesh Motwani1,2 Mikhail Baranchuk2 Martin Strohmeier3 Vijay Bolina4 Philip H.S. Torr2 Lewis Hammond2 Christian Schroeder de Witt2∗ 1UC Berkeley 2University of Oxford 3Armasuisse Science+Technology 4Google DeepMind” (Secret collusion among AI agents Multi-agent deception via steganography.txt @段1) || 研究方法 | 4 Collusion among Decentralized Collaborative AI Agents 1 3 2 4 ◊1,3,4 ◊1,2◊1 ◊∅ Figure 3: A schematic depiction of the DecAI setting. A subset of the commu- nication channels ♢G is labelled. The monitoring agent is omitted. We consider a decentralised setting (“DecAI”) in which n agents A =  a1, . . . , an are networked together by a complete set of omnidirectional multicast channels ♢G , G ∈ 2A. If an agent receives a message from channel ♢G′ , then it knows that the same message has been simultaneously sent t… | 0.82 | “We introduce a standardized model evaluation framework, mapping and identifying key capabilities necessary for generative AI agents to engage in steganography, evaluating models through structured input queries, building quantifiable metrics for performance measurement, releasing a software framework that implements a” (Secret collusion among AI agents Multi-agent deception via steganography.txt @段14) || 主要结论 | Abstract Recent advancements in generative AI suggest the potential for large-scale in- teraction between autonomous agents and humans across platforms such as the internet. While such interactions could foster productive cooperation, the ability of AI agents to circumvent security oversight raises critical multi-agent security problems, particularly in the form of unintended information sharing or undesirable coordination. In our work, we establish the subfield of secret collusion, a form of multi-agent deception… | 0.80 | “We propose a formal threat model for AI agents com- municating steganographically and derive rigorous theoretical insights about the capacity and incentives of large language models (LLMs) to perform secret collu- sion, in addition to the limitations of threat mitigation measures.” (Secret collusion among AI agents Multi-agent deception via steganography.txt @段2) |

- 主题标签：Web/Browser Agent 安全, 多智能体对抗与协作
- 方法标签：基准/Benchmark, 多智能体/Multi-Agent
- 源文件：`Secret collusion among AI agents Multi-agent deception via steganography.txt`

</details>
<details><summary>90. Engineering: Literature Review, Vision, and the Road（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Engineering: Literature Review, Vision, and the Road | 0.92 | “Engineering: Literature Review, Vision, and the Road” (LLM-based multi-agent systems for software engineering Literature review, vision, and the road ahea.txt @段8) || 作者 | Engineering: Literature Review, Vision, and the Road | 0.90 | “Engineering: Literature Review, Vision, and the Road” (LLM-based multi-agent systems for software engineering Literature review, vision, and the road ahea.txt @段1) || 发表年份 | 2025 | 0.90 | “Published: 24 May 2025” (LLM-based multi-agent systems for software engineering Literature review, vision, and the road ahea.txt @段1) || 期刊/会议 | Unknown venue (DOI: 10.1145/3712003) | 0.82 | “10.1145/3712003” (LLM-based multi-agent systems for software engineering Literature review, vision, and the road ahea.txt @段1) || 关键词 | systems, lma, singapore, software, engineering, research, 2025, acm, autonomous, development | 0.78 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3712003 . . RESEARCH-ARTICLE LLM-Based Multi-Agent Systems for Soware Engineering: Literature Review, Vision, and the Road Ahead JUNDA HE, Singapore Management University, Singapore City, Singapore . CHRISTOPH TREUDE, Singapore Management University, Singapore City, Si” (LLM-based multi-agent systems for software engineering Literature review, vision, and the road ahea.txt @段1) || 摘要 | . . Latest updates: hps://dl.acm.org/doi/10.1145/3712003 . . RESEARCH-ARTICLE LLM-Based Multi-Agent Systems for Soware Engineering: Literature Review, Vision, and the Road Ahead JUNDA HE, Singapore Management University, Singapore City, Singapore . CHRISTOPH TREUDE, Singapore Management University, Singapore City, Singapore . DAVID LO, Singapore Management University, Singapore City, Singapore . . . Open Access Support provided by: . Singapore Management University . PDF Download 3712003.pdf 20 January 2026 Tota… | 0.72 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3712003 . . RESEARCH-ARTICLE LLM-Based Multi-Agent Systems for Soware Engineering: Literature Review, Vision, and the Road Ahead JUNDA HE, Singapore Management University, Singapore City, Singapore . CHRISTOPH TREUDE, Singapore Management University, Singapore City, Si” (LLM-based multi-agent systems for software engineering Literature review, vision, and the road ahea.txt @段1) || 研究方法 | 5.2 Phase Two: Optimizing Agent Synergy In Phase Two, the spotlight turns toward optimizing agent synergy, underscoring the importance of collaboration and how to leverage the diverse strengths of individual agents. This phase delves into both the internal dynamics among agents and the role of external human intervention in enhancing the efficacy of the LMA system. Key research questions guiding this phase include: (1) How to best allocate tasks between humans and LLM-based agents? (2) How can we quantify the impa… | 0.82 | “By emulating organizational frameworks used by successful companies, LMA systems can improve their design and optimization processes.” (LLM-based multi-agent systems for software engineering Literature review, vision, and the road ahea.txt @段21) || 主要结论 | 3.5 End-to-End Software Development End-to-end software development encompasses the entire process of creating a software product. While conventional code generation is often limited to producing isolated components such as functions, classes, or modules, end-to-end development starts from high-level software requirements and progresses through design, implementation, testing, and ultimately delivering a fully functional and ready-to-use product. In practice, developers and stakeholders typically adopt established… | 0.80 | “Since software development processes can vary significantly depending on project requirements, ToP moves beyond the limitations of static, one-size-fits-all workflows to enable more flexible and efficient development practices.” (LLM-based multi-agent systems for software engineering Literature review, vision, and the road ahea.txt @段15) |

- 主题标签：GUI/Computer-Use Agent 安全, Web/Browser Agent 安全, 多智能体对抗与协作
- 方法标签：多智能体/Multi-Agent, 多模态/Multimodal, 调查/Survey
- 源文件：`LLM-based multi-agent systems for software engineering Literature review, vision, and the road ahea.txt`

</details>
<details><summary>91. Enhancing Game AI Behaviors with Large Language Models and（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Enhancing Game AI Behaviors with Large Language Models and | 0.92 | “Enhancing Game AI Behaviors with Large Language Models and” (Enhancing game AI behaviors with large language models and agentic AI.txt @段7) || 作者 | CIPRIAN I PĂDURARU, University of Bucharest, Bucharest, Bucharest, Romania | 0.90 | “CIPRIAN I PĂDURARU, University of Bucharest, Bucharest, Bucharest, Romania” (Enhancing game AI behaviors with large language models and agentic AI.txt @段1) || 发表年份 | 2025 | 0.90 | “Published: 23 June 2025” (Enhancing game AI behaviors with large language models and agentic AI.txt @段1) || 期刊/会议 | International Conference on the | 0.88 | “International Conference on the” (Enhancing game AI behaviors with large language models and agentic AI.txt @段1) || 关键词 | game, methods, behaviors, development, acm, bucharest, 2025, code, source, agentic | 0.78 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3728553 . . RESEARCH-ARTICLE Enhancing Game AI Behaviors with Large Language Models and Agentic AI CIPRIAN I PĂDURARU, University of Bucharest, Bucharest, Bucharest, Romania . MIRUNA GABRIELA PADURARU, Electronic Arts Inc, Redwood City, CA, United States . ALIN” (Enhancing game AI behaviors with large language models and agentic AI.txt @段1) || 摘要 | . . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3728553 . . RESEARCH-ARTICLE Enhancing Game AI Behaviors with Large Language Models and Agentic AI CIPRIAN I PĂDURARU, University of Bucharest, Bucharest, Bucharest, Romania . MIRUNA GABRIELA PADURARU, Electronic Arts Inc, Redwood City, CA, United States . ALIN STEFANESCU, University of Bucharest, Bucharest, Bucharest, Romania . . . Open Access Support provided by: . University of Bucharest . Electronic Arts Inc . PDF Download 3696630.3728553.pdf 20 January… | 0.72 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3696630.3728553 . . RESEARCH-ARTICLE Enhancing Game AI Behaviors with Large Language Models and Agentic AI CIPRIAN I PĂDURARU, University of Bucharest, Bucharest, Bucharest, Romania . MIRUNA GABRIELA PADURARU, Electronic Arts Inc, Redwood City, CA, United States . ALIN” (Enhancing game AI behaviors with large language models and agentic AI.txt @段1) || 研究方法 | Abstract Integrating advanced AI behaviors is central to creating immersive and dynamic video game experiences. This paper presents a novel approach to improving AI behaviors in games using Large Lan- guage Models (LLMs) and agent-based AI. By orchestrating various interconnected parts, we propose a framework that facilitates the creation of complex behavior trees (BTs) for non-player characters (NPCs). Our method bridges the gap between source code and visual tools in game engines and enables both technical and n… | 0.82 | “By orchestrating various interconnected parts, we propose a framework that facilitates the creation of complex behavior trees (BTs) for non-player characters (NPCs).” (Enhancing game AI behaviors with large language models and agentic AI.txt @段4) || 主要结论 | 2 Related work The literature review focuses on two perspectives: a) the previous work related to behavior trees and generation with LLMs and b) specific topics related to LLMs or agentic AI that were considered in the development and implementation of the methods proposed in our paper. Behavior Trees and LLMs. In our research, we found that pre- vious work, while informative to our work, does not meet the requirements for applying BT and LLM generation to generate AI behaviors for games, as they have different go… | 0.80 | “287 Enhancing Game AI Behaviors with Large Language Models and Agentic AI FSE Companion ’25, June 23–28, 2025, Trondheim, Norway Recent developments have significantly improved the efficiency, accuracy, and factuality of large language models (LLMs) with re- trieval extension.” (Enhancing game AI behaviors with large language models and agentic AI.txt @段7) |

- 主题标签：多智能体对抗与协作
- 方法标签：多智能体/Multi-Agent, 调查/Survey
- 源文件：`Enhancing game AI behaviors with large language models and agentic AI.txt`

</details>
<details><summary>92. Enhancing Human-IDE Interaction in the SDLC using LLM-based（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Enhancing Human-IDE Interaction in the SDLC using LLM-based | 0.92 | “Enhancing Human-IDE Interaction in the SDLC using LLM-based” (Enhancing human-IDE interaction in the SDLC using LLM-based mediator agents.txt @段1) || 作者 | be integrated into Integrated Development Environments (IDEs), | 0.90 | “be integrated into Integrated Development Environments (IDEs),” (Enhancing human-IDE interaction in the SDLC using LLM-based mediator agents.txt @段1) || 发表年份 | 2025 | 0.90 | “Ziyou Li and Maliheh Izadi. 2025. Enhancing Human-IDE Interaction in” (Enhancing human-IDE interaction in the SDLC using LLM-based mediator agents.txt @段1) || 期刊/会议 | Conference on the Foundations of Software Engineering (FSE Companion ’25), | 0.88 | “Conference on the Foundations of Software Engineering (FSE Companion ’25),” (Enhancing human-IDE interaction in the SDLC using LLM-based mediator agents.txt @段1) || 关键词 | ide, software, sdlc, automation, human, tools, development, interaction, systems, 2025 | 0.78 | “Enhancing Human-IDE Interaction in the SDLC using LLM-based Mediator Agents Ziyou Li ziyou.li@tudelft.nl Delft University of Technology The Netherlands Maliheh Izadi m.izadi@tudelft.nl Delft University of Technology The Netherlands” (Enhancing human-IDE interaction in the SDLC using LLM-based mediator agents.txt @段1) || 摘要 | Enhancing Human-IDE Interaction in the SDLC using LLM-based Mediator Agents Ziyou Li ziyou.li@tudelft.nl Delft University of Technology The Netherlands Maliheh Izadi m.izadi@tudelft.nl Delft University of Technology The Netherlands Abstract Large Language Model (LLM)-based autonomous agents provide state-of-the-art performance in various tasks throughout the Soft- ware Development Lifecycle (SDLC). Although these agents can be integrated into Integrated Development Environments (IDEs), standalone applications impo… | 0.72 | “Enhancing Human-IDE Interaction in the SDLC using LLM-based Mediator Agents Ziyou Li ziyou.li@tudelft.nl Delft University of Technology The Netherlands Maliheh Izadi m.izadi@tudelft.nl Delft University of Technology The Netherlands” (Enhancing human-IDE interaction in the SDLC using LLM-based mediator agents.txt @段1) || 研究方法 | 3.1 Levels of Software Engineering Automation Given the rapid development in the area and inspired by the SAE autonomous driving automation levels [3], we propose the levels of autonomy of SE assistants by the SDLC task level requiring human supervision, as shown in Figure 1. From Level 0 (fully manual) to Level 5 (fully autonomous), these levels delineate the shift of responsibilities from human programmers to intelligent agents in the four hierarchical SDLC stages: requirements, design, implemen- tation, and tes… | 0.82 | “3.1 Levels of Software Engineering Automation Given the rapid development in the area and inspired by the SAE autonomous driving automation levels [3], we propose the levels of autonomy of SE assistants by the SDLC task level requiring human supervision, as shown in Figure 1.” (Enhancing human-IDE interaction in the SDLC using LLM-based mediator agents.txt @段10) || 主要结论 | N/A | 0.50 | N/A |

- 主题标签：多智能体对抗与协作
- 方法标签：仿真/Simulator, 多智能体/Multi-Agent, 多模态/Multimodal
- 源文件：`Enhancing human-IDE interaction in the SDLC using LLM-based mediator agents.txt`

</details>
<details><summary>93. Environmental Injection Attacks（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Environmental Injection Attacks | 0.92 | “Environmental Injection Attacks” (Evaluating the robustness of multimodal agents against active environmental injection attacks.txt @段2) || 作者 | Hangzhou, Zhejiang, China | 0.90 | “Hangzhou, Zhejiang, China” (Evaluating the robustness of multimodal agents against active environmental injection attacks.txt @段1) || 发表年份 | 2025 | 0.90 | “© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.” (Evaluating the robustness of multimodal agents against active environmental injection attacks.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2502.13053v3  [cs.CL]  6 Aug 2025” (Evaluating the robustness of multimodal agents against active environmental injection attacks.txt @段1) || 关键词 | injection, environmental, aeia, mobile, operating, active, system, elements, zhejiang, reasoning | 0.78 | “Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks Yurun Chen yurunchen.research@gmail.com School of Software Technology Zhejiang University Hangzhou, Zhejiang, China Xavier Hu xavier.hu.research@gmail.com College of Computer Science and Technology Zhejiang University Hangzhou” (Evaluating the robustness of multimodal agents against active environmental injection attacks.txt @段1) || 摘要 | Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks Yurun Chen yurunchen.research@gmail.com School of Software Technology Zhejiang University Hangzhou, Zhejiang, China Xavier Hu xavier.hu.research@gmail.com College of Computer Science and Technology Zhejiang University Hangzhou, Zhejiang, China Keting Yin∗ yinkt@zju.edu.cn School of Software Technology Zhejiang University Hangzhou, Zhejiang, China Juncheng Li junchengli@zju.edu.cn School of Software Technology Zhejiang Uni… | 0.72 | “Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks Yurun Chen yurunchen.research@gmail.com School of Software Technology Zhejiang University Hangzhou, Zhejiang, China Xavier Hu xavier.hu.research@gmail.com College of Computer Science and Technology Zhejiang University Hangzhou” (Evaluating the robustness of multimodal agents against active environmental injection attacks.txt @段1) || 研究方法 | 2 Related Work There has been extensive research on agents in both web and mobile environments. Notable web-based agents include SeeAct [37], SeeClick [ 5], and WebAgent [ 9], while mobile agents con- sist of InfiGUIAgent [19], AppAgent [35], Mobile-Agent [26], and Mobile-Agent-v2 [25]. Additionally, there are various other agents [6, 11, 13, 15, 17, 24, 30, 32]. Due to the complex design of operating systems, these agents are often exposed to various security risks. Many studies on the security of OS agents have … | 0.82 | “Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks MM ’25, October 27–31, 2025, Dublin, Ireland Algorithm 1: The AEIA-MN Attack Procedure Input: Operating System 𝑂𝑆 , Goal State 𝑆𝑡𝑎𝑡𝑒 𝑔𝑜𝑎𝑙 Output: Final system state 𝑆𝑡𝑎𝑡𝑒 𝑓 𝑖𝑛” (Evaluating the robustness of multimodal agents against active environmental injection attacks.txt @段5) || 主要结论 | 4.3 Main Results on AndroidWorld We conducted a comprehensive evaluation of the robustness of agents based on various MLLM against AEIA-MN on the Android- World benchmark. The evaluation results are presented as follows. 4.3.1 Adversarial Attack. We present the evaluation results of dif- ferent MLLM-based Agents under Adversarial Attack in Table 2, which show that most agents have limited defense capabilities against such attacks. In I3A and M3A, the Adversarial Attack gen- erally reduces task success rates; howev… | 0.80 | “The results show that the Reasoning Gap Attack significantly disrupts the task execution of most agents, causing a notable decrease in task success rates across most models.” (Evaluating the robustness of multimodal agents against active environmental injection attacks.txt @段27) |

- 主题标签：GUI/Computer-Use Agent 安全, Prompt Injection/环境注入, Web/Browser Agent 安全, 防御与对齐机制
- 方法标签：仿真/Simulator, 基准/Benchmark, 多模态/Multimodal, 提示注入/Prompt Injection, 系统防御/Defense
- 源文件：`综述参考文献\Evaluating the robustness of multimodal agents against active environmental injection attacks.txt`

</details>
<details><summary>94. Facilitating Trustworthy Human-Agent Collaboration in（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Facilitating Trustworthy Human-Agent Collaboration in | 0.92 | “Facilitating Trustworthy Human-Agent Collaboration in” (Facilitating trustworthy human-agent collaboration in LLM-based multi-agent system oriented software.txt @段1) || 作者 | Gothenburg, Sweden | 0.90 | “Gothenburg, Sweden” (Facilitating trustworthy human-agent collaboration in LLM-based multi-agent system oriented software.txt @段1) || 发表年份 | 2025 | 0.90 | “Krishna Ronanki. 2025. Facilitating Trustworthy Human-Agent Collabora-” (Facilitating trustworthy human-agent collaboration in LLM-based multi-agent system oriented software.txt @段1) || 期刊/会议 | Companion Proceedings of the 33rd ACM Symposium on the Foundations of | 0.88 | “Companion Proceedings of the 33rd ACM Symposium on the Foundations of” (Facilitating trustworthy human-agent collaboration in LLM-based multi-agent system oriented software.txt @段1) || 关键词 | lma, oriented, software, trustworthy, framework, systems, ase, collaboration, human, implementation | 0.78 | “Facilitating Trustworthy Human-Agent Collaboration in LLM-based Multi-Agent System oriented Software Engineering Krishna Ronanki krishna.ronanki@gu.se Chalmers University of Technology | University of Gothenburg Gothenburg, Sweden” (Facilitating trustworthy human-agent collaboration in LLM-based multi-agent system oriented software.txt @段1) || 摘要 | Facilitating Trustworthy Human-Agent Collaboration in LLM-based Multi-Agent System oriented Software Engineering Krishna Ronanki krishna.ronanki@gu.se Chalmers University of Technology | University of Gothenburg Gothenburg, Sweden Abstract Multi-agent autonomous systems (MAS) are better at addressing challenges that spans across multiple domains than singular au- tonomous agents. This holds true within the field of software engi- neering (SE) as well. The state-of-the-art research on MAS within SE focuses on integ… | 0.72 | “Facilitating Trustworthy Human-Agent Collaboration in LLM-based Multi-Agent System oriented Software Engineering Krishna Ronanki krishna.ronanki@gu.se Chalmers University of Technology | University of Gothenburg Gothenburg, Sweden” (Facilitating trustworthy human-agent collaboration in LLM-based multi-agent system oriented software.txt @段1) || 研究方法 | 1 Introduction Software Engineering (SE) is an inherently collaborative field [18] and SE collaboration is often artefact-based [23]. The advantage of SE collaboration being an artefact-based process is that the artefact evaluation against the ground truth can be automated [ 3]. This, alongside the observed fact that software engineers are increas- ingly becoming more reliant on the support of automation with increasing complexity in software systems [ 24], gave rise to the field of automated software engineering … | 0.82 | “To addressed this challenge that hinders human-agent collabo- ration in LMA oriented SE, a RACI-based framework along with implementation guidelines were developed to help define the roles and responsibilities of the LMA system and humans within a SDLC.” (Facilitating trustworthy human-agent collaboration in LLM-based multi-agent system oriented software.txt @段4) || 主要结论 | N/A | 0.50 | N/A |

- 主题标签：GUI/Computer-Use Agent 安全, Web/Browser Agent 安全, 多智能体对抗与协作
- 方法标签：基准/Benchmark, 多智能体/Multi-Agent, 多模态/Multimodal
- 源文件：`Facilitating trustworthy human-agent collaboration in LLM-based multi-agent system oriented software.txt`

</details>
<details><summary>95. Framework for Computer-Use Agents（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Framework for Computer-Use Agents | 0.92 | “Framework for Computer-Use Agents” (AgentSentinel An end-to-end and real-time security defense framework for computer-use agents.txt @段8) || 作者 | AgentSentinel: An End-to-End and Real-Time Security Defense | 0.90 | “AgentSentinel: An End-to-End and Real-Time Security Defense” (AgentSentinel An end-to-end and real-time security defense framework for computer-use agents.txt @段1) || 发表年份 | 2025 | 0.90 | “Published: 19 November 2025” (AgentSentinel An end-to-end and real-time security defense framework for computer-use agents.txt @段1) || 期刊/会议 | CCS '25: ACM SIGSAC Conference on | 0.88 | “CCS '25: ACM SIGSAC Conference on” (AgentSentinel An end-to-end and real-time security defense framework for computer-use agents.txt @段1) || 关键词 | computer, end, 2025, agentsentinel, framework, real, task, tool, user, chen | 0.78 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3719027.3765064 . . RESEARCH-ARTICLE AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents HAITAO HU, ShanghaiTech University, Shanghai, China . PENG CHEN . YANPENG ZHAO . YUQI CHEN, ShanghaiTech University, Shanghai, China . . .” (AgentSentinel An end-to-end and real-time security defense framework for computer-use agents.txt @段1) || 摘要 | . . Latest updates: hps://dl.acm.org/doi/10.1145/3719027.3765064 . . RESEARCH-ARTICLE AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents HAITAO HU, ShanghaiTech University, Shanghai, China . PENG CHEN . YANPENG ZHAO . YUQI CHEN, ShanghaiTech University, Shanghai, China . . . Open Access Support provided by: . ShanghaiTech University . PDF Download 3719027.3765064.pdf 20 January 2026 Total Citations: 0 Total Downloads: 1486 . . Published: 19 November 2025 . . Citation in … | 0.72 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3719027.3765064 . . RESEARCH-ARTICLE AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents HAITAO HU, ShanghaiTech University, Shanghai, China . PENG CHEN . YANPENG ZHAO . YUQI CHEN, ShanghaiTech University, Shanghai, China . . .” (AgentSentinel An end-to-end and real-time security defense framework for computer-use agents.txt @段1) || 研究方法 | Abstract Large Language Models (LLMs) have been increasingly integrated into computer-use agents, which can autonomously operate tools on a user’s computer to accomplish complex tasks. However, due to the inherently unstable and unpredictable nature of LLM outputs, they may issue unintended tool commands or incorrect inputs, lead- ing to potentially harmful operations. Unlike traditional security risks stemming from insecure user prompts, tool execution results from LLM-driven decisions introduce new and unique se… | 0.82 | “To mitigate these risks, we propose AgentSen- tinel, an end-to-end, real-time defense framework designed to miti- gate potential security threats on a user’s computer.” (AgentSentinel An end-to-end and real-time security defense framework for computer-use agents.txt @段4) || 主要结论 | 1 Introduction Large Language Models (LLMs), such as GPT-4o [32] and Claude 3.7 Sonnet [2], represent the most advanced natural language pro- cessing technologies to date. These models accept user prompts, describing problems, requirements, or tasks, and generate reason- able and contextually appropriate responses. Due to their powerful task-processing capabilities, LLMs have been widely adopted across numerous domains and integrated into a variety of LLM-based ap- plications. To further enhance their capabilities… | 0.80 | “• Our evaluation results show that AgentSentinel achieves an average defense success rate of 79.6%, significantly outper- forming all baseline defenses.” (AgentSentinel An end-to-end and real-time security defense framework for computer-use agents.txt @段6) |

- 主题标签：GUI/Computer-Use Agent 安全, Web/Browser Agent 安全, 多智能体对抗与协作, 防御与对齐机制
- 方法标签：仿真/Simulator, 基准/Benchmark, 系统防御/Defense
- 源文件：`AgentSentinel An end-to-end and real-time security defense framework for computer-use agents.txt`

</details>
<details><summary>96. Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework | 0.92 | “Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework” (Fuzz-testing meets LLM-based agents An automated and efficient framework for jailbreaking text-to-i.txt @段1) || 作者 | Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework | 0.90 | “Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework” (Fuzz-testing meets LLM-based agents An automated and efficient framework for jailbreaking text-to-i.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2408.00523v3  [cs.CR]  24 Jun 2025” (Fuzz-testing meets LLM-based agents An automated and efficient framework for jailbreaking text-to-i.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2408.00523v3  [cs.CR]  24 Jun 2025” (Fuzz-testing meets LLM-based agents An automated and efficient framework for jailbreaking text-to-i.txt @段1) || 关键词 | prompts, jailbreak, image, jailfuzzer, t2i, text, filters, content, mutation, generate | 0.78 | “Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking Text-To-Image Generation Models Yingkai Dong1, Xiangtao Meng1, Ning Yu2, Zheng Li1,3,4(B), Shanqing Guo1,3,4(B) 1School of Cyber Science and Technology, Shandong University 2Netflix Eyeline Studios 3State Key Laboratory of Crypto” (Fuzz-testing meets LLM-based agents An automated and efficient framework for jailbreaking text-to-i.txt @段1) || 摘要 | Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking Text-To-Image Generation Models Yingkai Dong1, Xiangtao Meng1, Ning Yu2, Zheng Li1,3,4(B), Shanqing Guo1,3,4(B) 1School of Cyber Science and Technology, Shandong University 2Netflix Eyeline Studios 3State Key Laboratory of Cryptography and Digital Economy Security, Shandong University 4Shandong Key Laboratory of Artificial Intelligence Security, Shandong University {dongyingkai,mengxiangtao}@mail.sdu.edu.cn, ningyu.hust@gmai… | 0.72 | “Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking Text-To-Image Generation Models Yingkai Dong1, Xiangtao Meng1, Ning Yu2, Zheng Li1,3,4(B), Shanqing Guo1,3,4(B) 1School of Cyber Science and Technology, Shandong University 2Netflix Eyeline Studios 3State Key Laboratory of Crypto” (Fuzz-testing meets LLM-based agents An automated and efficient framework for jailbreaking text-to-i.txt @段1) || 研究方法 | 1.1 Our Contributions In this paper, we introduce JailFuzzer, an innovative frame- work designed to automatically and efficiently generate con- cise, meaningful, and fluent jailbreak prompts against T2I models. Specifically, JailFuzzer leverages the principles of fuzz-testing and is built on advanced Large Language Model (LLM)-based agents, allowing it to efficiently and effectively navigate the vast space of possible inputs. JailFuzzer ’s archi- tecture centers on two key design elements, which contribute to its … | 0.82 | “In summary, we make the following contributions: • We introduce JailFuzzer, a fuzz-testing-driven jailbreak attack framework designed to automatically and effi- ciently generate jailbreak prompts with only black-box access to victim T2I models.” (Fuzz-testing meets LLM-based agents An automated and efficient framework for jailbreaking text-to-i.txt @段4) || 主要结论 | 1.1 Our Contributions In this paper, we introduce JailFuzzer, an innovative frame- work designed to automatically and efficiently generate con- cise, meaningful, and fluent jailbreak prompts against T2I models. Specifically, JailFuzzer leverages the principles of fuzz-testing and is built on advanced Large Language Model (LLM)-based agents, allowing it to efficiently and effectively navigate the vast space of possible inputs. JailFuzzer ’s archi- tecture centers on two key design elements, which contribute to its … | 0.80 | “The results show thatJailFuzzer not only maintains high semantic similarity in gener- ated prompts but also achieves a remarkably high attack success rate using fewer queries and natural prompts, outperforming existing methods across all metrics.” (Fuzz-testing meets LLM-based agents An automated and efficient framework for jailbreaking text-to-i.txt @段4) |

- 主题标签：GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击, Web/Browser Agent 安全
- 方法标签：多模态/Multimodal, 越狱/Jailbreak
- 源文件：`Fuzz-testing meets LLM-based agents An automated and efficient framework for jailbreaking text-to-i.txt`

</details>
<details><summary>97. Generalized Visual CAPTCHA Solving（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Generalized Visual CAPTCHA Solving | 0.92 | “Generalized Visual CAPTCHA Solving” (Are CAPTCHAs still bot-hard Generalized visual CAPTCHA solving with agentic vision language model.txt @段8) || 作者 | Xiwen Teoh, Shanghai Jiao Tong University; National University of Singapore; | 0.90 | “Xiwen Teoh, Shanghai Jiao Tong University; National University of Singapore;” (Are CAPTCHAs still bot-hard Generalized visual CAPTCHA solving with agentic vision language model.txt @段1) || 发表年份 | 2025 | 0.86 | “August 13–15, 2025 • Seattle, WA, USA” (Are CAPTCHAs still bot-hard Generalized visual CAPTCHA solving with agentic vision language model.txt @段1) || 期刊/会议 | This paper is included in the Proceedings of the | 0.88 | “This paper is included in the Proceedings of the” (Are CAPTCHAs still bot-hard Generalized visual CAPTCHA solving with agentic vision language model.txt @段1) || 关键词 | captcha, visual, challenges, solving, halligan, frame, objective, search, solution, which | 0.78 | “This paper is included in the Proceedings of the 34th USENIX Security Symposium. August 13–15, 2025 • Seattle, WA, USA 978-1-9391 33-52-6 Open access to the Proceedings of the 34th USENIX Security Symposium is sponsored by USENIX. Are CAPTCHAs Still Bot-hard? Generalized Visual CAPTCHA Solving with Agentic Vision Langu” (Are CAPTCHAs still bot-hard Generalized visual CAPTCHA solving with agentic vision language model.txt @段1) || 摘要 | This paper is included in the Proceedings of the 34th USENIX Security Symposium. August 13–15, 2025 • Seattle, WA, USA 978-1-9391 33-52-6 Open access to the Proceedings of the 34th USENIX Security Symposium is sponsored by USENIX. Are CAPTCHAs Still Bot-hard? Generalized Visual CAPTCHA Solving with Agentic Vision Language Model Xiwen Teoh, Shanghai Jiao Tong University; National University of Singapore; Yun Lin, Shanghai Jiao Tong University; Siqi Li and Ruofan Liu, National University of Singapore; Avi Sollomoni … | 0.72 | “This paper is included in the Proceedings of the 34th USENIX Security Symposium. August 13–15, 2025 • Seattle, WA, USA 978-1-9391 33-52-6 Open access to the Proceedings of the 34th USENIX Security Symposium is sponsored by USENIX. Are CAPTCHAs Still Bot-hard? Generalized Visual CAPTCHA Solving with Agentic Vision Langu” (Are CAPTCHAs still bot-hard Generalized visual CAPTCHA solving with agentic vision language model.txt @段1) || 研究方法 | 1 Introduction Visual CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart) are challenge- ∗Corresponding author ` Figure 1: Diverse visual CAPTCHA challenges adopted in practice, expected to be bot-hard but human-friendly. response tests that verify if a user is human or bot [ 66], which establishes proof-of-personhood by presenting users with in- teractive puzzles, where solving them is a key step toward passing the CAPTCHA. It has been a conventional web-abuse prevention mechanis… | 0.82 | “We show that a visual challenge can be reduced to a search problem where (i) its instruction (in natural language) can be transformed into an optimization objective and (ii) its body or interface (comprising visible entities) can be transformed into a search space for the objective.” (Are CAPTCHAs still bot-hard Generalized visual CAPTCHA solving with agentic vision language model.txt @段3) || 主要结论 | 1 Introduction Visual CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart) are challenge- ∗Corresponding author ` Figure 1: Diverse visual CAPTCHA challenges adopted in practice, expected to be bot-hard but human-friendly. response tests that verify if a user is human or bot [ 66], which establishes proof-of-personhood by presenting users with in- teractive puzzles, where solving them is a key step toward passing the CAPTCHA. It has been a conventional web-abuse prevention mechanis… | 0.80 | “We show that a visual challenge can be reduced to a search problem where (i) its instruction (in natural language) can be transformed into an optimization objective and (ii) its body or interface (comprising visible entities) can be transformed into a search space for the objective.” (Are CAPTCHAs still bot-hard Generalized visual CAPTCHA solving with agentic vision language model.txt @段3) |

- 主题标签：Web/Browser Agent 安全
- 方法标签：多模态/Multimodal, 形式化/Optimization
- 源文件：`Are CAPTCHAs still bot-hard Generalized visual CAPTCHA solving with agentic vision language model.txt`

</details>
<details><summary>98. GitHub README.MD Summarization（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | GitHub README.MD Summarization | 0.92 | “GitHub README.MD Summarization” (Teamwork makes the dream work LLMs-based agents for GitHub README.MD summarization.txt @段2) || 作者 | Hanoi, Vietnam | 0.90 | “Hanoi, Vietnam” (Teamwork makes the dream work LLMs-based agents for GitHub README.MD summarization.txt @段1) || 发表年份 | 2025 | 0.90 | “and Davide Di Ruscio. 2025. Teamwork makes the dream work: LLMs-Based” (Teamwork makes the dream work LLMs-based agents for GitHub README.MD summarization.txt @段1) || 期刊/会议 | Conference on the Foundations of Software Engineering (FSE Companion ’25), | 0.88 | “Conference on the Foundations of Software Engineering (FSE Companion ’25),” (Teamwork makes the dream work LLMs-based agents for GitHub README.MD summarization.txt @段1) || 关键词 | aquila, 2025, nguyen, readme, summarization, university, acm, hanoi, metagente, software | 0.78 | “Teamwork makes the dream work: LLMs-Based Agents for GitHub README.MD Summarization Duc S. H. Nguyen duc.nsh231061m@sis.hust.edu.vn Hanoi University of Science and Technology Hanoi, Vietnam Bach G. Truong bach.tg210087@sis.hust.edu.vn Hanoi University of Science and Technology Hanoi, Vietnam Phuong T. Nguyen phuong.ngu” (Teamwork makes the dream work LLMs-based agents for GitHub README.MD summarization.txt @段1) || 摘要 | Teamwork makes the dream work: LLMs-Based Agents for GitHub README.MD Summarization Duc S. H. Nguyen duc.nsh231061m@sis.hust.edu.vn Hanoi University of Science and Technology Hanoi, Vietnam Bach G. Truong bach.tg210087@sis.hust.edu.vn Hanoi University of Science and Technology Hanoi, Vietnam Phuong T. Nguyen phuong.nguyen@univaq.it University of L’Aquila 67100 L’Aquila, Italy Juri Di Rocco juri.dirocco@univaq.it University of L’Aquila 67100 L’Aquila, Italy Davide Di Ruscio davide.diruscio@univaq.it University of L… | 0.72 | “Teamwork makes the dream work: LLMs-Based Agents for GitHub README.MD Summarization Duc S. H. Nguyen duc.nsh231061m@sis.hust.edu.vn Hanoi University of Science and Technology Hanoi, Vietnam Bach G. Truong bach.tg210087@sis.hust.edu.vn Hanoi University of Science and Technology Hanoi, Vietnam Phuong T. Nguyen phuong.ngu” (Teamwork makes the dream work LLMs-based agents for GitHub README.MD summarization.txt @段1) || 研究方法 | 1 Introduction Large Language Models (LLMs) have transformed how we approach dif- ferent tasks such as natural language understanding, creative writing, and software engineering [18, 19]. However, despite the individual strengths of LLMs, no single model can fully address the vast range of human language and problem domains. For example, while LLMs like GPT have excelled in natural language understanding, their performance on tasks requiring domain-specific expertise or complex multi-step reasoning remains limited… | 0.82 | “To illustrate a practical application of multi-agent LLM systems, we propose a novel framework targeting the problem of summarizing SE arti- facts.” (Teamwork makes the dream work LLMs-based agents for GitHub README.MD summarization.txt @段7) || 主要结论 | Abstract The proliferation of Large Language Models (LLMs) in recent years has realized many applications in various domains. Being trained with a huge of amount of data coming from various sources, LLMs can be deployed to solve different tasks, including those in Software Engineering (SE). Though they have been widely adopted, the potential of using LLMs cooperatively has not been thoroughly investigated. In this paper, we proposed Metagente as a novel approach to amplify the synergy of various LLMs. Metagente is… | 0.80 | “The results show that our proposed approach works efficiently and effectively, consuming a small amount of data for fine-tuning but still getting a high accuracy, thus substantially outperforming the baselines.” (Teamwork makes the dream work LLMs-based agents for GitHub README.MD summarization.txt @段5) |

- 主题标签：Web/Browser Agent 安全, 多智能体对抗与协作
- 方法标签：基准/Benchmark, 多智能体/Multi-Agent
- 源文件：`Teamwork makes the dream work LLMs-based agents for GitHub README.MD summarization.txt`

</details>
<details><summary>99. Graphormer-Guided Task Planning: Beyond Static Rules with LLM（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Graphormer-Guided Task Planning: Beyond Static Rules with LLM | 0.92 | “Graphormer-Guided Task Planning: Beyond Static Rules with LLM” (Graphormer-guided task planning Beyond static rules with LLM safety perception.txt @段1) || 作者 | Wanjing Huang1 , Tongjie Pan 2, Yalan Ye2 | 0.90 | “Wanjing Huang1 , Tongjie Pan 2, Yalan Ye2” (Graphormer-guided task planning Beyond static rules with LLM safety perception.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2503.06866v1  [cs.RO]  10 Mar 2025” (Graphormer-guided task planning Beyond static rules with LLM safety perception.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2503.06866v1  [cs.RO]  10 Mar 2025” (Graphormer-guided task planning Beyond static rules with LLM safety perception.txt @段1) || 关键词 | task, risk, planning, graph, rule, structured, constraints, robotic, framework, perception | 0.78 | “Graphormer-Guided Task Planning: Beyond Static Rules with LLM Safety Perception Wanjing Huang1 , Tongjie Pan 2, Yalan Ye2 Abstract— Recent advancements in large language models (LLMs) have expanded their role in robotic task planning. However, while LLMs have been explored for generating feasible task sequences, their” (Graphormer-guided task planning Beyond static rules with LLM safety perception.txt @段1) || 摘要 | Graphormer-Guided Task Planning: Beyond Static Rules with LLM Safety Perception Wanjing Huang1 , Tongjie Pan 2, Yalan Ye2 Abstract— Recent advancements in large language models (LLMs) have expanded their role in robotic task planning. However, while LLMs have been explored for generating feasible task sequences, their ability to ensure safe task execu- tion remains underdeveloped. Existing methods struggle with structured risk perception, making them inadequate for safety- critical applications where low-latency h… | 0.72 | “Graphormer-Guided Task Planning: Beyond Static Rules with LLM Safety Perception Wanjing Huang1 , Tongjie Pan 2, Yalan Ye2 Abstract— Recent advancements in large language models (LLMs) have expanded their role in robotic task planning. However, while LLMs have been explored for generating feasible task sequences, their” (Graphormer-guided task planning Beyond static rules with LLM safety perception.txt @段1) || 研究方法 | 17 hazards if near power outlets." 18 } We construct a synthetic risk-aware dataset by embedding these risk relationships into AI2-THOR environments. We introduce randomized human-agent interactions (e.g., chil- dren, adults, pets) to ensure diverse safety-critical scenarios. Each sample is labeled with a danger score, computed as: S(vi, vj) = r(vi, vj) × SP(vi, vj) (2) where the SP refers to spatial proximity, which accounts for spatial scene configurations, and in experiment part we chose: SP = ( 1 distance , if… | 0.82 | “CONCLUSION AND FUTURE WORK In this work, we introduced a graphormer-enhanced risk- aware task planning framework that integrates structured safety perception with LLM-driven decision-making for robotic agents in real-time environments.” (Graphormer-guided task planning Beyond static rules with LLM safety perception.txt @段8) || 主要结论 | Graphormer-Guided Task Planning: Beyond Static Rules with LLM Safety Perception Wanjing Huang1 , Tongjie Pan 2, Yalan Ye2 Abstract— Recent advancements in large language models (LLMs) have expanded their role in robotic task planning. However, while LLMs have been explored for generating feasible task sequences, their ability to ensure safe task execu- tion remains underdeveloped. Existing methods struggle with structured risk perception, making them inadequate for safety- critical applications where low-latency h… | 0.80 | “While LLMs enable flexible, generalizable task reasoning [2], [3], they lack structured risk perception, making them unsuitable for safety-critical Reinforcement learning (RL) approaches [4] can optimize decision-making in controlled settings but require extensive training and fail to generalize in low-latency scenario” (Graphormer-guided task planning Beyond static rules with LLM safety perception.txt @段1) |

- 主题标签：GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击, Web/Browser Agent 安全
- 方法标签：仿真/Simulator, 基准/Benchmark, 多模态/Multimodal
- 源文件：`综述参考文献\Graphormer-guided task planning Beyond static rules with LLM safety perception.txt`

</details>
<details><summary>100. Indirect Prompt Injection（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Indirect Prompt Injection | 0.92 | “Indirect Prompt Injection” (EVA Red-teaming GUI agents via evolving indirect prompt injection.txt @段2) || 作者 | Yijie Lu1, Tianjie Ju2, Manman Zhao1, Xinbei Ma2, Yuan Guo2, Zhuosheng Zhang2∗ | 0.90 | “Yijie Lu1, Tianjie Ju2, Manman Zhao1, Xinbei Ma2, Yuan Guo2, Zhuosheng Zhang2∗” (EVA Red-teaming GUI agents via evolving indirect prompt injection.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2505.14289v1  [cs.AI]  20 May 2025” (EVA Red-teaming GUI agents via evolving indirect prompt injection.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2505.14289v1  [cs.AI]  20 May 2025” (EVA Red-teaming GUI agents via evolving indirect prompt injection.txt @段1) || 关键词 | gui, injection, prompt, visual, indirect, static, attention, content, user, behavior | 0.78 | “EV A: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection Yijie Lu1, Tianjie Ju2, Manman Zhao1, Xinbei Ma2, Yuan Guo2, Zhuosheng Zhang2∗ 1Wuhan University , 2Shanghai Jiao Tong University {luin.j, zhaomm}@whu.edu.cn {jometeorie, sjtumaxb, gy2022, zhangzs}@sjtu.edu.cn” (EVA Red-teaming GUI agents via evolving indirect prompt injection.txt @段1) || 摘要 | EV A: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection Yijie Lu1, Tianjie Ju2, Manman Zhao1, Xinbei Ma2, Yuan Guo2, Zhuosheng Zhang2∗ 1Wuhan University , 2Shanghai Jiao Tong University {luin.j, zhaomm}@whu.edu.cn {jometeorie, sjtumaxb, gy2022, zhangzs}@sjtu.edu.cn Abstract As multimodal agents are increasingly trained to operate graphical user interfaces (GUIs) to complete user tasks, they face a growing threat from indirect prompt injection—attacks in which misleading instructions are embedded into t… | 0.72 | “EV A: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection Yijie Lu1, Tianjie Ju2, Manman Zhao1, Xinbei Ma2, Yuan Guo2, Zhuosheng Zhang2∗ 1Wuhan University , 2Shanghai Jiao Tong University {luin.j, zhaomm}@whu.edu.cn {jometeorie, sjtumaxb, gy2022, zhangzs}@sjtu.edu.cn” (EVA Red-teaming GUI agents via evolving indirect prompt injection.txt @段1) || 研究方法 | Abstract As multimodal agents are increasingly trained to operate graphical user interfaces (GUIs) to complete user tasks, they face a growing threat from indirect prompt injection—attacks in which misleading instructions are embedded into the agent’s visual environment, such as pop-ups or chat messages, and misinterpreted as part of the intended task. A typical example is environmental injection, in which GUI elements are manipulated to influence agent behavior without directly modifying the user prompt. To addre… | 0.82 | “To address these emerging attacks, we propose EV A, a red-teaming framework for indirect prompt injection which transforms the attack into a closed- loop optimization by continuously monitoring an agent’s attention distribution over the GUI and updating adversarial cues—keywords, phrasing, and layout—in response.” (EVA Red-teaming GUI agents via evolving indirect prompt injection.txt @段2) || 主要结论 | 5.2 Attention-Guided Vulnerability in GUI Agents Prior work has noted that GUI agents can be distracted by seemingly benign interface text [ 6, 7]. Our analysis strengthens this view by showing that what matters most is how tightly the model’s attention converges on a single region versus how broadly it is dispersed across the screen. Figure 4 illustrates the contrast. A centered pop-up concentrates attention on the confirm button, and the agent immediately clicks it, whereas a chat-based link distributes attentio… | 0.80 | “Together, these findings indicate that concentrated attention—especially when boosted by semantic overlap with the task—makes indirect prompt injection far more effective than injections that compete for attention across many interface elements.” (EVA Red-teaming GUI agents via evolving indirect prompt injection.txt @段18) |

- 主题标签：GUI/Computer-Use Agent 安全, Prompt Injection/环境注入
- 方法标签：仿真/Simulator, 多模态/Multimodal, 形式化/Optimization, 提示注入/Prompt Injection, 红队/Red-Teaming
- 源文件：`综述参考文献\EVA Red-teaming GUI agents via evolving indirect prompt injection.txt`

</details>
<details><summary>101. Issue-Oriented Code Review（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Issue-Oriented Code Review | 0.92 | “Issue-Oriented Code Review” (AutoReview An LLM-based multi-agent system for security issue-oriented code review.txt @段2) || 作者 | Shenzhen, China | 0.90 | “Shenzhen, China” (AutoReview An LLM-based multi-agent system for security issue-oriented code review.txt @段1) || 发表年份 | 2025 | 0.90 | “Yujia Chen. 2025. AutoReview: An LLM-based Multi-Agent System for” (AutoReview An LLM-based multi-agent system for security issue-oriented code review.txt @段1) || 期刊/会议 | Security Issue-Oriented Code Review. In33rd ACM International Conference | 0.88 | “Security Issue-Oriented Code Review. In33rd ACM International Conference” (AutoReview An LLM-based multi-agent system for security issue-oriented code review.txt @段1) || 关键词 | code, issue, review, acm, oriented, repair, detection, software, system, through | 0.78 | “AutoReview: An LLM-based Multi-Agent System for Security Issue-Oriented Code Review Yujia Chen yujiachen@stu.hit.edu.cn Harbin Institute of Technology Shenzhen, China” (AutoReview An LLM-based multi-agent system for security issue-oriented code review.txt @段1) || 摘要 | AutoReview: An LLM-based Multi-Agent System for Security Issue-Oriented Code Review Yujia Chen yujiachen@stu.hit.edu.cn Harbin Institute of Technology Shenzhen, China Abstract Software vulnerabilities can lead to severe security issues such as data breaches, financial losses, and service disruptions, making security issue-oriented code review a crucial part of the devel- opment process. Traditional approaches struggle with analyzing complex code and providing explanations, while large language models (LLMs) show p… | 0.72 | “AutoReview: An LLM-based Multi-Agent System for Security Issue-Oriented Code Review Yujia Chen yujiachen@stu.hit.edu.cn Harbin Institute of Technology Shenzhen, China” (AutoReview An LLM-based multi-agent system for security issue-oriented code review.txt @段1) || 研究方法 | Abstract Software vulnerabilities can lead to severe security issues such as data breaches, financial losses, and service disruptions, making security issue-oriented code review a crucial part of the devel- opment process. Traditional approaches struggle with analyzing complex code and providing explanations, while large language models (LLMs) show promise in code review but do not focus on security-related issues. To address these limitations, we propose AutoReview, an LLM-based multi-agent system for security co… | 0.82 | “To address these limitations, we propose AutoReview, an LLM-based multi-agent system for security code review.Itintegratesthreeagents:(1) Issue Detectoridentifyingpo- tential vulnerabilities using knowledge-level retrieval-augmented generation, (2)Issue Locator pinpoints the vulnerability positions through graph-based” (AutoReview An LLM-based multi-agent system for security issue-oriented code review.txt @段2) || 主要结论 | Abstract Software vulnerabilities can lead to severe security issues such as data breaches, financial losses, and service disruptions, making security issue-oriented code review a crucial part of the devel- opment process. Traditional approaches struggle with analyzing complex code and providing explanations, while large language models (LLMs) show promise in code review but do not focus on security-related issues. To address these limitations, we propose AutoReview, an LLM-based multi-agent system for security co… | 0.80 | “To address these limitations, we propose AutoReview, an LLM-based multi-agent system for security code review.Itintegratesthreeagents:(1) Issue Detectoridentifyingpo- tential vulnerabilities using knowledge-level retrieval-augmented generation, (2)Issue Locator pinpoints the vulnerability positions through graph-based” (AutoReview An LLM-based multi-agent system for security issue-oriented code review.txt @段2) |

- 主题标签：多智能体对抗与协作
- 方法标签：多智能体/Multi-Agent, 形式化/Optimization
- 源文件：`AutoReview An LLM-based multi-agent system for security issue-oriented code review.txt`

</details>
<details><summary>102. JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks | 0.92 | “JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks” (JailbreakRadar Comprehensive assessment of jailbreak attacks against LLMs.txt @段1) || 作者 | researchers have proposed different jailbreak attacks in depth, | 0.90 | “researchers have proposed different jailbreak attacks in depth,” (JailbreakRadar Comprehensive assessment of jailbreak attacks against LLMs.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2402.05668v3  [cs.CR]  26 May 2025” (JailbreakRadar Comprehensive assessment of jailbreak attacks against LLMs.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2402.05668v3  [cs.CR]  26 May 2025” (JailbreakRadar Comprehensive assessment of jailbreak attacks against LLMs.txt @段1) || 关键词 | jailbreak, generation, taxonomy, comprehensive, prompts, human, assessment, forbidden, heuristic, how | 0.78 | “JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs Junjie Chu Yugeng Liu Ziqing Yang Xinyue Shen Michael Backes Yang Zhang ♣ CISPA Helmholtz Center for Information Security” (JailbreakRadar Comprehensive assessment of jailbreak attacks against LLMs.txt @段1) || 摘要 | JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs Junjie Chu Yugeng Liu Ziqing Yang Xinyue Shen Michael Backes Yang Zhang ♣ CISPA Helmholtz Center for Information Security Abstract Jailbreak attacks aim to bypass the LLMs’ safeguards. While researchers have proposed different jailbreak attacks in depth, they have done so in isolation—either with unaligned set- tings or comparing a limited range of methods. To fill this gap, we present a large-scale evaluation of various jailbreak attacks. … | 0.72 | “JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs Junjie Chu Yugeng Liu Ziqing Yang Xinyue Shen Michael Backes Yang Zhang ♣ CISPA Helmholtz Center for Information Security” (JailbreakRadar Comprehensive assessment of jailbreak attacks against LLMs.txt @段1) || 研究方法 | 3.5 and GPT-4, are continuously updated to improve the util- ity of the model by incorporating feedback and insights from users and developers. In addition, improvements in safety alignment are commonly employed during the update process of these models without release notes, rendering many pre- 12https://status.openai.com/history. 14 19.01.2402.02.2416.02.2401.03.2415.03.2429.03.2412.04.2426.04.2410.05.2424.05.2407.06.2421.06.2405.07.2419.07.2402.08.2416.08.2430.08.24 Measurement Date (DD-MM-YY) 0.0 0.2 0.4 0.6 0… | 0.82 | “G.5 Heuristic-Based Method Methods in this category automatically optimize the jail- break prompts with different heuristic optimization algo- rithms [69, 91], including mutation, random search, and ge- netic algorithm.” (JailbreakRadar Comprehensive assessment of jailbreak attacks against LLMs.txt p.6584 @段35) || 主要结论 | References [1] Sahar Abdelnabi, Kai Greshake, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not What You’ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. In Workshop on Security and Artificial Intelligence (AISec), pages 79–90. ACM, 2023. 2 [2] EU AI Act. https://artificialintelligenceact.eu/ , 2024. 1, 21 [3] Gabriel Alon and Michael Kamfonas. Detecting Language Model Attacks with Perplexity. CoRR abs/2308.14132, 2023. 2, 6, 21 [4] Amazon. … | 0.80 | “Our results indicate that, for the black-box scenario, token counts of the human-based jailbreak prompt and many approaches that used this prompt as the initial prompt are significantly larger than others.” (JailbreakRadar Comprehensive assessment of jailbreak attacks against LLMs.txt p.6584 @段30) |

- 主题标签：Jailbreak/越狱攻击, Prompt Injection/环境注入, 防御与对齐机制
- 方法标签：基准/Benchmark, 提示注入/Prompt Injection, 系统防御/Defense, 越狱/Jailbreak
- 源文件：`综述参考文献\JailbreakRadar Comprehensive assessment of jailbreak attacks against LLMs.txt`

</details>
<details><summary>103. LLM Agents Should Employ Security Principles（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | LLM Agents Should Employ Security Principles | 0.92 | “LLM Agents Should Employ Security Principles” (LLM agents should employ security principles.txt @段2) || 作者 | Kaiyuan Zhang, Zian Su, Pin-Yu Chen†, Elisa Bertino, Xiangyu Zhang, Ninghui Li | 0.90 | “Kaiyuan Zhang, Zian Su, Pin-Yu Chen†, Elisa Bertino, Xiangyu Zhang, Ninghui Li” (LLM agents should employ security principles.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2505.24019v1  [cs.CR]  29 May 2025” (LLM agents should employ security principles.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2505.24019v1  [cs.CR]  29 May 2025” (LLM agents should employ security principles.txt @段1) || 关键词 | arxiv, task, policy, user, agentsandbox, principles, 2024, data, preprint, prompt | 0.78 | “arXiv:2505.24019v1 [cs.CR] 29 May 2025 LLM Agents Should Employ Security Principles Kaiyuan Zhang, Zian Su, Pin-Yu Chen†, Elisa Bertino, Xiangyu Zhang, Ninghui Li Purdue University,†IBM Research {zhan4057, su284, bertino, xyzhang, ninghui}@purdue.edu,†pin-yu.chen@ibm.com Large Language Model (LLM) agents show considera” (LLM agents should employ security principles.txt @段1) || 摘要 | arXiv:2505.24019v1 [cs.CR] 29 May 2025 LLM Agents Should Employ Security Principles Kaiyuan Zhang, Zian Su, Pin-Yu Chen†, Elisa Bertino, Xiangyu Zhang, Ninghui Li Purdue University,†IBM Research {zhan4057, su284, bertino, xyzhang, ninghui}@purdue.edu,†pin-yu.chen@ibm.com Large Language Model (LLM) agents show considerable promise for automating complex tasks using contextual reasoning; however, interactions involving multiple agents and the system’s susceptibility to prompt injection and other forms of context man… | 0.72 | “arXiv:2505.24019v1 [cs.CR] 29 May 2025 LLM Agents Should Employ Security Principles Kaiyuan Zhang, Zian Su, Pin-Yu Chen†, Elisa Bertino, Xiangyu Zhang, Ninghui Li Purdue University,†IBM Research {zhan4057, su284, bertino, xyzhang, ninghui}@purdue.edu,†pin-yu.chen@ibm.com Large Language Model (LLM) agents show considera” (LLM agents should employ security principles.txt @段1) || 研究方法 | arXiv:2505.24019v1 [cs.CR] 29 May 2025 LLM Agents Should Employ Security Principles Kaiyuan Zhang, Zian Su, Pin-Yu Chen†, Elisa Bertino, Xiangyu Zhang, Ninghui Li Purdue University,†IBM Research {zhan4057, su284, bertino, xyzhang, ninghui}@purdue.edu,†pin-yu.chen@ibm.com Large Language Model (LLM) agents show considerable promise for automating complex tasks using contextual reasoning; however, interactions involving multiple agents and the system’s susceptibility to prompt injection and other forms of context man… | 0.82 | “To illustrate how these security principles help bridge the gap in LLM agents and security, we propose a security framework called AgentSandbox, which applies these principles directly into the fabric of future agent communication protocols.” (LLM agents should employ security principles.txt @段1) || 主要结论 | arXiv:2505.24019v1 [cs.CR] 29 May 2025 LLM Agents Should Employ Security Principles Kaiyuan Zhang, Zian Su, Pin-Yu Chen†, Elisa Bertino, Xiangyu Zhang, Ninghui Li Purdue University,†IBM Research {zhan4057, su284, bertino, xyzhang, ninghui}@purdue.edu,†pin-yu.chen@ibm.com Large Language Model (LLM) agents show considerable promise for automating complex tasks using contextual reasoning; however, interactions involving multiple agents and the system’s susceptibility to prompt injection and other forms of context man… | 0.80 | “Psychological Acceptability The principle of psychological acceptability emphasizes that security mechanisms should be user-friendly and intuitive; that is, security measures should not significantly increase the difficulty or inconvenience for users to access resources or perform actions.” (LLM agents should employ security principles.txt @段1) |

- 主题标签：GUI/Computer-Use Agent 安全, Prompt Injection/环境注入, 防御与对齐机制, 隐私与记忆风险
- 方法标签：多模态/Multimodal, 提示注入/Prompt Injection, 系统防御/Defense
- 源文件：`综述参考文献\LLM agents should employ security principles.txt`

</details>
<details><summary>104. LLM-Agents Driven Automated Simulation Testing（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | LLM-Agents Driven Automated Simulation Testing | 0.92 | “LLM-Agents Driven Automated Simulation Testing” (LLM-agents driven automated simulation testing and analysis of small uncrewed aerial systems.txt @段1) || 作者 | Venkata Sai Aswath Duvvuru and Bohan Zhang | 0.90 | “Venkata Sai Aswath Duvvuru and Bohan Zhang” (LLM-agents driven automated simulation testing and analysis of small uncrewed aerial systems.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2501.11864v1  [cs.SE]  21 Jan 2025” (LLM-agents driven automated simulation testing and analysis of small uncrewed aerial systems.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2501.11864v1  [cs.SE]  21 Jan 2025” (LLM-agents driven automated simulation testing and analysis of small uncrewed aerial systems.txt @段1) || 关键词 | suas, simulation, scenario, analysis, testing, flight, analytics, framework, sut, generated | 0.78 | “LLM-Agents Driven Automated Simulation Testing and Analysis of small Uncrewed Aerial Systems Venkata Sai Aswath Duvvuru and Bohan Zhang Department of Computer Science Saint Louis University Saint Louis, USA {venkatasaiaswath.duvvuru, bohan.zhang.1}@slu.edu Michael Vierhauser Department of Computer Science University of” (LLM-agents driven automated simulation testing and analysis of small uncrewed aerial systems.txt @段1) || 摘要 | LLM-Agents Driven Automated Simulation Testing and Analysis of small Uncrewed Aerial Systems Venkata Sai Aswath Duvvuru and Bohan Zhang Department of Computer Science Saint Louis University Saint Louis, USA {venkatasaiaswath.duvvuru, bohan.zhang.1}@slu.edu Michael Vierhauser Department of Computer Science University of Innsbruck Innsbruck, Austria michael.vierhauser@uibk.ac.at Ankit Agrawal Department of Computer Science Saint Louis University Saint Louis, USA ankit.agrawal.1@slu.edu Abstract—Thorough simulation t… | 0.72 | “LLM-Agents Driven Automated Simulation Testing and Analysis of small Uncrewed Aerial Systems Venkata Sai Aswath Duvvuru and Bohan Zhang Department of Computer Science Saint Louis University Saint Louis, USA {venkatasaiaswath.duvvuru, bohan.zhang.1}@slu.edu Michael Vierhauser Department of Computer Science University of” (LLM-agents driven automated simulation testing and analysis of small uncrewed aerial systems.txt @段1) || 研究方法 | LLM-Agents Driven Automated Simulation Testing and Analysis of small Uncrewed Aerial Systems Venkata Sai Aswath Duvvuru and Bohan Zhang Department of Computer Science Saint Louis University Saint Louis, USA {venkatasaiaswath.duvvuru, bohan.zhang.1}@slu.edu Michael Vierhauser Department of Computer Science University of Innsbruck Innsbruck, Austria michael.vierhauser@uibk.ac.at Ankit Agrawal Department of Computer Science Saint Louis University Saint Louis, USA ankit.agrawal.1@slu.edu Abstract—Thorough simulation t… | 0.82 | “Therefore, in this paper, we present a novel framework called 1https://www.reddit.com/r/dji/comments/1amf9rg/oops solid crash while tracking myself on the/ arXiv:2501.11864v1 [cs.SE] 21 Jan 2025 E n v - A g e n t M - A g e n t S - A g e n t Analytics-Agent SuT sUASSimulator SuT SetupConfig Test Properties Rule-BasedVal” (LLM-agents driven automated simulation testing and analysis of small uncrewed aerial systems.txt @段1) || 主要结论 | LLM-Agents Driven Automated Simulation Testing and Analysis of small Uncrewed Aerial Systems Venkata Sai Aswath Duvvuru and Bohan Zhang Department of Computer Science Saint Louis University Saint Louis, USA {venkatasaiaswath.duvvuru, bohan.zhang.1}@slu.edu Michael Vierhauser Department of Computer Science University of Innsbruck Innsbruck, Austria michael.vierhauser@uibk.ac.at Ankit Agrawal Department of Computer Science Saint Louis University Saint Louis, USA ankit.agrawal.1@slu.edu Abstract—Thorough simulation t… | 0.80 | “Our findings indicate that A UTO SIMTEST significantly improves the efficiency and scope of the sUAS testing process, allowing for more comprehensive and varied scenario evaluations while re- ducing the manual effort.” (LLM-agents driven automated simulation testing and analysis of small uncrewed aerial systems.txt @段1) |

- 主题标签：N/A
- 方法标签：仿真/Simulator
- 源文件：`LLM-agents driven automated simulation testing and analysis of small uncrewed aerial systems.txt`

</details>
<details><summary>105. Mediating between Human Programmers and Integrated（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Mediating between Human Programmers and Integrated | 0.92 | “Mediating between Human Programmers and Integrated” (Mediating between human programmers and integrated development environments using LLM-based agents.txt @段1) || 作者 | Mediating between Human Programmers and Integrated | 0.90 | “Mediating between Human Programmers and Integrated” (Mediating between human programmers and integrated development environments using LLM-based agents.txt @段1) || 发表年份 | 2025 | 0.90 | “Ziyou Li. 2025. Mediating between Human Programmers and Integrated” (Mediating between human programmers and integrated development environments using LLM-based agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “https://doi.org/10.48550/arXiv.2304.08103 arXiv:2304.08103 [cs].” (Mediating between human programmers and integrated development environments using LLM-based agents.txt @段1) || 关键词 | development, human, tools, software, ide, 2025, code, multi, programmers, systems | 0.78 | “Mediating between Human Programmers and Integrated Development Environments using LLM-based Agents Ziyou Li ziyou.li@tudelft.nl Delft University of Technology The Netherlands” (Mediating between human programmers and integrated development environments using LLM-based agents.txt @段1) || 摘要 | Mediating between Human Programmers and Integrated Development Environments using LLM-based Agents Ziyou Li ziyou.li@tudelft.nl Delft University of Technology The Netherlands Abstract Large Language Model (LLM)-based autonomous agents provide state-of-the-art performance in various tasks throughout the soft- ware development lifecycle. Though the applications employing the agents can be implemented in Integrated Development Envi- ronments (IDEs), the standalone applications are not optimal for human programmers’ u… | 0.72 | “Mediating between Human Programmers and Integrated Development Environments using LLM-based Agents Ziyou Li ziyou.li@tudelft.nl Delft University of Technology The Netherlands” (Mediating between human programmers and integrated development environments using LLM-based agents.txt @段1) || 研究方法 | 2 Related Work Software development automation has been a long-standing goal since the mid-20th century [8]. Early efforts focused on static code analysis techniques that enabled such as autocomplete and code refactoring [5]. These features have been implemented in IDEs to streamline the development process. Building on this foundation, recent advancements in transformer-based LLMs, lift coding assis- tances to a higher level of autonomy [7]. Adapting LLM, agentic applications have risen as novel solutions that de… | 0.82 | “Consequently, a research gap lies in the development of a unified human-IDE in- teraction framework, that orchestrates the inputs and outputs of conventional IDE tools, specialized agent tools, and external agent systems, thereby minimizing the cognitive burden of programmers.” (Mediating between human programmers and integrated development environments using LLM-based agents.txt @段5) || 主要结论 | N/A | 0.50 | N/A |

- 主题标签：多智能体对抗与协作
- 方法标签：仿真/Simulator, 多智能体/Multi-Agent
- 源文件：`Mediating between human programmers and integrated development environments using LLM-based agents.txt`

</details>
<details><summary>106. Mimicking Human Expertise（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Mimicking Human Expertise | 0.92 | “Mimicking Human Expertise” (Patchagent A practical program repair agent mimicking human expertise.txt @段8) || 作者 | Zheng Yu, Ziyi Guo, Yuhang Wu, and Jiahao Yu, Northwestern University; | 0.90 | “Zheng Yu, Ziyi Guo, Yuhang Wu, and Jiahao Yu, Northwestern University;” (Patchagent A practical program repair agent mimicking human expertise.txt @段1) || 发表年份 | 2025 | 0.86 | “August 13–15, 2025 • Seattle, WA, USA” (Patchagent A practical program repair agent mimicking human expertise.txt @段1) || 期刊/会议 | This paper is included in the Proceedings of the | 0.88 | “This paper is included in the Proceedings of the” (Patchagent A practical program repair agent mimicking human expertise.txt @段1) || 关键词 | patch, program, apr, bug, code, which, repair, techniques, analysis, vulnerabilities | 0.78 | “This paper is included in the Proceedings of the 34th USENIX Security Symposium. August 13–15, 2025 • Seattle, WA, USA 978-1-9391 33-52-6 Open access to the Proceedings of the 34th USENIX Security Symposium is sponsored by USENIX. Patchagent: A Practical Program Repair Agent Mimicking Human Expertise Zheng Yu, Ziyi Guo” (Patchagent A practical program repair agent mimicking human expertise.txt @段1) || 摘要 | This paper is included in the Proceedings of the 34th USENIX Security Symposium. August 13–15, 2025 • Seattle, WA, USA 978-1-9391 33-52-6 Open access to the Proceedings of the 34th USENIX Security Symposium is sponsored by USENIX. Patchagent: A Practical Program Repair Agent Mimicking Human Expertise Zheng Yu, Ziyi Guo, Yuhang Wu, and Jiahao Yu, Northwestern University; Meng Xu, University of Waterloo; Dongliang Mu, Independent Researcher; Yan Chen and Xinyu Xing, Northwestern University https://www.usenix.org/con… | 0.72 | “This paper is included in the Proceedings of the 34th USENIX Security Symposium. August 13–15, 2025 • Seattle, WA, USA 978-1-9391 33-52-6 Open access to the Proceedings of the 34th USENIX Security Symposium is sponsored by USENIX. Patchagent: A Practical Program Repair Agent Mimicking Human Expertise Zheng Yu, Ziyi Guo” (Patchagent A practical program repair agent mimicking human expertise.txt @段1) || 研究方法 | 1 Introduction As programs grow in size and complexity, they also become increasingly vulnerable, as evident in reports [4] and CVE records [57]. For example, fuzz testing (a.k.a. fuzzing) alone has enabled the discovery of thousands of vulnerabilities in large and complex software [85, 91], not to mention other program analysis tools [17, 92] that continuously run on the software deployment pipelines. However, merely discovering vulnerabilities is not sufficient to eliminate the threat; these vulnerabilities must… | 0.82 | “Simultaneously, we understand that even with the intro- duction of these four distinct optimization components in our PATCHAGENT framework, it does not imply that our sys- tem has achieved human-comparable capabilities in holis- tic program repair.” (Patchagent A practical program repair agent mimicking human expertise.txt @段3) || 主要结论 | 7.2 Effectiveness of P ATCHAGENT Table 1 provides a comprehensive comparison of PATCHA- GENT ’s effectiveness in repairing vulnerabilities across differ- ent large language models. The vulnerabilities are classified into four main error types based on their nature and the suc- cess rate of repair: Temporal Error, Spatial Error, Null Dereference, and Numeric Error. These categories encom- pass nine specific bug types: use-after-free, double free, and invalid free (under Temporal Error); stack overflow, global overf… | 0.80 | “The results clearly indicate that PATCHAGENT outperforms the other two methods in almost all the listed vulnerabilities.” (Patchagent A practical program repair agent mimicking human expertise.txt @段64) |

- 主题标签：防御与对齐机制
- 方法标签：多模态/Multimodal, 系统防御/Defense
- 源文件：`Patchagent A practical program repair agent mimicking human expertise.txt`

</details>
<details><summary>107. Mind the Web: The Security of Web Use Agents（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Mind the Web: The Security of Web Use Agents | 0.92 | “Mind the Web: The Security of Web Use Agents” (Mind the web The security of web use agents.txt @段1) || 作者 | Avishag Shapira*, Parth Atulbhai Gandhi*, Edan Habler, and Asaf Shabtai | 0.90 | “Avishag Shapira*, Parth Atulbhai Gandhi*, Edan Habler, and Asaf Shabtai” (Mind the web The security of web use agents.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2506.07153v2  [cs.CR]  20 Oct 2025” (Mind the web The security of web use agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2506.07153v2  [cs.CR]  20 Oct 2025” (Mind the web The security of web use agents.txt @段1) || 关键词 | web, task, content, browser, capabilities, during, mechanisms, payloads, pipeline, ability | 0.78 | “Mind the Web: The Security of Web Use Agents Avishag Shapira*, Parth Atulbhai Gandhi*, Edan Habler, and Asaf Shabtai Ben-Gurion University of the Negev, Israel {shavish,gandhip,habler}@post.bgu.ac.il,shabtaia@bgu.ac.il” (Mind the web The security of web use agents.txt @段1) || 摘要 | Mind the Web: The Security of Web Use Agents Avishag Shapira*, Parth Atulbhai Gandhi*, Edan Habler, and Asaf Shabtai Ben-Gurion University of the Negev, Israel {shavish,gandhip,habler}@post.bgu.ac.il,shabtaia@bgu.ac.il Abstract Web-use agents are rapidly being deployed to automate complex web tasks with extensive browser capabilities including multi-tab navigation, DOM manipulation, and authenticated session access. However, these capabilities create a critical and previously unex- plored attack surface. This pape… | 0.72 | “Mind the Web: The Security of Web Use Agents Avishag Shapira*, Parth Atulbhai Gandhi*, Edan Habler, and Asaf Shabtai Ben-Gurion University of the Negev, Israel {shavish,gandhip,habler}@post.bgu.ac.il,shabtaia@bgu.ac.il” (Mind the web The security of web use agents.txt @段1) || 研究方法 | Abstract Web-use agents are rapidly being deployed to automate complex web tasks with extensive browser capabilities including multi-tab navigation, DOM manipulation, and authenticated session access. However, these capabilities create a critical and previously unex- plored attack surface. This paper demonstrates how attackers can exploit web-use agents by embedding malicious content in web pages, such as comments, reviews, or advertisements, that agents encounter during legitimate browsing tasks. We introduce the… | 0.82 | “To scale this attack, we developed an automated three-stage pipeline that generates effec- tive injections without manual annotation or costly online agent interactions during training, remaining efficient even with limited training data.” (Mind the web The security of web use agents.txt @段2) || 主要结论 | 1 Introduction Web-use agents represent a rapidly growing category of AI agents that automate complex browser tasks through natural language instructions. These agents can autonomously navigate websites, complete forms, make purchases, and perform multi-step workflows across arbitrary web interfaces on behalf of users [12, 16]. Unlike traditional automation frameworks that rely on predetermined scripts [5], web-use agents leverage large language models (LLMs) to dynamically interpret user goals and adapt to divers… | 0.80 | “We in- troduce thetask-alignedinjection technique that frames malicious commands as helpful task guidance rather than obvious attacks, exploiting fundamental limitations in LLMs’ contextual reasoning i.e., agents struggle to maintain coherent contextual awareness and fail to detect when seemingly helpful web content co” (Mind the web The security of web use agents.txt @段4) |

- 主题标签：GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击, Prompt Injection/环境注入, Web/Browser Agent 安全
- 方法标签：多模态/Multimodal, 提示注入/Prompt Injection
- 源文件：`综述参考文献\Mind the web The security of web use agents.txt`

</details>
<details><summary>108. Optimal Multi-Agent Navigation in Constrained Environments（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Optimal Multi-Agent Navigation in Constrained Environments | 0.92 | “Optimal Multi-Agent Navigation in Constrained Environments” (GameChat Multi-LLM dialogue for safe, agile, and socially optimal multi-agent navigation in constra.txt @段2) || 作者 | GAME CHAT: Multi-LLM Dialogue for Safe, Agile, and Socially | 0.90 | “GAME CHAT: Multi-LLM Dialogue for Safe, Agile, and Socially” (GameChat Multi-LLM dialogue for safe, agile, and socially optimal multi-agent navigation in constra.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2503.12333v1  [cs.RO]  16 Mar 2025” (GameChat Multi-LLM dialogue for safe, agile, and socially optimal multi-agent navigation in constra.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2503.12333v1  [cs.RO]  16 Mar 2025” (GameChat Multi-LLM dialogue for safe, agile, and socially optimal multi-agent navigation in constra.txt @段1) || 关键词 | game, smg, chat, time, ieee, multi, which, control, methods, robot | 0.78 | “GAME CHAT: Multi-LLM Dialogue for Safe, Agile, and Socially Optimal Multi-Agent Navigation in Constrained Environments Vagul Mahadevan, Shangtong Zhang, and Rohan Chandra {dub5nq, shangtong, aar8xx }@virginia.edu University of Virginia Code, Videos at gamechat-uva.github.io/ Abstract— Safe, agile, and socially complian” (GameChat Multi-LLM dialogue for safe, agile, and socially optimal multi-agent navigation in constra.txt @段1) || 摘要 | GAME CHAT: Multi-LLM Dialogue for Safe, Agile, and Socially Optimal Multi-Agent Navigation in Constrained Environments Vagul Mahadevan, Shangtong Zhang, and Rohan Chandra {dub5nq, shangtong, aar8xx }@virginia.edu University of Virginia Code, Videos at gamechat-uva.github.io/ Abstract— Safe, agile, and socially compliant multi-robot navigation in cluttered and constrained environments remains a critical challenge. This is especially difficult with self-interested agents in decentralized settings, where there is no … | 0.72 | “GAME CHAT: Multi-LLM Dialogue for Safe, Agile, and Socially Optimal Multi-Agent Navigation in Constrained Environments Vagul Mahadevan, Shangtong Zhang, and Rohan Chandra {dub5nq, shangtong, aar8xx }@virginia.edu University of Virginia Code, Videos at gamechat-uva.github.io/ Abstract— Safe, agile, and socially complian” (GameChat Multi-LLM dialogue for safe, agile, and socially optimal multi-agent navigation in constra.txt @段1) || 研究方法 | GAME CHAT: Multi-LLM Dialogue for Safe, Agile, and Socially Optimal Multi-Agent Navigation in Constrained Environments Vagul Mahadevan, Shangtong Zhang, and Rohan Chandra {dub5nq, shangtong, aar8xx }@virginia.edu University of Virginia Code, Videos at gamechat-uva.github.io/ Abstract— Safe, agile, and socially compliant multi-robot navigation in cluttered and constrained environments remains a critical challenge. This is especially difficult with self-interested agents in decentralized settings, where there is no … | 0.82 | “Main Contributions We propose a new algorithm for safe, agile, deadlock-free, and decentralized multi-robot navigation in symmetric, con- strained environments (like passing through doorways and narrow hallways).” (GameChat Multi-LLM dialogue for safe, agile, and socially optimal multi-agent navigation in constra.txt @段1) || 主要结论 | GAME CHAT: Multi-LLM Dialogue for Safe, Agile, and Socially Optimal Multi-Agent Navigation in Constrained Environments Vagul Mahadevan, Shangtong Zhang, and Rohan Chandra {dub5nq, shangtong, aar8xx }@virginia.edu University of Virginia Code, Videos at gamechat-uva.github.io/ Abstract— Safe, agile, and socially compliant multi-robot navigation in cluttered and constrained environments remains a critical challenge. This is especially difficult with self-interested agents in decentralized settings, where there is no … | 0.80 | “The results show that even in the worst case, GAME CHAT reduces the time for all agents to reach their goals by over 35% from a naive baseline and by over 20% from SMG-CBF in the intersection scenario, while doubling the rate of ensuring the agent with a higher priority task reaches the goal first, from 50% (equivalent” (GameChat Multi-LLM dialogue for safe, agile, and socially optimal multi-agent navigation in constra.txt @段1) |

- 主题标签：多智能体对抗与协作
- 方法标签：仿真/Simulator, 多智能体/Multi-Agent
- 源文件：`综述参考文献\GameChat Multi-LLM dialogue for safe, agile, and socially optimal multi-agent navigation in constra.txt`

</details>
<details><summary>109. Poster: Agentic Shell Honeypot Using Structured Logging（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Poster: Agentic Shell Honeypot Using Structured Logging | 0.92 | “Poster: Agentic Shell Honeypot Using Structured Logging” (Poster Agentic shell honeypot using structured logging.txt @段7) || 作者 | KAI WEI, University of South Florida, Tampa, Tampa, FL, United States | 0.90 | “KAI WEI, University of South Florida, Tampa, Tampa, FL, United States” (Poster Agentic shell honeypot using structured logging.txt @段1) || 发表年份 | 2025 | 0.90 | “Published: 19 November 2025” (Poster Agentic shell honeypot using structured logging.txt @段1) || 期刊/会议 | CCS '25: ACM SIGSAC Conference on | 0.88 | “CCS '25: ACM SIGSAC Conference on” (Poster Agentic shell honeypot using structured logging.txt @段1) || 关键词 | shell, honeypots, system, 2025, command, logs, response, acm, design, florida | 0.78 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3719027.3760731 . . POSTER Poster: Agentic Shell Honeypot Using Structured Logging KAI WEI, University of South Florida, Tampa, Tampa, FL, United States . GUANGJING WANG, University of South Florida, Tampa, Tampa, FL, United States . . . Open Access Support provided by:” (Poster Agentic shell honeypot using structured logging.txt @段1) || 摘要 | . . Latest updates: hps://dl.acm.org/doi/10.1145/3719027.3760731 . . POSTER Poster: Agentic Shell Honeypot Using Structured Logging KAI WEI, University of South Florida, Tampa, Tampa, FL, United States . GUANGJING WANG, University of South Florida, Tampa, Tampa, FL, United States . . . Open Access Support provided by: . University of South Florida, Tampa . PDF Download 3719027.3760731.pdf 20 January 2026 Total Citations: 0 Total Downloads: 1208 . . Published: 19 November 2025 . . Citation in BibTeX format . . CCS… | 0.72 | “. . Latest updates: hps://dl.acm.org/doi/10.1145/3719027.3760731 . . POSTER Poster: Agentic Shell Honeypot Using Structured Logging KAI WEI, University of South Florida, Tampa, Tampa, FL, United States . GUANGJING WANG, University of South Florida, Tampa, Tampa, FL, United States . . . Open Access Support provided by:” (Poster Agentic shell honeypot using structured logging.txt @段1) || 研究方法 | 2 System Design In this section, we introduce the design of HoneyAgents, which in- cludes three core components: the terminal, the logging mechanism, and the multi-agent framework. The coordination between different components makes HoneyAgents a robust, realistic, and efficient shell honeypot system. As shown in Figure 1, Step① occurs when an attacker establishes a connection and enters the terminal to launch attacks. During this process, auth_log records all login information for subsequent analysis. Then, the V… | 0.82 | “2 System Design In this section, we introduce the design of HoneyAgents, which in- cludes three core components: the terminal, the logging mechanism, and the multi-agent framework.” (Poster Agentic shell honeypot using structured logging.txt @段7) || 主要结论 | N/A | 0.50 | N/A |

- 主题标签：Prompt Injection/环境注入, 多智能体对抗与协作
- 方法标签：仿真/Simulator, 多智能体/Multi-Agent, 提示注入/Prompt Injection
- 源文件：`Poster Agentic shell honeypot using structured logging.txt`

</details>
<details><summary>110. Proactive Defenses Against LLM Agents（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Proactive Defenses Against LLM Agents | 0.92 | “Proactive Defenses Against LLM Agents” (Cloak, honey, trap Proactive defenses against LLM agents.txt @段8) || 作者 | Cloak, Honey, Trap: | 0.90 | “Cloak, Honey, Trap:” (Cloak, honey, trap Proactive defenses against LLM agents.txt @段1) || 发表年份 | 2025 | 0.86 | “August 13–15, 2025 • Seattle, WA, USA” (Cloak, honey, trap Proactive defenses against LLM agents.txt @段1) || 期刊/会议 | This paper is included in the Proceedings of the | 0.88 | “This paper is included in the Proceedings of the” (Cloak, honey, trap Proactive defenses against LLM agents.txt @段1) || 关键词 | tokens, against, honey, prompt, usenix, defenders, detect, framework, trap, assets | 0.78 | “This paper is included in the Proceedings of the 34th USENIX Security Symposium. August 13–15, 2025 • Seattle, WA, USA 978-1-9391 33-52-6 Open access to the Proceedings of the 34th USENIX Security Symposium is sponsored by USENIX. Cloak, Honey, Trap: Proactive Defenses Against LLM Agents Daniel Ayzenshteyn, Roy Weiss,” (Cloak, honey, trap Proactive defenses against LLM agents.txt @段1) || 摘要 | This paper is included in the Proceedings of the 34th USENIX Security Symposium. August 13–15, 2025 • Seattle, WA, USA 978-1-9391 33-52-6 Open access to the Proceedings of the 34th USENIX Security Symposium is sponsored by USENIX. Cloak, Honey, Trap: Proactive Defenses Against LLM Agents Daniel Ayzenshteyn, Roy Weiss, and Yisroel Mirsky, Ben Gurion University of the Negev https://www.usenix.org/conference/usenixsecurity25/presentation/ayzenshteyn Cloak, Honey, Trap: Proactive Defenses Against LLM Agents Daniel Ayz… | 0.72 | “This paper is included in the Proceedings of the 34th USENIX Security Symposium. August 13–15, 2025 • Seattle, WA, USA 978-1-9391 33-52-6 Open access to the Proceedings of the 34th USENIX Security Symposium is sponsored by USENIX. Cloak, Honey, Trap: Proactive Defenses Against LLM Agents Daniel Ayzenshteyn, Roy Weiss,” (Cloak, honey, trap Proactive defenses against LLM agents.txt @段1) || 研究方法 | 1 Introduction Advancements in AI have rapidly transformed numerous sec- tors, with LLMs leading the way in automating complex pro- cesses and enabling sophisticated decision-making. These models excel in natural language understanding, content gen- eration, and problem-solving, achieving unprecedented re- sults across diverse applications [36]. As LLMs evolve, their influence has extended to critical fields like cybersecurity. Harnessing their reasoning and automation capabilities, re- searchers and practitioners… | 0.82 | “• Defense Framework: We propose a multifaceted defense framework centered around three core components to hide, stop, and detect attacks: – Cloak: Employs misdirection tactics and token manipula- tions to obscure or trivialize critical content, encouraging LLM agents to overlook or disregard it.” (Cloak, honey, trap Proactive defenses against LLM agents.txt @段3) || 主要结论 | 0 5000 10000 15000 20000 0% 25% 50% 75% 100% PTT Size (tokens) Mean Accuracy (%) claude3.5-sonnet gemini1.5-pro gpt-4o llama3.1-70b Figure 5: PentestGPT’s task selection accuracy as irrelevant tasks are added to its task list (PTT) using T4.1 and T4.2. and Gemini 1.5 Pro (128k, 128k, 200k, 2000k, token context, respectively). PentestGPT maintains its internal state K as a textual task list called the PTT, which enumerates possible actions and their statuses. We started with a PTT of one avail- able task (700 token… | 0.80 | “Our results in Section 6.1 indicate the effect is worse when distractors contain enticing misinformation, such as fake credentials.” (Cloak, honey, trap Proactive defenses against LLM agents.txt @段31) |

- 主题标签：Jailbreak/越狱攻击, Web/Browser Agent 安全, 防御与对齐机制, 隐私与记忆风险
- 方法标签：系统防御/Defense, 调查/Survey
- 源文件：`Cloak, honey, trap Proactive defenses against LLM agents.txt`

</details>
<details><summary>111. ProphetAgent: Automatically Synthesizing GUI Tests from Test（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | ProphetAgent: Automatically Synthesizing GUI Tests from Test | 0.92 | “ProphetAgent: Automatically Synthesizing GUI Tests from Test” (ProphetAgent Automatically synthesizing GUI tests from test cases in natural language for mobile ap.txt @段1) || 作者 | Shanghai, China | 0.90 | “Shanghai, China” (ProphetAgent Automatically synthesizing GUI tests from test cases in natural language for mobile ap.txt @段1) || 发表年份 | 2025 | 0.90 | “FSE Companion ’25, June 23–28, 2025, Trondheim, Norway” (ProphetAgent Automatically synthesizing GUI tests from test cases in natural language for mobile ap.txt @段1) || 期刊/会议 | Apps. In 33rd ACM International Conference on the Foundations of Software | 0.88 | “Apps. In 33rd ACM International Conference on the Foundations of Software” (ProphetAgent Automatically synthesizing GUI tests from test cases in natural language for mobile ap.txt @段1) || 关键词 | test, gui, cases, prophetagent, natural, bytedance, china, app, graph, com | 0.78 | “ProphetAgent: Automatically Synthesizing GUI Tests from Test Cases in Natural Language for Mobile Apps Qichao Kong∗ East China Normal University / ByteDance Shanghai, China kongqichao@bytedance.com Zhengwei Lv ByteDance Beijing, China lvzhengwei.m@bytedance.com Yiheng Xiong∗ East China Normal University Shanghai, China” (ProphetAgent Automatically synthesizing GUI tests from test cases in natural language for mobile ap.txt @段1) || 摘要 | ProphetAgent: Automatically Synthesizing GUI Tests from Test Cases in Natural Language for Mobile Apps Qichao Kong∗ East China Normal University / ByteDance Shanghai, China kongqichao@bytedance.com Zhengwei Lv ByteDance Beijing, China lvzhengwei.m@bytedance.com Yiheng Xiong∗ East China Normal University Shanghai, China xyh@stu.ecnu.edu.cn Jingling Sun† University of Electronic Science and Technology of China Chengdu, China jingling.sun910@gmail.com Ting Su∗† East China Normal University Shanghai, China tsu@sei.ecn… | 0.72 | “ProphetAgent: Automatically Synthesizing GUI Tests from Test Cases in Natural Language for Mobile Apps Qichao Kong∗ East China Normal University / ByteDance Shanghai, China kongqichao@bytedance.com Zhengwei Lv ByteDance Beijing, China lvzhengwei.m@bytedance.com Yiheng Xiong∗ East China Normal University Shanghai, China” (ProphetAgent Automatically synthesizing GUI tests from test cases in natural language for mobile ap.txt @段1) || 研究方法 | FSE Companion ’25, June 23–28, 2025, Trondheim, Norway Qichao Kong, Zhengwei Lv, Yiheng Xiong, Jingling Sun, Ting Su, Dingchun Wang, Letao Li, Xu Yang, and Gang Huo test step to the appropriate UI widgets, writing GUI tests based on the identified UI widgets, and modifying the GUI tests if they fail during execution. A report from a testing team in ByteDance shows that each novice tester can only write (synthesizing GUI tests from natural language) and maintain (updating them with each app version change) 150 test… | 0.82 | “Our approach offers two advantages over existing techniques: (1) By pre-building a CUTG with semantically annotated nodes, our approach provides more precise semantic information to assist LLMs in matching nat- ural language descriptions with UI widgets, (2) It does not require using LLM to process all the app informat” (ProphetAgent Automatically synthesizing GUI tests from test cases in natural language for mobile ap.txt @段5) || 主要结论 | FSE Companion ’25, June 23–28, 2025, Trondheim, Norway Qichao Kong, Zhengwei Lv, Yiheng Xiong, Jingling Sun, Ting Su, Dingchun Wang, Letao Li, Xu Yang, and Gang Huo test step to the appropriate UI widgets, writing GUI tests based on the identified UI widgets, and modifying the GUI tests if they fail during execution. A report from a testing team in ByteDance shows that each novice tester can only write (synthesizing GUI tests from natural language) and maintain (updating them with each app version change) 150 test… | 0.80 | “The results show that ProphetAgent achieved a 78.1% completion rate, significantly outperforming existing tools (21.4% for AppAgent [31] and 32.5% for AutoDroid [28]).” (ProphetAgent Automatically synthesizing GUI tests from test cases in natural language for mobile ap.txt @段5) |

- 主题标签：GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击
- 方法标签：多模态/Multimodal
- 源文件：`ProphetAgent Automatically synthesizing GUI tests from test cases in natural language for mobile ap.txt`

</details>
<details><summary>112. RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage | 0.92 | “RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage” (RTBAS Defending LLM agents against prompt injection and privacy leakage.txt @段1) || 作者 | RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage | 0.90 | “RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage” (RTBAS Defending LLM agents against prompt injection and privacy leakage.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2502.08966v2  [cs.CR]  14 Feb 2025” (RTBAS Defending LLM agents against prompt injection and privacy leakage.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2502.08966v2  [cs.CR]  14 Feb 2025” (RTBAS Defending LLM agents against prompt injection and privacy leakage.txt @段1) || 关键词 | tool, prompt, tbas, data, user, injection, lms, calls, integrity, regions | 0.78 | “RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage Peter Yong Zhong*1, Siyuan Chen*1, Ruiqi Wang1, McKenna McCall1, Ben L. Titzer1, Heather Miller1, 2, and Phillip B. Gibbons1 1Carnegie Mellon University 2Two Sigma Investments” (RTBAS Defending LLM agents against prompt injection and privacy leakage.txt @段1) || 摘要 | RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage Peter Yong Zhong*1, Siyuan Chen*1, Ruiqi Wang1, McKenna McCall1, Ben L. Titzer1, Heather Miller1, 2, and Phillip B. Gibbons1 1Carnegie Mellon University 2Two Sigma Investments Abstract Tool-Based Agent Systems (TBAS) allow Language Mod- els (LMs) to use external tools for tasks beyond their stan- dalone capabilities, such as searching websites, booking flights, or making financial transactions. However, these tools greatly increase the risks … | 0.72 | “RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage Peter Yong Zhong*1, Siyuan Chen*1, Ruiqi Wang1, McKenna McCall1, Ben L. Titzer1, Heather Miller1, 2, and Phillip B. Gibbons1 1Carnegie Mellon University 2Two Sigma Investments” (RTBAS Defending LLM agents against prompt injection and privacy leakage.txt @段1) || 研究方法 | 1 Introduction Language Models (LMs) excel at complex tasks, using rea- soning and planning when prompted with natural language instructions. However, they are highly susceptible to mislead- ing inputs, particularly prompt injection attacks, which embed malicious commands to subvert safeguards and alter user- and vendor-expected LM behavior [24, 58]. Meanwhile, recent advancements have led to the develop- ment of Agents–advanced applications of LMs where LMs can interact with external environments by making API ca… | 0.82 | “To address these challenges, we introduce Robust TBAS (RTBAS), an information flow-based framework that se- lectively propagates security metadata using dependency screening.” (RTBAS Defending LLM agents against prompt injection and privacy leakage.txt @段3) || 主要结论 | 1 Introduction Language Models (LMs) excel at complex tasks, using rea- soning and planning when prompted with natural language instructions. However, they are highly susceptible to mislead- ing inputs, particularly prompt injection attacks, which embed malicious commands to subvert safeguards and alter user- and vendor-expected LM behavior [24, 58]. Meanwhile, recent advancements have led to the develop- ment of Agents–advanced applications of LMs where LMs can interact with external environments by making API ca… | 0.80 | “In evaluation, RTBAS outperforms SOTA de- fenses by (i) detecting and executing without user confirma- tion the same set of tool calls as the oracle for all but one task, while matching the oracle’s confidentiality protection, and (ii) maximizing overall task utility relative to SOTA defenses, even those requiring 100%” (RTBAS Defending LLM agents against prompt injection and privacy leakage.txt @段3) |

- 主题标签：GUI/Computer-Use Agent 安全, Prompt Injection/环境注入, Web/Browser Agent 安全, 防御与对齐机制, 隐私与记忆风险
- 方法标签：仿真/Simulator, 提示注入/Prompt Injection, 系统防御/Defense
- 源文件：`综述参考文献\RTBAS Defending LLM agents against prompt injection and privacy leakage.txt`

</details>
<details><summary>113. Real AI Agents with Fake Memories: Fatal Context Manipulation Attacks on Web3（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Real AI Agents with Fake Memories: Fatal Context Manipulation Attacks on Web3 | 0.92 | “Real AI Agents with Fake Memories: Fatal Context Manipulation Attacks on Web3” (Real AI agents with fake memories Fatal context manipulation attacks on Web3 agents.txt @段1) || 作者 | financial protocols and immutable smart contracts. This paper | 0.90 | “financial protocols and immutable smart contracts. This paper” (Real AI agents with fake memories Fatal context manipulation attacks on Web3 agents.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2503.16248v3  [cs.CR]  9 Jul 2025” (Real AI agents with fake memories Fatal context manipulation attacks on Web3 agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2503.16248v3  [cs.CR]  9 Jul 2025” (Real AI agents with fake memories Fatal context manipulation attacks on Web3 agents.txt @段1) || 关键词 | memory, injection, prompt, context, user, arxiv, elizaos, malicious, data, system | 0.78 | “Real AI Agents with Fake Memories: Fatal Context Manipulation Attacks on Web3 Agents Atharv Singh Patlan Princeton University atharvsp@princeton.edu Peiyao Sheng Sentient Foundation peiyao@sentient.xyz S. Ashwin Hebbar Princeton University hebbar@princeton.edu Prateek Mittal Princeton University pmittal@princeton.edu P” (Real AI agents with fake memories Fatal context manipulation attacks on Web3 agents.txt @段1) || 摘要 | Real AI Agents with Fake Memories: Fatal Context Manipulation Attacks on Web3 Agents Atharv Singh Patlan Princeton University atharvsp@princeton.edu Peiyao Sheng Sentient Foundation peiyao@sentient.xyz S. Ashwin Hebbar Princeton University hebbar@princeton.edu Prateek Mittal Princeton University pmittal@princeton.edu Pramod Viswanath Princeton University & Sentient Foundation pramodv@princeton.edu Abstract—AI agents integrated with Web3 offer autonomy and openness but raise security concerns as they interact with … | 0.72 | “Real AI Agents with Fake Memories: Fatal Context Manipulation Attacks on Web3 Agents Atharv Singh Patlan Princeton University atharvsp@princeton.edu Peiyao Sheng Sentient Foundation peiyao@sentient.xyz S. Ashwin Hebbar Princeton University hebbar@princeton.edu Prateek Mittal Princeton University pmittal@princeton.edu P” (Real AI agents with fake memories Fatal context manipulation attacks on Web3 agents.txt @段1) || 研究方法 | pt ⊕ δp, d t, k, h t  . An attacker embeds malicious instructions directly into the user prompt pt. • Indirect Prompt Injections: c∗ = ( pt, d t ⊕ δd, k, h t) Attackers poison external data sources such as API responses or blockchain-derived data with malicious instructions. Comparing indirect memory injections with prompt in- jection and backdoor attacks. It is important to distinguish indirect memory injection from standard prompt injection and backdoor attacks, as they differ significantly in both structure an… | 0.82 | “Case Study: Evaluating ElizaOS on Con- text Manipulation Attacks We present a case study of ElizaOS [34], an open- source, modular framework designed to facilitate the creation, deployment, and management of AI agents within the Web3 ecosystem.” (Real AI agents with fake memories Fatal context manipulation attacks on Web3 agents.txt @段2) || 主要结论 | Real AI Agents with Fake Memories: Fatal Context Manipulation Attacks on Web3 Agents Atharv Singh Patlan Princeton University atharvsp@princeton.edu Peiyao Sheng Sentient Foundation peiyao@sentient.xyz S. Ashwin Hebbar Princeton University hebbar@princeton.edu Prateek Mittal Princeton University pmittal@princeton.edu Pramod Viswanath Princeton University & Sentient Foundation pramodv@princeton.edu Abstract—AI agents integrated with Web3 offer autonomy and openness but raise security concerns as they interact with … | 0.80 | “We showcase practical attacks on popular agentic libraries such as ElizaOS on the Ethereum blockchain, revealing that AI-driven Web3 agents face significant and under- explored security threats which are readily exploited in a financial manner, leading to potentially devastating losses, Furthermore, we demonstrate that” (Real AI agents with fake memories Fatal context manipulation attacks on Web3 agents.txt @段1) |

- 主题标签：Backdoor/投毒, GUI/Computer-Use Agent 安全, Prompt Injection/环境注入, Web/Browser Agent 安全, 防御与对齐机制, 隐私与记忆风险
- 方法标签：后门/Backdoor, 多模态/Multimodal, 提示注入/Prompt Injection, 系统防御/Defense
- 源文件：`综述参考文献\Real AI agents with fake memories Fatal context manipulation attacks on Web3 agents.txt`

</details>
<details><summary>114. Safety of Computer Use Agents（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Safety of Computer Use Agents | 0.92 | “Safety of Computer Use Agents” (OS-harm A benchmark for measuring safety of computer use agents.txt @段2) || 作者 | Thomas Kuntz1,∗, Agatha Duzan1,∗, Hao Zhao1, Francesco Croce1, | 0.90 | “Thomas Kuntz1,∗, Agatha Duzan1,∗, Hao Zhao1, Francesco Croce1,” (OS-harm A benchmark for measuring safety of computer use agents.txt @段1) || 发表年份 | 2025 | 0.86 | “(Andriushchenko et al., 2025b; Kumar et al., 2025). While LLMs can expose harmful information” (OS-harm A benchmark for measuring safety of computer use agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2506.14866v2  [cs.SE]  29 Oct 2025” (OS-harm A benchmark for measuring safety of computer use agents.txt @段1) || 关键词 | tasks, 2024, computer, 2025, benchmark, prompt, harm, injection, osworld, user | 0.78 | “OS-HARM: A Benchmark for Measuring Safety of Computer Use Agents Thomas Kuntz1,∗, Agatha Duzan1,∗, Hao Zhao1, Francesco Croce1, Zico Kolter2,Nicolas Flammarion 1,Maksym Andriushchenko 1 1EPFL, 2Carnegie Mellon University, ∗Equal contribution” (OS-harm A benchmark for measuring safety of computer use agents.txt @段1) || 摘要 | OS-HARM: A Benchmark for Measuring Safety of Computer Use Agents Thomas Kuntz1,∗, Agatha Duzan1,∗, Hao Zhao1, Francesco Croce1, Zico Kolter2,Nicolas Flammarion 1,Maksym Andriushchenko 1 1EPFL, 2Carnegie Mellon University, ∗Equal contribution Abstract Computer use agents are LLM-based agents that can directly interact with a graph- ical user interface, by processing screenshots or accessibility trees. While these systems are gaining popularity, their safety has been largely overlooked, despite the fact that evaluat… | 0.72 | “OS-HARM: A Benchmark for Measuring Safety of Computer Use Agents Thomas Kuntz1,∗, Agatha Duzan1,∗, Hao Zhao1, Francesco Croce1, Zico Kolter2,Nicolas Flammarion 1,Maksym Andriushchenko 1 1EPFL, 2Carnegie Mellon University, ∗Equal contribution” (OS-harm A benchmark for measuring safety of computer use agents.txt @段1) || 研究方法 | 15 30 60 Number of Steps 20% 30% 40% 50%Completion Rate Deliberate misuse Prompt injection Model misbehavior Figure 6:Comparison of different number of maximum steps on o4-mini.Running agents for longer leads to better completion rate only in some cases, but overall does not affect the main findings from Table 2. B.3 Deliberate Misuse Breakdown over Categories and Apps We use the 50 manually annotated execution traces of o4-mini obtained from the deliberate misuse tasks. This manual safety evaluation, shown in Fig… | 0.82 | “B.4 Extended Experiments for the LLM Judge In this section, we present more experiment results about LLM judges, revolving around the judge frameworks, used select which information to give the judge to make its judgment, and around the different LLMs we tested after picking the best judge framework.” (OS-harm A benchmark for measuring safety of computer use agents.txt @段24) || 主要结论 | 15 30 60 Number of Steps 20% 30% 40% 50%Completion Rate Deliberate misuse Prompt injection Model misbehavior Figure 6:Comparison of different number of maximum steps on o4-mini.Running agents for longer leads to better completion rate only in some cases, but overall does not affect the main findings from Table 2. B.3 Deliberate Misuse Breakdown over Categories and Apps We use the 50 manually annotated execution traces of o4-mini obtained from the deliberate misuse tasks. This manual safety evaluation, shown in Fig… | 0.80 | “Copyright CybercrimeDisinformation Fraud Harassment 0 20 40 60 80Percentage 75% 25% 86% 14% 50% 50% 60% 40% 71% 29% Unsafe Completed GIMP LO CalcLO ImpressLO WriterThunderbird VS Code OS Multi-app 0 20 40 60 80 100Percentage 50% 50% 50% 50% 100% 71% 29% 100% 80% 20% 86% 14% 100% Unsafe Completed Figure 7:Breakdown of s” (OS-harm A benchmark for measuring safety of computer use agents.txt @段24) |

- 主题标签：GUI/Computer-Use Agent 安全, Prompt Injection/环境注入, Web/Browser Agent 安全
- 方法标签：仿真/Simulator, 基准/Benchmark, 提示注入/Prompt Injection
- 源文件：`综述参考文献\OS-harm A benchmark for measuring safety of computer use agents.txt`

</details>
<details><summary>115. Security Risks of Mobile LLM Agents（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Security Risks of Mobile LLM Agents | 0.92 | “Security Risks of Mobile LLM Agents” (From assistants to adversaries Exploring the security risks of mobile LLM agents.txt @段2) || 作者 | Liangxuan Wu ∗, Chao Wang ∗, Tianming Liu, Yanjie Zhao and Haoyu Wang B, | 0.90 | “Liangxuan Wu ∗, Chao Wang ∗, Tianming Liu, Yanjie Zhao and Haoyu Wang B,” (From assistants to adversaries Exploring the security risks of mobile LLM agents.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2505.12981v2  [cs.CR]  20 May 2025” (From assistants to adversaries Exploring the security risks of mobile LLM agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2505.12981v2  [cs.CR]  20 May 2025” (From assistants to adversaries Exploring the security risks of mobile LLM agents.txt @段1) || 关键词 | mobile, system, arxiv, user, 2024, interaction, app, forgery, level, malicious | 0.78 | “From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents Liangxuan Wu ∗, Chao Wang ∗, Tianming Liu, Yanjie Zhao and Haoyu Wang B, Huazhong University of Science and Technology Email: {liangxuanw, chaowang , tmliu, yanjie zhao, haoyuwang}@hust.edu.cn Abstract—The growing adoption of large langua” (From assistants to adversaries Exploring the security risks of mobile LLM agents.txt @段1) || 摘要 | From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents Liangxuan Wu ∗, Chao Wang ∗, Tianming Liu, Yanjie Zhao and Haoyu Wang B, Huazhong University of Science and Technology Email: {liangxuanw, chaowang , tmliu, yanjie zhao, haoyuwang}@hust.edu.cn Abstract—The growing adoption of large language models (LLMs) has led to a new paradigm in mobile computing—LLM- powered mobile AI agents—capable of decomposing and au- tomating complex tasks directly on smartphones. However, the security impli… | 0.72 | “From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents Liangxuan Wu ∗, Chao Wang ∗, Tianming Liu, Yanjie Zhao and Haoyu Wang B, Huazhong University of Science and Technology Email: {liangxuanw, chaowang , tmliu, yanjie zhao, haoyuwang}@hust.edu.cn Abstract—The growing adoption of large langua” (From assistants to adversaries Exploring the security risks of mobile LLM agents.txt @段1) || 研究方法 | From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents Liangxuan Wu ∗, Chao Wang ∗, Tianming Liu, Yanjie Zhao and Haoyu Wang B, Huazhong University of Science and Technology Email: {liangxuanw, chaowang , tmliu, yanjie zhao, haoyuwang}@hust.edu.cn Abstract—The growing adoption of large language models (LLMs) has led to a new paradigm in mobile computing—LLM- powered mobile AI agents—capable of decomposing and au- tomating complex tasks directly on smartphones. However, the security impli… | 0.82 | “In this paper, we present the first comprehensive security analysis of mobile LLM agents, encompassing three represen- tative categories: System-level AI Agents developed by original equipment manufacturers (e.g., YOYO Assistant), Third-party Universal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g” (From assistants to adversaries Exploring the security risks of mobile LLM agents.txt @段1) || 主要结论 | N/A | 0.50 | N/A |

- 主题标签：GUI/Computer-Use Agent 安全
- 方法标签：多模态/Multimodal
- 源文件：`综述参考文献\From assistants to adversaries Exploring the security risks of mobile LLM agents.txt`

</details>
<details><summary>116. Systematic Categorization, Construction and Evaluation of New Attacks against（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Systematic Categorization, Construction and Evaluation of New Attacks against | 0.92 | “Systematic Categorization, Construction and Evaluation of New Attacks against” (Systematic categorization, construction and evaluation of new attacks against multi-modal mobile GUI.txt @段1) || 作者 | Systematic Categorization, Construction and Evaluation of New Attacks against | 0.90 | “Systematic Categorization, Construction and Evaluation of New Attacks against” (Systematic categorization, construction and evaluation of new attacks against multi-modal mobile GUI.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2407.09295v3  [cs.CR]  16 Mar 2025” (Systematic categorization, construction and evaluation of new attacks against multi-modal mobile GUI.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2407.09295v3  [cs.CR]  16 Mar 2025” (Systematic categorization, construction and evaluation of new attacks against multi-modal mobile GUI.txt @段1) || 关键词 | mobile, gui, multi, modal, image, text, adversary, memory, data, action | 0.78 | “Systematic Categorization, Construction and Evaluation of New Attacks against Multi-modal Mobile GUI Agents Yulong Yang Xi’an Jiaotong University Chenhao Lin Xi’an Jiaotong University Gelei Deng Nanyang Technological University Xinshan Yang Xi’an Jiaotong University Shuaidong Li Nankai University Ziheng Tang Xi’an Jiao” (Systematic categorization, construction and evaluation of new attacks against multi-modal mobile GUI.txt @段1) || 摘要 | Systematic Categorization, Construction and Evaluation of New Attacks against Multi-modal Mobile GUI Agents Yulong Yang Xi’an Jiaotong University Chenhao Lin Xi’an Jiaotong University Gelei Deng Nanyang Technological University Xinshan Yang Xi’an Jiaotong University Shuaidong Li Nankai University Ziheng Tang Xi’an Jiaotong University Zhengyu Zhao Xi’an Jiaotong University Qian Wang Wuhan University Tianwei Zhang Nanyang Technological University Chao Shen Xi’an Jiaotong UniversityAbstract The integration of Large L… | 0.72 | “Systematic Categorization, Construction and Evaluation of New Attacks against Multi-modal Mobile GUI Agents Yulong Yang Xi’an Jiaotong University Chenhao Lin Xi’an Jiaotong University Gelei Deng Nanyang Technological University Xinshan Yang Xi’an Jiaotong University Shuaidong Li Nankai University Ziheng Tang Xi’an Jiao” (Systematic categorization, construction and evaluation of new attacks against multi-modal mobile GUI.txt @段1) || 研究方法 | Systematic Categorization, Construction and Evaluation of New Attacks against Multi-modal Mobile GUI Agents Yulong Yang Xi’an Jiaotong University Chenhao Lin Xi’an Jiaotong University Gelei Deng Nanyang Technological University Xinshan Yang Xi’an Jiaotong University Shuaidong Li Nankai University Ziheng Tang Xi’an Jiaotong University Zhengyu Zhao Xi’an Jiaotong University Qian Wang Wuhan University Tianwei Zhang Nanyang Technological University Chao Shen Xi’an Jiaotong UniversityAbstract The integration of Large L… | 0.82 | “Our contributions are twofold: (1) we propose a novel threat modeling methodology, leading to the discovery and feasibility analysis of 34 previously un- reported attacks, and (2) we design an attack framework to systematically construct and evaluate these threats.” (Systematic categorization, construction and evaluation of new attacks against multi-modal mobile GUI.txt @段1) || 主要结论 | Systematic Categorization, Construction and Evaluation of New Attacks against Multi-modal Mobile GUI Agents Yulong Yang Xi’an Jiaotong University Chenhao Lin Xi’an Jiaotong University Gelei Deng Nanyang Technological University Xinshan Yang Xi’an Jiaotong University Shuaidong Li Nankai University Ziheng Tang Xi’an Jiaotong University Zhengyu Zhao Xi’an Jiaotong University Qian Wang Wuhan University Tianwei Zhang Nanyang Technological University Chao Shen Xi’an Jiaotong UniversityAbstract The integration of Large L… | 0.80 | “Systematic Categorization, Construction and Evaluation of New Attacks against Multi-modal Mobile GUI Agents Yulong Yang Xi’an Jiaotong University Chenhao Lin Xi’an Jiaotong University Gelei Deng Nanyang Technological University Xinshan Yang Xi’an Jiaotong University Shuaidong Li Nankai University Ziheng Tang Xi’an Jiao” (Systematic categorization, construction and evaluation of new attacks against multi-modal mobile GUI.txt @段1) |

- 主题标签：GUI/Computer-Use Agent 安全, 隐私与记忆风险
- 方法标签：基准/Benchmark, 多模态/Multimodal
- 源文件：`综述参考文献\Systematic categorization, construction and evaluation of new attacks against multi-modal mobile GUI.txt`

</details>
<details><summary>117. Taint-Style Vulnerabilities in LLM-based Agents（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Taint-Style Vulnerabilities in LLM-based Agents | 0.92 | “Taint-Style Vulnerabilities in LLM-based Agents” (Make agent defeat agent Automatic detection of taint-style vulnerabilities in LLM-based agents.txt @段8) || 作者 | Fengyu Liu, Yuan Zhang, Jiaqi Luo, Jiarun Dai, Tian Chen, Letian Yuan, | 0.90 | “Fengyu Liu, Yuan Zhang, Jiaqi Luo, Jiarun Dai, Tian Chen, Letian Yuan,” (Make agent defeat agent Automatic detection of taint-style vulnerabilities in LLM-based agents.txt @段1) || 发表年份 | 2025 | 0.86 | “August 13–15, 2025 • Seattle, WA, USA” (Make agent defeat agent Automatic detection of taint-style vulnerabilities in LLM-based agents.txt @段1) || 期刊/会议 | This paper is included in the Proceedings of the | 0.88 | “This paper is included in the Proceedings of the” (Make agent defeat agent Automatic detection of taint-style vulnerabilities in LLM-based agents.txt @段1) || 关键词 | vulnerabilities, style, taint, agentfuzz, code, seed, fuzzing, prompts, specific, user | 0.78 | “This paper is included in the Proceedings of the 34th USENIX Security Symposium. August 13–15, 2025 • Seattle, WA, USA 978-1-9391 33-52-6 Open access to the Proceedings of the 34th USENIX Security Symposium is sponsored by USENIX. Make Agent Defeat Agent: Automatic Detection of Taint-Style Vulnerabilities in LLM-based” (Make agent defeat agent Automatic detection of taint-style vulnerabilities in LLM-based agents.txt @段1) || 摘要 | This paper is included in the Proceedings of the 34th USENIX Security Symposium. August 13–15, 2025 • Seattle, WA, USA 978-1-9391 33-52-6 Open access to the Proceedings of the 34th USENIX Security Symposium is sponsored by USENIX. Make Agent Defeat Agent: Automatic Detection of Taint-Style Vulnerabilities in LLM-based Agents Fengyu Liu, Yuan Zhang, Jiaqi Luo, Jiarun Dai, Tian Chen, Letian Yuan, Zhengmin Yu, Youkun Shi, Ke Li, and Chengyuan Zhou, Fudan University; Hao Chen, UC Davis; Min Yang, Fudan University http… | 0.72 | “This paper is included in the Proceedings of the 34th USENIX Security Symposium. August 13–15, 2025 • Seattle, WA, USA 978-1-9391 33-52-6 Open access to the Proceedings of the 34th USENIX Security Symposium is sponsored by USENIX. Make Agent Defeat Agent: Automatic Detection of Taint-Style Vulnerabilities in LLM-based” (Make agent defeat agent Automatic detection of taint-style vulnerabilities in LLM-based agents.txt @段1) || 研究方法 | Abstract Large Language Models (LLMs) have revolutionized software development, enabling the creation of AI-powered applica- tions known as LLM-based agents. However, recent studies reveal that LLM-based agents are highly susceptible to taint- style vulnerabilities, which allow malicious prompts to exploit security-sensitive operations. These vulnerabilities pose se- vere threats to the security of agents, potentially allowing attackers to take over the entire agent remotely. In this paper, we propose a novel dire… | 0.82 | “In this paper, we propose a novel directed greybox fuzzing approach, called AgentFuzz, the first fuzzing framework for detecting taint-style vulnerabilities in LLM-based agents.” (Make agent defeat agent Automatic detection of taint-style vulnerabilities in LLM-based agents.txt @段2) || 主要结论 | 1 Introduction Large Language Models (LLMs) have demonstrated remark- able advancement in various downstream tasks, such as code generation [41,43], question answering [29,39,60], etc. Nowa- days, developers are actively integrating LLMs to build AI- powered applications, which are widely known as LLM- based agents [65, 68]. These emerging LLM-based agents could understand natural language instructions, perceive exter- nal environments, and intelligently carry out various actions. Currently, the ecosystem of LLM-b… | 0.80 | “Our evaluation shows that AgentFuzz suc- cessfully identified 34 0-day taint-style vulnerabilities (24 of which were undetected by existing approaches), achieving a precision rate of 100%, outperforming the state-of-the-art approach LLMSmith [49] by 33 times.” (Make agent defeat agent Automatic detection of taint-style vulnerabilities in LLM-based agents.txt @段3) |

- 主题标签：Jailbreak/越狱攻击, Web/Browser Agent 安全, 隐私与记忆风险
- 方法标签：仿真/Simulator, 基准/Benchmark
- 源文件：`Make agent defeat agent Automatic detection of taint-style vulnerabilities in LLM-based agents.txt`

</details>
<details><summary>118. The Hidden Dangers of Browsing AI Agents（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | The Hidden Dangers of Browsing AI Agents | 0.92 | “The Hidden Dangers of Browsing AI Agents” (The hidden dangers of browsing AI agents.txt @段2) || 作者 | Mykyta Mudryi*, 1, 2, Markiyan Chaklosh*, 1, 4, and Grzegorz Marcin | 0.90 | “Mykyta Mudryi*, 1, 2, Markiyan Chaklosh*, 1, 4, and Grzegorz Marcin” (The hidden dangers of browsing AI agents.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2505.13076v1  [cs.CR]  19 May 2025” (The hidden dangers of browsing AI agents.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2505.13076v1  [cs.CR]  19 May 2025” (The hidden dangers of browsing AI agents.txt @段1) || 关键词 | web, browsing, autonomous, tasks, browser, real, user, dangers, hidden, open | 0.78 | “arXiv:2505.13076v1 [cs.CR] 19 May 2025 The Hidden Dangers of Browsing AI Agents Mykyta Mudryi*, 1, 2, Markiyan Chaklosh*, 1, 4, and Grzegorz Marcin W´ ojcik2, 3 1ARIMLABS.AI 2Polish-Japanese Academy of Information Technology 3Maria Curie-Sklodowska University in Lublin 4University of the National Education Commission i” (The hidden dangers of browsing AI agents.txt @段1) || 摘要 | arXiv:2505.13076v1 [cs.CR] 19 May 2025 The Hidden Dangers of Browsing AI Agents Mykyta Mudryi*, 1, 2, Markiyan Chaklosh*, 1, 4, and Grzegorz Marcin W´ ojcik2, 3 1ARIMLABS.AI 2Polish-Japanese Academy of Information Technology 3Maria Curie-Sklodowska University in Lublin 4University of the National Education Commission in Krak´ ow {mmudryi, mchaklosh }@arimlabs.ai May 19, 2025 Abstract Autonomous browsing agents powered by large language models (LLMs) are increasingly used to automate web-based tasks. However, their… | 0.72 | “arXiv:2505.13076v1 [cs.CR] 19 May 2025 The Hidden Dangers of Browsing AI Agents Mykyta Mudryi*, 1, 2, Markiyan Chaklosh*, 1, 4, and Grzegorz Marcin W´ ojcik2, 3 1ARIMLABS.AI 2Polish-Japanese Academy of Information Technology 3Maria Curie-Sklodowska University in Lublin 4University of the National Education Commission i” (The hidden dangers of browsing AI agents.txt @段1) || 研究方法 | 4.4 Conclusion of the Mitigation Analysis Building secure autonomous browsing agents requires a multi-layered approach. Practical implementation strategies like prompt sanitization, sandboxing browsers, tokenizing au- thentication, and revoking credentials - address immediate technical risks in how agents operate. Meanwhile, theoretical models and frameworks such as the f-secure LLM sys- tem’s information flow control pipeline or AI agents augmented with formal security analyzers - provide blueprints for deeper re… | 0.82 | “Meanwhile, theoretical models and frameworks such as the f-secure LLM sys- tem’s information flow control pipeline or AI agents augmented with formal security analyzers - provide blueprints for deeper resilience by design.” (The hidden dangers of browsing AI agents.txt @段29) || 主要结论 | Introduction The process begins with the activation of the Perception stage, triggered by a user- defined task. Based on the literature review and demo observations, we found that ad- vanced AI agent platforms—such as OpenAI’sOperator[32] and Anthropic’sComputer Use[12]—implement this stage using smaller, specialized models instead of deploying a full-scale LLM. This design choice significantly improves computational efficiency and reduces processing overhead. In contrast, smaller-scale implementations and open-so… | 0.80 | “Security Considerations and Model Limitations While the aforementioned approach, which incorporates a specifically tuned SLM, may introduce additional susceptibility to prompt injection attacks due to its limited post-training phase, it remains inconclusive whether smaller models are inherently more vulnerable than lar” (The hidden dangers of browsing AI agents.txt @段10) |

- 主题标签：GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击, Web/Browser Agent 安全, 防御与对齐机制
- 方法标签：仿真/Simulator, 基准/Benchmark, 多模态/Multimodal, 形式化/Optimization, 系统防御/Defense, 调查/Survey
- 源文件：`综述参考文献\The hidden dangers of browsing AI agents.txt`

</details>
<details><summary>119. Towards Adaptive Software Agents for Debugging（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Towards Adaptive Software Agents for Debugging | 0.92 | “Towards Adaptive Software Agents for Debugging” (Towards adaptive software agents for debugging.txt @段1) || 作者 | IReSCoMath Research Lab, Faculty of | 0.90 | “IReSCoMath Research Lab, Faculty of” (Towards adaptive software agents for debugging.txt @段1) || 发表年份 | 2025 | 0.90 | “Yacine Majdoub, Eya Ben Charrada, and Haifa Touati. 2025. Towards Adap-” (Towards adaptive software agents for debugging.txt @段1) || 期刊/会议 | tive Software Agents for Debugging. In 33rd ACM International Conference | 0.88 | “tive Software Agents for Debugging. In 33rd ACM International Conference” (Towards adaptive software agents for debugging.txt @段1) || 关键词 | debugging, agentic, design, software, adaptive, number, acm, problem, systems, complex | 0.78 | “Towards Adaptive Software Agents for Debugging Yacine Majdoub∗ IReSCoMath Research Lab, Faculty of Sciences, University of Gabes, Tunisia yacine.majdoub@enig.rnu.tn Eya Ben Charrada IReSCoMath Research Lab, Faculty of Sciences, University of Gabes, Tunisia eya.bencharrada@fsg.rnu.tn Haifa Touati IReSCoMath Research Lab” (Towards adaptive software agents for debugging.txt @段1) || 摘要 | Towards Adaptive Software Agents for Debugging Yacine Majdoub∗ IReSCoMath Research Lab, Faculty of Sciences, University of Gabes, Tunisia yacine.majdoub@enig.rnu.tn Eya Ben Charrada IReSCoMath Research Lab, Faculty of Sciences, University of Gabes, Tunisia eya.bencharrada@fsg.rnu.tn Haifa Touati IReSCoMath Research Lab, Faculty of Sciences, University of Gabes, Tunisia haifa.touati@univgb.tn ABSTRACT Using multiple agents was found to improve the debugging capabil- ities of Large Language Models. However, increasi… | 0.72 | “Towards Adaptive Software Agents for Debugging Yacine Majdoub∗ IReSCoMath Research Lab, Faculty of Sciences, University of Gabes, Tunisia yacine.majdoub@enig.rnu.tn Eya Ben Charrada IReSCoMath Research Lab, Faculty of Sciences, University of Gabes, Tunisia eya.bencharrada@fsg.rnu.tn Haifa Touati IReSCoMath Research Lab” (Towards adaptive software agents for debugging.txt @段1) || 研究方法 | 4.1 Experiment design Dataset. For the initial evaluation, we used a dataset consisting of 50 buggy code instances from the benchmark DebugBench [21]. The code instances, which are written in python, are solution to coding problems from LeetCode1. Each instance includes different types of bugs with different levels of complexity. The bugs are categorized into four types of bugs: syntax errors, reference errors, logical errors and multiple errors . The complexity level of the instances ranges from simple problems r… | 0.82 | “The complexity level of the instances ranges from simple problems requiring basic syntax fixes to more advanced issues that involve algorithmic logic and optimization challenges.” (Towards adaptive software agents for debugging.txt @段15) || 主要结论 | N/A | 0.50 | N/A |

- 主题标签：N/A
- 方法标签：基准/Benchmark, 形式化/Optimization
- 源文件：`Towards adaptive software agents for debugging.txt`

</details>
<details><summary>120. Vulnerability to Fine-Print Injections（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | Vulnerability to Fine-Print Injections | 0.92 | “Vulnerability to Fine-Print Injections” (The obvious invisible threat LLM-powered GUI agents' vulnerability to fine-print injections.txt @段2) || 作者 | Notre Dame, Indiana, USA | 0.90 | “Notre Dame, Indiana, USA” (The obvious invisible threat LLM-powered GUI agents' vulnerability to fine-print injections.txt @段1) || 发表年份 | 2025 | 0.86 | “arXiv:2504.11281v1  [cs.HC]  15 Apr 2025” (The obvious invisible threat LLM-powered GUI agents' vulnerability to fine-print injections.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2504.11281v1  [cs.HC]  15 Apr 2025” (The obvious invisible threat LLM-powered GUI agents' vulnerability to fine-print injections.txt @段1) || 关键词 | gui, human, privacy, edu, usa, user, dame, notre, adversarial, interface | 0.78 | “The Obvious Invisible Threat: LLM-Powered GUI Agents’ Vulnerability to Fine-Print Injections Chaoran Chen cchen25@nd.edu University of Notre Dame Notre Dame, Indiana, USA Zhiping Zhang zhang.zhip@northeastern.edu Northeastern University Boston, Massachusetts, USA Bingcan Guo bguoac@uw.edu University of Washington Seatt” (The obvious invisible threat LLM-powered GUI agents' vulnerability to fine-print injections.txt @段1) || 摘要 | The Obvious Invisible Threat: LLM-Powered GUI Agents’ Vulnerability to Fine-Print Injections Chaoran Chen cchen25@nd.edu University of Notre Dame Notre Dame, Indiana, USA Zhiping Zhang zhang.zhip@northeastern.edu Northeastern University Boston, Massachusetts, USA Bingcan Guo bguoac@uw.edu University of Washington Seattle, Washington, USA Shang Ma sma5@nd.edu University of Notre Dame Notre Dame, Indiana, USA Ibrahim Khalilov ibrahimk@vt.edu Virginia Tech Blacksburg, Virginia, USA Simret A Gebreegziabher sgebreeg@nd… | 0.72 | “The Obvious Invisible Threat: LLM-Powered GUI Agents’ Vulnerability to Fine-Print Injections Chaoran Chen cchen25@nd.edu University of Notre Dame Notre Dame, Indiana, USA Zhiping Zhang zhang.zhip@northeastern.edu Northeastern University Boston, Massachusetts, USA Bingcan Guo bguoac@uw.edu University of Washington Seatt” (The obvious invisible threat LLM-powered GUI agents' vulnerability to fine-print injections.txt @段1) || 研究方法 | N/A | 0.50 | N/A || 主要结论 | 8 Conclusion We evaluated GUI agents across six types of privacy and security attacks and benchmarked agent performance against human users, revealing widespread vulnerabilities in realistic interaction scenar- ios. Among our contributions is Fine-Print Injection, a novel attack that exploits agents’ overreliance on low-salience content, which proved highly effective across multiple models. Our results showed that even advanced agents frequently leaked sensitive data or fell for deceptive prompts, especially in su… | 0.80 | “Our results showed that even advanced agents frequently leaked sensitive data or fell for deceptive prompts, especially in subtle, context-embedded at- tacks.” (The obvious invisible threat LLM-powered GUI agents' vulnerability to fine-print injections.txt @段29) |

- 主题标签：GUI/Computer-Use Agent 安全, Prompt Injection/环境注入, 隐私与记忆风险
- 方法标签：基准/Benchmark, 多模态/Multimodal, 提示注入/Prompt Injection
- 源文件：`综述参考文献\The obvious invisible threat LLM-powered GUI agents' vulnerability to fine-print injections.txt`

</details>
<details><summary>121. design architecture that demonstrates the feasibility of execution（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | design architecture that demonstrates the feasibility of execution | 0.92 | “design architecture that demonstrates the feasibility of execution” (IsolateGPT An execution isolation architecture for LLM-based systems.txt @段23) || 作者 | Y uhao Wu→, Franziska Roesner †, Tadayoshi Kohno †, Ning Zhang →, Umar Iqbal → | 0.90 | “Y uhao Wu→, Franziska Roesner †, Tadayoshi Kohno †, Ning Zhang →, Umar Iqbal →” (IsolateGPT An execution isolation architecture for LLM-based systems.txt @段1) || 发表年份 | 2025 | 0.86 | “Network and Distributed System Security (NDSS) Symposium 2025” (IsolateGPT An execution isolation architecture for LLM-based systems.txt @段1) || 期刊/会议 | Unknown venue (DOI: 10.14722/ndss.2025.241131) | 0.82 | “10.14722/ndss.2025.241131” (IsolateGPT An execution isolation architecture for LLM-based systems.txt @段1) || 关键词 | apps, user, tegpt, data, sola, spoke, system, hub, execution, memory | 0.78 | “ISOLA TEGPT: An Execution Isolation Architecture for LLM-Based Agentic Systems Y uhao Wu→, Franziska Roesner †, Tadayoshi Kohno †, Ning Zhang →, Umar Iqbal → →Washington University in St. Louis, †University of Washington {yuhao.wu, zhang.ning, umar.iqbal }@wustl.edu, {franzi, yoshi }@cs.washington.edu Abstract—Large la” (IsolateGPT An execution isolation architecture for LLM-based systems.txt @段1) || 摘要 | ISOLA TEGPT: An Execution Isolation Architecture for LLM-Based Agentic Systems Y uhao Wu→, Franziska Roesner †, Tadayoshi Kohno †, Ning Zhang →, Umar Iqbal → →Washington University in St. Louis, †University of Washington {yuhao.wu, zhang.ning, umar.iqbal }@wustl.edu, {franzi, yoshi }@cs.washington.edu Abstract—Large language models (LLMs) extended as systems, such as ChatGPT, have begun supporting third-party applica- tions. These LLM apps leverage the de facto natural language- based automated execution paradigm … | 0.72 | “ISOLA TEGPT: An Execution Isolation Architecture for LLM-Based Agentic Systems Y uhao Wu→, Franziska Roesner †, Tadayoshi Kohno †, Ning Zhang →, Umar Iqbal → →Washington University in St. Louis, †University of Washington {yuhao.wu, zhang.ning, umar.iqbal }@wustl.edu, {franzi, yoshi }@cs.washington.edu Abstract—Large la” (IsolateGPT An execution isolation architecture for LLM-based systems.txt @段1) || 研究方法 | REFERENCES [1] OpenAI, “Introducing chatgpt,” https://openai.com/blog/chatgpt, 2023. [Online]. Available: https://openai.com/blog/chatgpt [2] Google, “Google gemini,” https://gemini.google.com/, 2023. [3] Amazon, “Previewing the future of alexa,” https://aboutamazon.com/ news/devices/amazon-alexa-generative-ai, 2023. [4] Rabbit, “Rabbit os,” https://www.rabbit.tech/rabbit-os, 2024. [5] Humane, “Ai pin,” https://hu.ma.ne/aipin, 2024. [6] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Y ang, J. Zhang, Z. Chen, J. Tang, X. Ch… | 0.82 | “APPENDIX A ADDITIONAL DESIGN AND IMPLEMENTA TION DETAILS We develop I SOLA TEGPT using LangChain (version 0.1.10), an open-source LLM framework [ 14].” (IsolateGPT An execution isolation architecture for LLM-based systems.txt @段3) || 主要结论 | ISOLA TEGPT: An Execution Isolation Architecture for LLM-Based Agentic Systems Y uhao Wu→, Franziska Roesner †, Tadayoshi Kohno †, Ning Zhang →, Umar Iqbal → →Washington University in St. Louis, †University of Washington {yuhao.wu, zhang.ning, umar.iqbal }@wustl.edu, {franzi, yoshi }@cs.washington.edu Abstract—Large language models (LLMs) extended as systems, such as ChatGPT, have begun supporting third-party applica- tions. These LLM apps leverage the de facto natural language- based automated execution paradigm … | 0.80 | “Our key contributions are as follows: 1) We demonstrate the feasibility of execution isolation in the natural language-based automated execution paradigm of LLM-based systems in mitigating security and privacy issues that arise with the execution of third-party apps.” (IsolateGPT An execution isolation architecture for LLM-based systems.txt @段1) |

- 主题标签：防御与对齐机制, 隐私与记忆风险
- 方法标签：调查/Survey
- 源文件：`IsolateGPT An execution isolation architecture for LLM-based systems.txt`

</details>
<details><summary>122. testing in realistic but controlled environments or ignore hybrid web-OS attack（2025）</summary>

| 字段 | 抽取结果 | 置信度 | 可追溯证据（原文句段） |
|---|---|---:|---|
| 标题 | testing in realistic but controlled environments or ignore hybrid web-OS attack | 0.92 | “testing in realistic but controlled environments or ignore hybrid web-OS attack” (RedTeamCUA Realistic adversarial testing of computer-use agents in hybrid web-OS environments.txt @段14) || 作者 | systems (OS) and the web, but remain vulnerable to indirect prompt injection, | 0.90 | “systems (OS) and the web, but remain vulnerable to indirect prompt injection,” (RedTeamCUA Realistic adversarial testing of computer-use agents in hybrid web-OS environments.txt @段1) || 发表年份 | 2025 | 0.86 | “The development of computer-use agents (CUAs) (Anthropic., 2024c; OpenAI., 2025b) capable of” (RedTeamCUA Realistic adversarial testing of computer-use agents in hybrid web-OS environments.txt @段1) || 期刊/会议 | arXiv | 0.90 | “arXiv:2505.21936v3  [cs.CL]  15 Oct 2025” (RedTeamCUA Realistic adversarial testing of computer-use agents in hybrid web-OS environments.txt @段1) || 关键词 | adversarial, web, cuas, environments, 2025, injection, realistic, redteamcua, testing, 2024 | 0.78 | “RedTeamCUA REDTEAMCUA: REALISTICADVERSARIALTESTING OFCOMPUTER- USEAGENTS INHYBRIDWEB-OS ENVIRONMENTS Zeyi Liao∗ Jaylen Jones∗ Linxi Jiang∗ Yuting Ning Eric Fosler-Lussier Yu Su Zhiqiang Lin Huan Sun The Ohio State University {liao.629, jones.6278, jiang.3002, sun.397}@osu.edu” (RedTeamCUA Realistic adversarial testing of computer-use agents in hybrid web-OS environments.txt @段1) || 摘要 | RedTeamCUA REDTEAMCUA: REALISTICADVERSARIALTESTING OFCOMPUTER- USEAGENTS INHYBRIDWEB-OS ENVIRONMENTS Zeyi Liao∗ Jaylen Jones∗ Linxi Jiang∗ Yuting Ning Eric Fosler-Lussier Yu Su Zhiqiang Lin Huan Sun The Ohio State University {liao.629, jones.6278, jiang.3002, sun.397}@osu.edu ABSTRACT Computer-use agents (CUAs) promise to automate complex tasks across operating systems (OS) and the web, but remain vulnerable to indirect prompt injection, where attackers embed malicious content into the environment to hijack agent … | 0.72 | “RedTeamCUA REDTEAMCUA: REALISTICADVERSARIALTESTING OFCOMPUTER- USEAGENTS INHYBRIDWEB-OS ENVIRONMENTS Zeyi Liao∗ Jaylen Jones∗ Linxi Jiang∗ Yuting Ning Eric Fosler-Lussier Yu Su Zhiqiang Lin Huan Sun The Ohio State University {liao.629, jones.6278, jiang.3002, sun.397}@osu.edu” (RedTeamCUA Realistic adversarial testing of computer-use agents in hybrid web-OS environments.txt @段1) || 研究方法 | 3 REDTEAMCUA - HYBRIDENVIRONMENTSANDBOX To enable realistic and systematic adversarial testing of CUAs, we propose REDTEAMCUA, a flexible framework featuring a hybrid sandbox that integrates established OS and web evaluation platforms to marry their strengths (details in Figure 6). This section outlines the OS and web components used in our hybrid sandbox approach along with core features within our framework tailored specifically for rigorous and scalable adversarial evaluation. Using REDTEAMCUA, we enable flexib… | 0.82 | “3 REDTEAMCUA - HYBRIDENVIRONMENTSANDBOX To enable realistic and systematic adversarial testing of CUAs, we propose REDTEAMCUA, a flexible framework featuring a hybrid sandbox that integrates established OS and web evaluation platforms to marry their strengths (details in Figure 6).” (RedTeamCUA Realistic adversarial testing of computer-use agents in hybrid web-OS environments.txt @段7) || 主要结论 | REFERENCES Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: A compositional generalist-specialist framework for computer use agents.arXiv preprint arXiv:2504.00906, 2025. Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, J Zico Kolter, Matt Fredrikson, Yarin Gal, and Xander Davies. Agentharm: A benchmark for measuring harmfulness of LLM agents. InThe Thirteenth International Conference on Learning Repre… | 0.80 | “Our results highlight an urgent need for defenses specifically tailored to protect CUAs against indirect prompt injection, and we hope that our sandbox REDTEAMCUA and benchmark RTC-BENCHcan serve as resources for advancing this line of research.” (RedTeamCUA Realistic adversarial testing of computer-use agents in hybrid web-OS environments.txt @段26) |

- 主题标签：GUI/Computer-Use Agent 安全, Jailbreak/越狱攻击, Prompt Injection/环境注入, Web/Browser Agent 安全
- 方法标签：仿真/Simulator, 基准/Benchmark, 提示注入/Prompt Injection, 红队/Red-Teaming
- 源文件：`综述参考文献\RedTeamCUA Realistic adversarial testing of computer-use agents in hybrid web-OS environments.txt`

</details>


## 参考文献列表（按文件）
- Great, Now Write an Article About That: (n.d.). The Crescendo Multi-Turn LLM Jailbreak Attack. Unknown venue. 〔源文件：`Great, Now Write an Article About That- The Crescendo Multi-Turn LLM Jailbreak Attack.txt`〕- SHUNING ZHANG, Tsinghua University, China (2018). Characterizing Unintended Consequences in Human-GUI Agent. arXiv. 〔源文件：`综述参考文献\Characterizing unintended consequences in human-GUI agent collaboration for web browsing.txt`〕- Aishan Liu1, Yuguang Zhou1, Xianglong Liu1*, Tianyuan Zhang1, Siyuan Liang2, Jiakai (2021). Compromising Embodied Agents with Contextual. arXiv. 〔源文件：`Compromising embodied agents with contextual backdoor attacks.txt`〕- Saikat Barua1,a, Mostafizur Rahman2, Rafiul Islam3, Shehenaz Khaled4, Md Jafor Sadek5, and Dr. Ahmedul Kabir6 (2022). Deceptive Alignment · Reverse Turing Test · Multi-Agent Systems · Prompt Injection · Agent Autonomy · Ethical. arXiv. 〔源文件：`综述参考文献\Guardians of the agentic system Preventing many shots jailbreak with agentic system.txt`〕- The strong planning and reasoning capabilities of (2023). AGENT VIGIL : Generic Black-Box Red-teaming for Indirect Prompt Injection. arXiv. 〔源文件：`综述参考文献\AgentVigil Generic black-box red-teaming for indirect prompt injection against LLM agents.txt`〕- August 11-16, 2024 ©2024 Association for Computational Linguistics (2023). BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 9811–9827. 〔源文件：`BadAgent Inserting and activating backdoor attacks in LLM agents.txt`〕- Findings of the Association for Computational Linguistics: ACL 2024 , pages 9909–9953 (2023). Boosting LLM Agents with Recursive Contemplation. Findings of the Association for Computational Linguistics: ACL 2024 , pages 9909–9953. 〔源文件：`Boosting LLM agents with recursive contemplation for effective deception handling.txt`〕- Itay Nakash, George Kour, Guy Uziel, Ateret Anaby-Tavor (2023). Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In. arXiv. 〔源文件：`综述参考文献\Breaking ReAct agents Foot-in-the-door attack will get you in.txt`〕- Large Language Model based Multi-Agents: A Survey of Progress and Challenges (2023). DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis. Proceedings of the Thirty-Third International Joint Conference on Artiﬁcial Intelligence (IJCAI-24). 〔源文件：`DiffECG Diffusion model-powered label-efficient and personalized arrhythmia diagnosis.txt`〕- (LLMs) have revitalized in LLM-based agents, ex- (2023). Evil Geniuses: Delving into the Safety of LLM-based Agents. arXiv. 〔源文件：`综述参考文献\Evil geniuses Delving into the safety of LLM-based agents.txt`〕- ♠ The Ohio State University ♣ University of Chicago ✠University of Wisconsin, Madison (2023). GENERALIST WEB AGENTS FOR PRIVACY LEAKAGE. arXiv. 〔源文件：`综述参考文献\EIA Environmental injection attack on generalist web agents for privacy leakage.txt`〕- In recent years, large language models (LLMs) (2023). LLM Agents can Autonomously Hack Websites. arXiv. 〔源文件：`综述参考文献\LLM agents can autonomously hack websites.txt`〕- 1The Ohio State University 2University of Wisconsin, Madison (2023). Mapping Adversarial Attacks against Language Agents. arXiv. 〔源文件：`综述参考文献\A trembling house of cards Mapping adversarial attacks against language agents.txt`〕- Multi-Agent Security Tax: Trading Off Security and Collaboration Capabilities in (2023). Multi-Agent Systems. The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25). 〔源文件：`Multi-agent security tax Trading off security and collaboration capabilities in multi-agent systems.txt`〕- agent can receive instructions, capture images, (2023). Multimodal LLM Agents Exponentially Fast. Proceedings of the 41 st International Conference on Machine. 〔源文件：`Agent smith A single image can jailbreak one million multimodal LLM agents exponentially fast.txt`〕- July 27 - August 1, 2025 ©2025 Association for Computational Linguistics (2023). Optimized Prompt Attacks. Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 9661–9674. 〔源文件：`Agents under siege Breaking pragmatic multi-agent LLM systems with optimized prompt attacks.txt`〕- 1 University of Wisconsin–Madison, 2 USC, 3 University of California, Davis (2023). PROMPTS ON ALIGNED LARGE LANGUAGE MODELS. arXiv. 〔源文件：`综述参考文献\AutoDAN Generating stealthy jailbreak prompts on aligned large language models.txt`〕- PsySafe: A Comprehensive Framework for Psychological-based Attack, (2023). PsySafe: A Comprehensive Framework for Psychological-based Attack,. arXiv. 〔源文件：`综述参考文献\PsySafe A comprehensive framework for psychological-based attack, defense, and evaluation of multi-.txt`〕- Shengbin Yue1, Siyuan Wang2, Wei Chen3, Xuanjing Huang 1, (2023). Synergistic Multi-Agent Framework with Trajectory Learning for. The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI- 25). 〔源文件：`Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks.txt`〕- Findings of the Association for Computational Linguistics: ACL 2024 , pages 10471–10506 (2023). Tool-Integrated Large Language Model Agents. Findings of the Association for Computational Linguistics: ACL 2024 , pages 10471–10506. 〔源文件：`InjecAgent Benchmarking indirect prompt injections in tool-integrated large language model agents.txt`〕- TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING 1 (2023). Tool-Using Large Language Model Agents. Unknown venue. 〔源文件：`PrivacyAsst Safeguarding user privacy in tool-using large language model agents.txt`〕- TrustAgent: Towards Safe and Trustworthy LLM-based Agents (2023). TrustAgent: Towards Safe and Trustworthy LLM-based Agents. arXiv. 〔源文件：`综述参考文献\TrustAgent Towards safe and trustworthy LLM-based agents.txt`〕- Zhuosheng Zhang1*, Aston Zhang 2∗ (2023). Y ou Only Look at Screens: Multimodal Chain-of-Action Agents. arXiv. 〔源文件：`综述参考文献\You only look at screens Multimodal chain-of-action agents.txt`〕- A Survey on the Safety and Security Threats of Computer-Using Agents (2024). A Survey on the Safety and Security Threats of Computer-Using Agents. arXiv. 〔源文件：`JARVIS or Ultron A Survey on the Safety and Security Threats of Computer-Using Agents.txt`〕- Ke Yang†∗, Yao Liu ♢, Sapana Chaudhary ♢, Rasool Fakoor ♢, Pratik Chaudhari ♢, George (2024). AGENT OCCAM : A S IMPLE YET STRONG BASELINE. arXiv. 〔源文件：`综述参考文献\AgentOccam A simple yet strong baseline for LLM-based web agents.txt`〕- Zhexin Zhang∗, Shiyao Cui∗, Yida Lu∗, Jingzhuo Zhou∗, Junxiao Yang, (2024). AGENT-SAFETY BENCH : Evaluating the Safety of. arXiv. 〔源文件：`综述参考文献\Agent-SafetyBench Evaluating the safety of LLM agents.txt`〕- July 27 - August 1, 2025 ©2025 Association for Computational Linguistics (2024). AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety. Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 8104–8139. 〔源文件：`AGrail A lifelong agent guardrail with effective and adaptive safety detection.txt`〕- Large Language Models (LLMs) can elicit unintended and even harmful content (2024). ALI-Agent: Assessing LLMs'Alignment with Human. 38th Conference on Neural Information Processing Systems (NeurIPS 2024).. 〔源文件：`ALI-agent Assessing LLMs'alignment with human values via agent-based evaluation.txt`〕- Qiusi Zhan1, Richard Fang 1, Henil Shalin Panchal 2, Daniel Kang 1 (2024). Adaptive Attacks Break Defenses Against Indirect Prompt Injection. arXiv. 〔源文件：`综述参考文献\Adaptive attacks break defenses against indirect prompt injection attacks on LLM agents.txt`〕- used to automate complex tasks, enhancing effi- (2024). AdvAgent: Controllable Blackbox Red-teaming on Web Agents. Proceedings of the 42 nd International Conference on Machine. 〔源文件：`AdvAgent Controllable blackbox red-teaming on web agents.txt`〕- SHIYU SUN, George Mason University, Fairfax, VA, United States (2024). Agent Large Language Model. CCS '24: ACM SIGSAC Conference on. 〔源文件：`Poster Repairing bugs with the introduction of new variables a multi-agent large language model.txt`〕- RepairAgent: An Autonomous, LLM-Based (2024). Agent for Program Repair. arXiv. 〔源文件：`RepairAgent An autonomous, LLM-based agent for program repair.txt`〕- erful for complex tasks such as web shopping, (2024). Agents by Dynamically Hijacking Their Own Reasoning. Proceedings of the 42 nd International Conference on Machine. 〔源文件：`UDora A unified red teaming framework against LLM agents by dynamically hijacking their own reasoni.txt`〕- EUGENE BAGDASARIAN, Google LLC, Mountain View, CA, United States (2024). AirGapAgent: Protecting Privacy-Conscious Conversational Agents. CCS '24: ACM SIGSAC Conference on. 〔源文件：`AirGapAgent Protecting privacy-conscious conversational agents.txt`〕- Hao Wen1, Yuanchun Li1,2,†, Guohong Liu1, Shanhui Zhao1,∗, Tao Yu1,∗, (2024). Android task automation with 158 common tasks. The results. arXiv. 〔源文件：`综述参考文献\AutoDroid LLM-powered task automation in android.txt`〕- July 27 - August 1, 2025 ©2025 Association for Computational Linguistics (2024). Attacking Vision-Language Computer Agents via Pop-ups. Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 8387–8401. 〔源文件：`Attacking vision-language computer agents via pop-ups.txt`〕- attacks, where malicious tasks embedded in tool- (2024). Attacks in AI Agents. Proceedings of the 42 nd International Conference on Machine. 〔源文件：`MELON Provable defense against indirect prompt injection attacks in AI agents.txt`〕- Jean Marie Tshimula,1,2 Xavier Ndona,3 D’Jeff K. Nkashama,1 Pierre-Martin Tardif,1 (2024). Cyber Defense Perspective. arXiv. 〔源文件：`综述参考文献\Preventing jailbreak prompts as malicious tools for cybercriminals A cyber defense perspective.txt`〕- Chen Henry Wu, Rishi Shah, Jing Yu Koh, Ruslan Salakhutdinov, (2024). DISSECTING ADVERSARIAL ROBUSTNESS OF. arXiv. 〔源文件：`综述参考文献\Dissecting adversarial robustness of multimodal LM agents.txt`〕- Pengyu Zhu1,⋆ (2024). Dynamically Encrypted Multi-Backdoor Im-. arXiv. 〔源文件：`综述参考文献\DemonAgent Dynamically encrypted multi-backdoor implantation attack on LLM-based agent.txt`〕- Findings of the Association for Computational Linguistics: ACL 2025 , pages 15143–15168 (2024). Enhancing LLM Agent Safety via Causal Influence Prompting. Findings of the Association for Computational Linguistics: ACL 2025 , pages 15143–15168. 〔源文件：`Enhancing LLM agent safety via causal influence prompting.txt`〕- Evaluating Cultural and Social Awareness of LLM Web Agents (2024). Evaluating Cultural and Social Awareness of LLM Web Agents. arXiv. 〔源文件：`综述参考文献\Evaluating cultural and social awareness of LLM web agents.txt`〕- Leo Boisvert†‡, Mihir Bansal†, Chandra Kiran Reddy Evuru†, Gabriel Huang†, Abhay Puri†, (2024). Evolving Security Threats. arXiv. 〔源文件：`综述参考文献\DoomArena A framework for testing AI agents against evolving security threats.txt`〕- complex data, their potential misalignment (i.e., lack of transparency regarding (2024). Existence of Probably Approximately Aligned Policies. 38th Conference on Neural Information Processing Systems (NeurIPS 2024).. 〔源文件：`Can an AI agent safely run a government Existence of probably approximately aligned policies.txt`〕- DEVJEET ROY, Washington State University Pullman, Pullman, WA, United States (2024). Exploring LLM-Based Agents for Root Cause Analysis. Conference on the Foundations of. 〔源文件：`Exploring LLM-based agents for root cause analysis.txt`〕- GUI-W ORLD : A V IDEO BENCHMARK AND DATASET (2024). FOR MULTIMODAL GUI- ORIENTED UNDERSTANDING. arXiv. 〔源文件：`综述参考文献\GUI-world A video benchmark and dataset for multimodal GUI-oriented understanding.txt`〕- Ning Wang1 , Zihan Yan1 , Weiyang Li1 , Chuan Ma1∗ , He Chen2 and Tao Xiang1 (2024). From Safety Benchmarks to Input Moderation. Proceedings of the Thirty-Fourth International Joint Conference on Artiﬁcial Intelligence (IJCAI-25). 〔源文件：`Advancing embodied agent security From safety benchmarks to input moderation.txt`〕- Zehao Wang, Dong Jae Kim, Tse-Husn (Peter) Chen (2024). Identifying Performance-Sensitive Configurations in Software. arXiv. 〔源文件：`Identifying performance-sensitive configurations in software systems through code analysis with LLM.txt`〕- Combining Fine-tuning and LLM-based Agents for (2024). Intuitive Smart Contract Auditing with Justifications. arXiv. 〔源文件：`Combining fine-tuning and LLM-based agents for intuitive smart contract auditing with justifications.txt`〕- Yifan Zeng1,*, Yiran Wu2,*, Xiao Zhang3, Huazheng Wang1, Qingyun Wu2 (2024). Jailbreak Attacks. arXiv. 〔源文件：`综述参考文献\AutoDefense Multi-agent LLM defense against jailbreak attacks.txt`〕- Nan Zhuang1*, Boyu Cao 2*, Yi Yang2*, Jing Xu 3*, Mingda Xu 2*, Yuxiao Wang2, Qi Liu †2 (2024). LLM Agents Can Be Choice-Supportive Biased Evaluators: An Empirical Study. The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25). 〔源文件：`LLM agents can be choice-supportive biased evaluators An empirical study.txt`〕- July 27 - August 1, 2025 ©2025 Association for Computational Linguistics (2024). LLM Agents are Susceptible to Environmental Distractions. Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 22324–22339. 〔源文件：`Caution for the environment Multimodal LLM agents are susceptible to environmental distractions.txt`〕- July 27 - August 1, 2025 ©2025 Association for Computational Linguistics (2024). LLM-based Multi-agent Systems. Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 7261–7276. 〔源文件：`G-safeguard A topology-guided security lens and treatment on LLM-based multi-agent systems.txt`〕- Pengzhou Cheng, Haowen Hu, Zheng Wu, Zongru Wu, Tianjie Ju (2024). MLLM-Powered Mobile GUI Agents. arXiv. 〔源文件：`综述参考文献\Hidden ghost hand Unveiling backdoor vulnerabilities in MLLM-powered mobile GUI agents.txt`〕- Songqin Nong*, Jiali Zhu*, Rui Wu*, Jiongchao Jin, Shuo Shan, Xiutian Huang, Wenhao Xu (2024). Mandarin apps. This paper introduces MobileFlow, a multimodal large language model meticulously. arXiv. 〔源文件：`综述参考文献\MobileFlow A multimodal LLM for mobile GUI agent.txt`〕- Vardaan Pahuja1∗†, Yadong Lu 2*¶, Corby Rosset 2, Boyu Gou 1, (2024). Multimodal Web Agents. arXiv. 〔源文件：`综述参考文献\Explorer Scaling exploration-driven web trajectory synthesis for multimodal web agents.txt`〕- due to the collaboration of expert agents, each (2024). On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents. arXiv. 〔源文件：`综述参考文献\On the resilience of LLM-based multi-agent collaboration with faulty agents.txt`〕- Poison Once, Control Anywhere: Clean-Text Visual Backdoors in (2024). Poison Once, Control Anywhere: Clean-Text Visual Backdoors in. arXiv. 〔源文件：`综述参考文献\Poison once, control anywhere Clean-text visual backdoors in VLM-based mobile agents.txt`〕- Zhaorun Chen1∗, Zhen Xiang2, Chaowei Xiao3, Dawn Song4, Bo Li12 ∗ (2024). Poisoning Memory or Knowledge Bases. 38th Conference on Neural Information Processing Systems (NeurIPS 2024).. 〔源文件：`AGENTPOISON Red-teaming LLM agents via poisoning memory or knowledge bases.txt`〕- Preemptive Detection and Correction of Misaligned Actions in LLM Agents (2024). Preemptive Detection and Correction of Misaligned Actions in LLM Agents. arXiv. 〔源文件：`综述参考文献\Preemptive detection and correction of misaligned actions in LLM agents.txt`〕- Progress and Prospects (2024). Progress and Prospects. arXiv. 〔源文件：`LLM-Powered GUI Agents in Phone Automation Surveying Progress and Prospects.txt`〕- productivity by automating routine tasks such as filing taxes and paying bills. (2024). Prompt Injection Attacks. arXiv. 〔源文件：`综述参考文献\WASP Benchmarking web agent security against prompt injection attacks.txt`〕- Prompt Injection Attacks and Defenses (2024). Prompt Injection Attacks and Defenses. 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.. 〔源文件：`AgentDojo A dynamic environment to evaluate prompt injection attacks and defenses for LLM agents.txt`〕- Tianneng Shi1, Kaijie Zhu2, Zhun Wang1, Yuqi Jia3, (2024). PromptArmor: Simple yet Effective Prompt Injection Defenses. arXiv. 〔源文件：`综述参考文献\PromptArmor Simple yet effective prompt injection defenses.txt`〕- Findings of the Association for Computational Linguistics: ACL 2025 , pages 6726–6747 (2024). Red-Teaming LLM Multi-Agent Systems via Communication Attacks. Findings of the Association for Computational Linguistics: ACL 2025 , pages 6726–6747. 〔源文件：`Red-teaming LLM multi-agent systems via communication attacks.txt`〕- Priyanshu Kumar*1, Elaine Lau◦3, Saranya Vijayakumar◦1, T u Trinh◦3, Scale Red Team3, (2024). Refusal-Trained LLMs Are Easily Jailbroken. arXiv. 〔源文件：`综述参考文献\Refusal-trained LLMs are easily jailbroken as browser agents.txt`〕- ous real-world applications. However, they re- (2024). SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning. arXiv. 〔源文件：`综述参考文献\ShieldAgent Shielding agents via verifiable safety policy reasoning.txt`〕- Feng Lin 1, Dong Jae Kim 2, Tse-Hsun (Peter) Chen 1 (2024). Software Process Models Using Large Language. arXiv. 〔源文件：`SOEN-101 Code generation by emulating software process models using large language model agents.txt`〕- July 27 - August 1, 2025 ©2025 Association for Computational Linguistics (2024). The Task Shield: Enforcing Task Alignment to Defend Against Indirect. Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 29680–29697. 〔源文件：`The task shield Enforcing task alignment to defend against indirect prompt injection in LLM agents.txt`〕- KONRAD CINKUSZ, Warsaw University of Technology, Warsaw, MA, Poland (2024). Towards LLM-augmented multiagent systems for agile soware. arXiv. 〔源文件：`Towards LLM-augmented multiagent systems for agile software engineering.txt`〕- Yucheng Shi1, Wenhao Yu2, Wenlin Yao3, Wenhu Chen4, Ninghao Liu 1 (2024). Towards Trustworthy GUI Agents: A Survey. arXiv. 〔源文件：`综述参考文献\Towards trustworthy GUI agents A survey.txt`〕- LUQIAO WANG, Xidian University, Xi'an, Shaanxi, China (2024). Unity Is Strength: Collaborative LLM-Based Agents for Code Reviewer. Conference on Automated Soware. 〔源文件：`Unity is strength Collaborative LLM-based agents for code reviewer recommendation.txt`〕- July 27 - August 1, 2025 ©2025 Association for Computational Linguistics (2024). Unveiling Privacy Risks in LLM Agent Memory. Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers) , pages 25241–25260. 〔源文件：`Unveiling privacy risks in LLM agent memory.txt`〕- Wenkai Yang∗1, Xiaohan Bi∗2, Yankai Lin†1, Sishuo Chen2, Jie Zhou3, Xu Sun†4 (2024). Watch Out for Your Agents! Investigating Backdoor. 38th Conference on Neural Information Processing Systems (NeurIPS 2024).. 〔源文件：`Watch out for your agents! Investigating backdoor threats to LLM-based agents.txt`〕- Despite their growing adoption across domains, (2024). Y our Agent Can Defend Itself against Backdoor Attacks. arXiv. 〔源文件：`综述参考文献\Your agent can defend itself against backdoor attacks.txt`〕- RedCode: Risky Code Execution and Generation (2024). evaluation of unsafe code generation and execution, diverse input formats, and high-. 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.. 〔源文件：`RedCode Risky code execution and generation benchmark for code agents.txt`〕- With the fast development of large language models (LLMs), (2024). introduce a novel threat, WIPI, that indirectly controls Web. arXiv. 〔源文件：`综述参考文献\WIPI A new web threat for LLM-driven web agents.txt`〕- Models, Retrieval Augmented Generation, and Chain-of-Thought (2025). A Multi-agent Onboarding Assistant based on Large Language. Conference on the Foundations of Software Engineering (FSE Companion ’25),. 〔源文件：`A multi-agent onboarding assistant based on large language models, retrieval augmented generation, a.txt`〕- XINCHEN WANG, Harbin Institute of Technology Shenzhen, Shenzhen, Guangdong, China (2025). AEGIS: An Agent-based Framework for Bug Reproduction from Issue. International Conference on the. 〔源文件：`AEGIS An agent-based framework for bug reproduction from issue descriptions.txt`〕- 2830 IEEE TRANSACTIONS ON SOFTW ARE ENGINEERING, VOL. 51, NO. 10, OCTO BER 2025 (2025). Advanced Smart Contract Vulnerability Detection. Unknown venue (DOI: 10.1109/TSE.2025.3597319). 〔源文件：`Advanced smart contract vulnerability detection via LLM-powered multi-agent systems.txt`〕- LINGZHE ZHANG, Peking University, Beijing, China (2025). AgentFM: Role-Aware Failure Management for Distributed Databases. International Conference on the. 〔源文件：`AgentFM Role-aware failure management for distributed databases with LLM-driven multi-agents.txt`〕- Haowei Wang1,2,3, Junjie Wang1,2,3∗ (2025). Agents via Advertising Delivery. arXiv. 〔源文件：`综述参考文献\AdInject Real-world black-box attacks on web agents via advertising delivery.txt`〕- JOURNAL OF LATEX CLASS FILES, VOL. X, NO. Y , FEBRUARY 2025 1 (2025). Amplified Vulnerabilities: Structured Jailbreak Attacks. arXiv. 〔源文件：`综述参考文献\Amplified vulnerabilities Structured jailbreak attacks on LLM-based multi-agent debate.txt`〕- YIRAN ZHANG, Nanyang Technological University, Singapore City, Singapore (2025). Architecture Design. International Conference on the. 〔源文件：`Knowledge-based multi-agent framework for automated software architecture design.txt`〕- 1Huazhong University of Science and Technology 2Shanghai Jiaotong University (2025). Automated Risk Simulator. arXiv. 〔源文件：`综述参考文献\SafeAgent Safeguarding LLM agents via an automated risk simulator.txt`〕- ANETA PONISZEWSKA-MARAŃDA, Lodz University of Technology, Lodz, LD, Poland (2025). Autonomous agents in soware development for information retrieval. International Conference on the. 〔源文件：`Autonomous agents in software development for information retrieval using LLM models.txt`〕- YINGWEI MA, Alibaba Group Holding Limited, Hangzhou, Zhejiang, China (2025). Comprehensive Repository Exploration. International Conference on the. 〔源文件：`Alibaba LingmaAgent Improving automated issue resolution via comprehensive repository exploration.txt`〕- Tri Cao1, Bennett Lim1∗, Yue Liu1, Yuan Sui1, Yuexin Li1, (2025). Computer-Use Agents. arXiv. 〔源文件：`综述参考文献\VPI-bench Visual prompt injection attacks for computer-use agents.txt`〕- Sumeet Ramesh Motwani1,2 Mikhail Baranchuk2 Martin Strohmeier3 Vijay Bolina4 (2025). Deception via Steganography. 38th Conference on Neural Information Processing Systems (NeurIPS 2024).. 〔源文件：`Secret collusion among AI agents Multi-agent deception via steganography.txt`〕- Engineering: Literature Review, Vision, and the Road (2025). Engineering: Literature Review, Vision, and the Road. Unknown venue (DOI: 10.1145/3712003). 〔源文件：`LLM-based multi-agent systems for software engineering Literature review, vision, and the road ahea.txt`〕- CIPRIAN I PĂDURARU, University of Bucharest, Bucharest, Bucharest, Romania (2025). Enhancing Game AI Behaviors with Large Language Models and. International Conference on the. 〔源文件：`Enhancing game AI behaviors with large language models and agentic AI.txt`〕- be integrated into Integrated Development Environments (IDEs), (2025). Enhancing Human-IDE Interaction in the SDLC using LLM-based. Conference on the Foundations of Software Engineering (FSE Companion ’25),. 〔源文件：`Enhancing human-IDE interaction in the SDLC using LLM-based mediator agents.txt`〕- Hangzhou, Zhejiang, China (2025). Environmental Injection Attacks. arXiv. 〔源文件：`综述参考文献\Evaluating the robustness of multimodal agents against active environmental injection attacks.txt`〕- Gothenburg, Sweden (2025). Facilitating Trustworthy Human-Agent Collaboration in. Companion Proceedings of the 33rd ACM Symposium on the Foundations of. 〔源文件：`Facilitating trustworthy human-agent collaboration in LLM-based multi-agent system oriented software.txt`〕- AgentSentinel: An End-to-End and Real-Time Security Defense (2025). Framework for Computer-Use Agents. CCS '25: ACM SIGSAC Conference on. 〔源文件：`AgentSentinel An end-to-end and real-time security defense framework for computer-use agents.txt`〕- Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework (2025). Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework. arXiv. 〔源文件：`Fuzz-testing meets LLM-based agents An automated and efficient framework for jailbreaking text-to-i.txt`〕- Xiwen Teoh, Shanghai Jiao Tong University; National University of Singapore; (2025). Generalized Visual CAPTCHA Solving. This paper is included in the Proceedings of the. 〔源文件：`Are CAPTCHAs still bot-hard Generalized visual CAPTCHA solving with agentic vision language model.txt`〕- Hanoi, Vietnam (2025). GitHub README.MD Summarization. Conference on the Foundations of Software Engineering (FSE Companion ’25),. 〔源文件：`Teamwork makes the dream work LLMs-based agents for GitHub README.MD summarization.txt`〕- Wanjing Huang1 , Tongjie Pan 2, Yalan Ye2 (2025). Graphormer-Guided Task Planning: Beyond Static Rules with LLM. arXiv. 〔源文件：`综述参考文献\Graphormer-guided task planning Beyond static rules with LLM safety perception.txt`〕- Yijie Lu1, Tianjie Ju2, Manman Zhao1, Xinbei Ma2, Yuan Guo2, Zhuosheng Zhang2∗ (2025). Indirect Prompt Injection. arXiv. 〔源文件：`综述参考文献\EVA Red-teaming GUI agents via evolving indirect prompt injection.txt`〕- Shenzhen, China (2025). Issue-Oriented Code Review. Security Issue-Oriented Code Review. In33rd ACM International Conference. 〔源文件：`AutoReview An LLM-based multi-agent system for security issue-oriented code review.txt`〕- researchers have proposed different jailbreak attacks in depth, (2025). JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks. arXiv. 〔源文件：`综述参考文献\JailbreakRadar Comprehensive assessment of jailbreak attacks against LLMs.txt`〕- Kaiyuan Zhang, Zian Su, Pin-Yu Chen†, Elisa Bertino, Xiangyu Zhang, Ninghui Li (2025). LLM Agents Should Employ Security Principles. arXiv. 〔源文件：`综述参考文献\LLM agents should employ security principles.txt`〕- Venkata Sai Aswath Duvvuru and Bohan Zhang (2025). LLM-Agents Driven Automated Simulation Testing. arXiv. 〔源文件：`LLM-agents driven automated simulation testing and analysis of small uncrewed aerial systems.txt`〕- Mediating between Human Programmers and Integrated (2025). Mediating between Human Programmers and Integrated. arXiv. 〔源文件：`Mediating between human programmers and integrated development environments using LLM-based agents.txt`〕- Zheng Yu, Ziyi Guo, Yuhang Wu, and Jiahao Yu, Northwestern University; (2025). Mimicking Human Expertise. This paper is included in the Proceedings of the. 〔源文件：`Patchagent A practical program repair agent mimicking human expertise.txt`〕- Avishag Shapira*, Parth Atulbhai Gandhi*, Edan Habler, and Asaf Shabtai (2025). Mind the Web: The Security of Web Use Agents. arXiv. 〔源文件：`综述参考文献\Mind the web The security of web use agents.txt`〕- GAME CHAT: Multi-LLM Dialogue for Safe, Agile, and Socially (2025). Optimal Multi-Agent Navigation in Constrained Environments. arXiv. 〔源文件：`综述参考文献\GameChat Multi-LLM dialogue for safe, agile, and socially optimal multi-agent navigation in constra.txt`〕- KAI WEI, University of South Florida, Tampa, Tampa, FL, United States (2025). Poster: Agentic Shell Honeypot Using Structured Logging. CCS '25: ACM SIGSAC Conference on. 〔源文件：`Poster Agentic shell honeypot using structured logging.txt`〕- Cloak, Honey, Trap: (2025). Proactive Defenses Against LLM Agents. This paper is included in the Proceedings of the. 〔源文件：`Cloak, honey, trap Proactive defenses against LLM agents.txt`〕- Shanghai, China (2025). ProphetAgent: Automatically Synthesizing GUI Tests from Test. Apps. In 33rd ACM International Conference on the Foundations of Software. 〔源文件：`ProphetAgent Automatically synthesizing GUI tests from test cases in natural language for mobile ap.txt`〕- RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage (2025). RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage. arXiv. 〔源文件：`综述参考文献\RTBAS Defending LLM agents against prompt injection and privacy leakage.txt`〕- financial protocols and immutable smart contracts. This paper (2025). Real AI Agents with Fake Memories: Fatal Context Manipulation Attacks on Web3. arXiv. 〔源文件：`综述参考文献\Real AI agents with fake memories Fatal context manipulation attacks on Web3 agents.txt`〕- Thomas Kuntz1,∗, Agatha Duzan1,∗, Hao Zhao1, Francesco Croce1, (2025). Safety of Computer Use Agents. arXiv. 〔源文件：`综述参考文献\OS-harm A benchmark for measuring safety of computer use agents.txt`〕- Liangxuan Wu ∗, Chao Wang ∗, Tianming Liu, Yanjie Zhao and Haoyu Wang B, (2025). Security Risks of Mobile LLM Agents. arXiv. 〔源文件：`综述参考文献\From assistants to adversaries Exploring the security risks of mobile LLM agents.txt`〕- Systematic Categorization, Construction and Evaluation of New Attacks against (2025). Systematic Categorization, Construction and Evaluation of New Attacks against. arXiv. 〔源文件：`综述参考文献\Systematic categorization, construction and evaluation of new attacks against multi-modal mobile GUI.txt`〕- Fengyu Liu, Yuan Zhang, Jiaqi Luo, Jiarun Dai, Tian Chen, Letian Yuan, (2025). Taint-Style Vulnerabilities in LLM-based Agents. This paper is included in the Proceedings of the. 〔源文件：`Make agent defeat agent Automatic detection of taint-style vulnerabilities in LLM-based agents.txt`〕- Mykyta Mudryi*, 1, 2, Markiyan Chaklosh*, 1, 4, and Grzegorz Marcin (2025). The Hidden Dangers of Browsing AI Agents. arXiv. 〔源文件：`综述参考文献\The hidden dangers of browsing AI agents.txt`〕- IReSCoMath Research Lab, Faculty of (2025). Towards Adaptive Software Agents for Debugging. tive Software Agents for Debugging. In 33rd ACM International Conference. 〔源文件：`Towards adaptive software agents for debugging.txt`〕- Notre Dame, Indiana, USA (2025). Vulnerability to Fine-Print Injections. arXiv. 〔源文件：`综述参考文献\The obvious invisible threat LLM-powered GUI agents' vulnerability to fine-print injections.txt`〕- Y uhao Wu→, Franziska Roesner †, Tadayoshi Kohno †, Ning Zhang →, Umar Iqbal → (2025). design architecture that demonstrates the feasibility of execution. Unknown venue (DOI: 10.14722/ndss.2025.241131). 〔源文件：`IsolateGPT An execution isolation architecture for LLM-based systems.txt`〕- systems (OS) and the web, but remain vulnerable to indirect prompt injection, (2025). testing in realistic but controlled environments or ignore hybrid web-OS attack. arXiv. 〔源文件：`综述参考文献\RedTeamCUA Realistic adversarial testing of computer-use agents in hybrid web-OS environments.txt`〕

## 图表索引
- 图 1：主题—方法—结论知识图谱（HTML 交互图）
- 图 2：近五年高频关键词趋势折线图（HTML 交互图）

